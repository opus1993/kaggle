---
title: "Crop Yield"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

![](https://eos.com/wp-content/uploads/2020/10/field-with-well-grown-crop.jpg)

# Introduction  {.tabset .tabset-fade .tabset-pills}

The goal of this competition is to predict the crop yield.

My notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.

Lets dive right in.

The Kaggle kernels have many of the common r packages built in.  

## Load libraries

```{r }
#| label: setup
#| warning: false
#| message: false

if (dir.exists("/kaggle")){
  path <- "/kaggle/input/crop-yield-prediction-challenge/"
    
# options(repos = c(CRAN = "https://packagemanager.posit.co/cran/2021-03-22"))

install.packages("pak")
pak::pak("tidymodels/bonsai#119")
pak::pak("tidymodels/parsnip#1306")
    
remotes::install_github("luisdva/hexsession", quiet = TRUE)    
#remotes::install_github("tidymodels/plsmod", quiet = TRUE)    
remotes::install_url('https://github.com/catboost/catboost/releases/download/v1.2.8/catboost-R-linux-x86_64-1.2.8.tgz', 
                     INSTALL_opts = c("--no-multiarch", "--no-test-load"),
                    quiet = TRUE)  

cores <- future::availableCores()

} else {
  path <- stringr::str_c(here::here("data"),"/")
  orig_path <- stringr::str_c(here::here("data"),"/")

  cores <- future::availableCores(omit = 2)
}
 
suppressPackageStartupMessages({
library(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages
library(tidymodels, quietly = TRUE) # metapackage see https://www.tidymodels.org/
    
library(bonsai)  # Tidymodels access to Catboost
library(catboost)
library(stacks)  # Tidymodels ensembling
    
library(DALEXtra) # Tidymodels Explainability         

})

tidymodels_prefer()

options(tidymodels.dark = TRUE)

theme_set(ggdark::dark_theme_minimal())

hexsession::make_tile(packages = c("DALEXtra", "DALEX", "catboost","bonsai","correlationfunnel","yardstick","workflows",
                                   "tune","rsample","recipes","parsnip","infer","dials","broom","tidymodels","lubridate",
                                   "forcats","stringr","dplyr","purrr","readr","tidyr","tibble","ggplot2","tidyverse","reprex","testthat","usethis"))


```



## Load Data

```{r }
#| label: load data
#| warning: false
#| message: false

get_mode <- function(x) {
  ux <- unique(x[!is.na(x)])
  # which.max returns the index of the first maximum value
  ux[which.max(tabulate(match(x, ux)))]
}

preprocessor <- function(dataframe) {


dataframe <- dataframe |>
    janitor::clean_names() |>

    mutate(harvest_date = lubridate::ymd(harvest_date)) |>
  
    arrange(field_id, harvest_date) |> 
  
    group_by(field_id) |> 
    mutate(nth_crop = row_number(harvest_date),
           days_harvest_to_harvest = as.numeric(harvest_date - dplyr::lag(harvest_date, n = 1L, default = min(harvest_date))),
           lag_fertilizer_amount = dplyr::lag(fertilizer_amount, n = 1L, default = 0),
           lag_total_rainfall = dplyr::lag(total_rainfall, n = 1L, default = 0),
           lag_pesticide_usage = dplyr::lag(pesticide_usage, n = 1L, default = 0),
           lag_crop_type = dplyr::lag(crop_type, n = 1L),
          ) |> 
    dplyr::mutate(lag_crop_type = replace_na(lag_crop_type, get_mode(lag_crop_type))) |> 
    ungroup() |> 
    
    mutate(field_id3 = str_sub(field_id, 3, 3),
           field_id4 = str_sub(field_id, 4, 4),
           field_id5 = str_sub(field_id, 5, 5)) |> 

    mutate(fertilizer_amount = round(fertilizer_amount, 1),
           total_rainfall = round(total_rainfall, 0),
           pesticide_usage = round(pesticide_usage, 3)) |> 
    
    mutate(freq_total_rainfall= n(),
          .by = total_rainfall) |> 

    mutate(across(c(fertilizer_amount, total_rainfall, pesticide_usage, soil_moisture),
           \(x) as.numeric(scale(x)),
           .names = "scale_crop_type_{.col}"),
           .by = crop_type) |> 
  
    mutate(across(c(fertilizer_amount, total_rainfall, pesticide_usage, soil_moisture),
           \(x) as.numeric(scale(x)),
           .names = "scale_field_id_{.col}"),
           .by = field_id) |>     

    mutate(across(c(fertilizer_amount, total_rainfall, pesticide_usage, soil_moisture),
           \(x) as.numeric(scale(x)),
           .names = "scale_region_{.col}"),
           .by = region) |>     
    
      mutate(across(c(fertilizer_amount, total_rainfall, pesticide_usage, soil_moisture),
           \(x) as.numeric(scale(x)),
           .names = "scale_season_{.col}"),
           .by = season) |>   
    
    mutate(across(c(where(is.character)), ~ as.factor(.x))) 

return(dataframe)
}

raw_df <- read_csv(str_c(path, "crop_yield_train.csv"),
                   show_col_types = FALSE) 

tst_df <- read_csv(str_c(path, "crop_yield_test.csv"),
                   show_col_types = FALSE)  

all_df <-
    bind_rows(raw_df %>% mutate(source = "train"),
              tst_df %>% mutate(source = "test")) |>
          preprocessor() 

train_df <- all_df %>% 
  filter(source == "train") %>% 
  select(-source) 

competition_df <- all_df %>% 
  filter(source == "test") %>% 
  select(-source, -yield_tpha)


```

# EDA {.tabset .tabset-fade .tabset-pills}

## Features

We will de-duplicate the training set and calculate a mean yield_tpha figure.

```{r}
#| label: Features
#| warning: false
#| message: false
#| fig.width: 6

conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("where", "dplyr")

features <- train_df %>%
  dplyr::select(-id, -yield_tpha) |> 
  names()

train_df <- train_df %>% 
  dplyr::group_by_at(all_of(features)) |> 
  dplyr::summarise(id = min(id),
            yield_tpha = median(yield_tpha),
            .groups = "drop")

nom_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.factor)) |> 
  names() 

logical_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.logical)) |> 
  names() 

num_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.numeric)) |> 
  names()
```

Nominal features:

`r nom_features`

Numeric features: 

`r num_features`

Logical features: 

`r logical_features`


Size of the combined train and competition datasets:

`r nrow(all_df)`

Size of training set after de-duplication

`r nrow(train_df)`


## Numeric features



```{r}
#| label: numeric
#| warning: false
#| message: false
#| fig.height: 18

train_df %>% 
  dplyr::select(all_of(num_features), yield_tpha ) %>% 
  pivot_longer(-yield_tpha,
    names_to = "metric",
    values_to = "value"
  ) %>%
  ggplot(aes(value, fill = ggplot2::cut_number(yield_tpha, 5))) +
  geom_density(position = "stack") +
  scale_fill_brewer(type = "seq", palette = "Greens") +
  facet_wrap(vars(metric), scales = "free", ncol = 2) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "top") +
  labs(color = NULL, fill = "accident_risk",
       title = "Numeric Feature Univariate Distributions",
       caption = "Data: Kaggle | Visual: Jim Gruman")

```


## Nominal and Logical features

Explore the distribution of outcome class by factor level.


```{r}
#| label: nominal
#| warning: false
#| message: false
#| fig.height: 12
#| fig.width: 6


if(length(nom_features) >0){

train_df %>% 
  select(all_of(nom_features), all_of(logical_features), yield_tpha) %>% 
  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other'),
         across(logical_features, \(x) factor(x))) %>%
  pivot_longer(-yield_tpha,
    names_to = "metric",
    values_to = "value"
  ) %>%
    
  filter(!is.na(yield_tpha)) %>% 
    
  summarise(n = n(),
            .by = c(yield_tpha, metric, value)) %>%
      
  mutate(value = tidytext::reorder_within(value, n, metric)) %>%
    
  ggplot(aes(x = n, y = value, fill = ggplot2::cut_number(yield_tpha, 5))) +
  geom_col() +
  tidytext::scale_y_reordered() +
  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +
  scale_fill_brewer(type = "seq", palette = "Greens") +
  facet_wrap(vars(metric), scales = "free", ncol = 2) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
       legend.position = "bottom") +
  labs(title = "Nominal Feature Counts",
       fill = NULL,
       caption = "Data: Kaggle | Visual: Jim Gruman")

}

```

## Time series


```{r}
#| label: time series
#| warning: false
#| message: false
#| fig.height: 12
#| fig.width: 12

all_df |> 
  ggplot(aes(harvest_date, yield_tpha, color = source)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth() +
  geom_rug() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b")+
  scale_color_brewer(type = "qual", palette = "Dark2") +
  facet_wrap(vars(crop_type, region))

all_df |> 
  ggplot(aes(harvest_date, yield_tpha, color = source)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth() +
  geom_rug() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b")+
  scale_color_brewer(type = "qual", palette = "Dark2") +
  facet_wrap(vars(crop_type, season))

all_df |> 
  ggplot(aes(harvest_date, yield_tpha, color = source)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth() +
  geom_rug() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b")+
  scale_color_brewer(type = "qual", palette = "Dark2") +
  facet_wrap(vars(season, region))

all_df |> 
  ggplot(aes(harvest_date, yield_tpha, color = source)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth() +
  geom_rug() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b")+
  scale_color_brewer(type = "qual", palette = "Dark2") +
  facet_wrap(vars(field_id3, field_id4))

all_df |> 
  ggplot(aes(harvest_date, yield_tpha, color = source)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth() +
  geom_rug() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b")+
  scale_color_brewer(type = "qual", palette = "Dark2") +
  facet_wrap(vars(field_id3, field_id5))

all_df |> 
  ggplot(aes(harvest_date, yield_tpha, color = source)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth() +
  geom_rug() +
  scale_x_date(date_breaks = "3 months", date_labels = "%b")+
  scale_color_brewer(type = "qual", palette = "Dark2") +
  facet_wrap(vars(crop_type, field_id3))

```

## Counts of Missingness

                  
```{r}
#| label: counts of missingness

train_df %>% 
  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% 
  pivot_longer(everything(),
              names_to = "feature",
              values_to = "Count of Missing") %>% 
                   knitr::kable()

competition_df %>% 
  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% 
  pivot_longer(everything(),
              names_to = "feature",
              values_to = "Count of Missing") %>% 
                   knitr::kable()

```

## Counts of Distinct
                   
               
```{r}
#| label: counts of distinct
               
train_df %>%
  summarize(across(all_of(features), n_distinct)) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "Count of distinct train") |>
  left_join(
    competition_df %>%
      summarize(across(all_of(features), n_distinct)) %>%
      pivot_longer(everything(), names_to = "feature", values_to = "Count of distinct test"),
    by = join_by(feature)
  ) %>% 
                   knitr::kable()
               
```

## Duplicated

Is this competition transaction already in the training data with a correct label?


```{r}
#| label: duplicates
#| warning: false
#| message: false

bind_rows(train_df %>% mutate(source = "train"),
              competition_df %>% mutate(source = "test")) |> 
    group_by_at(features) %>%
    mutate(num_dups = n(),
           dup_id = row_number()) %>% 
    ungroup() %>% 
    group_by(source) %>%
    mutate(is_duplicated = dup_id > 1) %>% 
    count(is_duplicated) %>% 
                   knitr::kable()



```


## Pairwise Correlations
                   
`ggcorrplot` provides a quick look at numeric features where the correlation may be significant. 

```{r}
#| label: pairwise correlations
#| fig.width: 18
#| fig.height: 18                 
                 
trainp.mat <- train_df %>% 
  select(all_of(num_features), yield_tpha) %>% 
  ggcorrplot::cor_pmat() 
                   
train_df %>% 
  select(all_of(num_features), yield_tpha) %>%                    
  cor() %>% 
  ggcorrplot::ggcorrplot(hc.order = TRUE, 
                         lab = TRUE,
                         p.mat = trainp.mat,
    type = "lower", insig = "blank") +
  labs(title = "Pairwise Correlations Training Set")
                   
competitionp.mat <- competition_df %>% 
  select(all_of(num_features)) %>% 
  ggcorrplot::cor_pmat() 
                   
competition_df %>% 
  select(all_of(num_features)) %>%                   
  cor()   %>% 
  ggcorrplot::ggcorrplot(hc.order = TRUE, lab = TRUE,
                         p.mat = competitionp.mat,
    type = "lower", insig = "blank") +
  labs(title = "Pairwise Correlations Competition Set")

```                    
               
                   
## Target

The outcome variable distribution is skewed with values between 0 and 1.

```{r}
#| label: outcome 
#| warning: false
#| message: false
#| fig.width: 6

train_df |> count(yield_tpha) |> arrange(desc(yield_tpha))

train_df |> count(yield_tpha) |> arrange(yield_tpha)     

train_df %>% 
  ggplot(aes(yield_tpha)) +
  geom_histogram(bins = 100) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "accident_risk",
       caption = "Data: Kaggle.com | Visual: Jim Gruman")


```
                
# Regression Machine Learning {.tabset .tabset-fade .tabset-pills}


## Recipe

```{r}
#| label: recipe
#| warning: false
#| message: false
#| fig.width: 6
                   
catboost_rec <- recipe(formula(paste0("yield_tpha ~ ", str_c(features, collapse = " + "))), data = train_df) |>
                   step_date(harvest_date, features = c("doy","month", "dow"), keep_original_cols = FALSE)

folds <- vfold_cv(train_df,
                  v = 65,
                  repeats = 1,
                  strata = yield_tpha)

                   
```


## Catboost 


```{r}
#| label: catboost 
#| warning: false
#| message: false

boost_tree_catboost_spec <- 
  boost_tree(
    trees = 1500,
    tree_depth = tune(),
 #   sample_size = tune(),
    learn_rate =  tune(),
    stop_iter = tune()                                ### watch this one
  ) %>% 
  set_engine(engine = "catboost" ,
             thread_count = cores,
         #    eval_metric = "AUC",
         #    loss_function =  "Logloss",
         #    early_stopping_rounds = 100,
             bagging_temperature = 1,    # default is 1.0
             l2_leaf_reg = 3.0) %>%  #3.0 is the default. Any positive number is allowed.
  set_mode(mode = "regression")                    

dep_models <-
   workflow_set(
      preproc = list(base = catboost_rec),
      models = list(catboost = boost_tree_catboost_spec),
      cross = FALSE
   ) %>%
  option_add_parameters() |>
  option_add(
    control = finetune::control_sim_anneal( save_pred = TRUE, save_workflow = TRUE, verbose = TRUE),
    metrics = metric_set(rmse)
  )

catboost_params <- dep_models |>
 extract_workflow("base_catboost") |>
 extract_parameter_set_dials() |>
 update(
         learn_rate = learn_rate(range = c(-2.7,-1.8)),
         tree_depth = tree_depth(range = c(3,7)),
     #    sample_size = sample_size(),
         stop_iter = stop_iter(range = c(900,1500))
 ) 

catboost_models <- dep_models |>
 option_add(
   param_info = catboost_params,
   id = "base_catboost"
 ) |>
  workflow_map("tune_sim_anneal", 
               resamples = folds, 
               iter = 20,
               metrics = metric_set(rmse), verbose = TRUE)
                   

autoplot(catboost_models) +
  geom_text(aes(y = mean -0.0002, label = wflow_id), angle = 90, hjust = 1)+
  scale_y_continuous(limits = c(0,NA), expand = c(0, 0)) |>
  theme(legend.position = "none")

rank_results(catboost_models, rank_metric = "rmse", select_best = TRUE) %>%
   select(rank, mean, model, wflow_id, .config)
 
catboost_models %>%
  dplyr::filter(grepl("catboost", wflow_id)) %>%
  mutate(metrics = map(result, collect_metrics)) %>%
  dplyr::select(wflow_id, metrics) %>%
  tidyr::unnest(cols = metrics) |>
  arrange(mean)

catboost_models |>
  workflowsets::extract_workflow_set_result("base_catboost") |>
  autoplot() +
  labs(title = "Catboost Hyperparameter Search")

#best_params <- catboost_models |>
#  workflowsets::extract_workflow_set_result("base_catboost") |>
#  tune::select_best(metric = "rmse")

#regression_fit <- workflow(catboost_rec, boost_tree_catboost_spec) |>
#  finalize_workflow(best_params) |>
#  fit(train_df)

dep_stack <- stacks() %>%
  add_candidates(catboost_models) %>%
  blend_predictions(  metric =  metric_set(rmse),
      penalty = c(10^seq(-2.7, -1.3, 0.1)),
      non_negative = TRUE,
      control = tune::control_grid(allow_par = TRUE))

autoplot(dep_stack)

autoplot(dep_stack, type = "members")        
                   
autoplot(dep_stack, "weights")
                   
regression_fit <- dep_stack %>% 
    fit_members()
                   

```



# Performance {.tabset .tabset-fade .tabset-pills}

## DALEX

```{r}
#| label: explainer
#| warning: false
#| message: false
#| fig.height: 12

explainer <- 
  explain_tidymodels(
    regression_fit, 
    data = train_df %>% dplyr::select(all_of(features)), 
    y = as.numeric(train_df$yield_tpha),
    label = "Ensemble",
    verbose = FALSE
  )  %>% 
  model_parts()

ggplot_imp <- function(...) {
  obj <- list(...)
  metric_name <- attr(obj[[1]], "loss_name")
  metric_lab <- paste(metric_name, 
                      "after permutations\n(higher indicates more important)")
  
  full_vip <- bind_rows(obj) %>%
    filter(variable != "_baseline_")
  
  perm_vals <- full_vip %>% 
    filter(variable == "_full_model_") %>% 
    group_by(label) %>% 
    summarise(dropout_loss = mean(dropout_loss))
  
  p <- full_vip %>%
    filter(variable != "_full_model_") %>% 
    mutate(variable = fct_reorder(variable, -dropout_loss)) %>%
    ggplot(aes(dropout_loss, variable)) 
  
  if(length(obj) > 1) {
    p <- p + 
      facet_wrap(vars(label)) +
      geom_vline(data = perm_vals, aes(xintercept = dropout_loss, color = label),
                 linewidth = 1.4, lty = 2, alpha = 0.7) +
      geom_boxplot(aes(color = label, fill = label), alpha = 0.2)
  } else {
    p <- p + 
      geom_vline(data = perm_vals, aes(xintercept = dropout_loss),
                 linewidth = 1.4, lty = 2, alpha = 0.7) +
      geom_boxplot(fill = "#91CBD765", alpha = 0.4)
    
  }
  p +
    theme(legend.position = "none") +
    labs(x = metric_lab, 
         y = NULL,  fill = NULL,  color = NULL)
}
                   
ggplot_imp(explainer)      
                   

```                      


                   
# Submission
                   

```{r}
#| label: submission
#| warning: false
#| message: false
#| fig.height: 6
#| fig.width: 6                    
                   
augment(regression_fit, train_df) %>% 
  ggplot(aes(.pred, yield_tpha)) +
  geom_point(alpha = 0.1, shape = 20) +
  geom_abline(color = "green") +
  expand_limits(x = 0, y = 0)

augment(regression_fit, train_df) %>% 
  rmse(yield_tpha, .pred)

augment(regression_fit, train_df) %>% 
  ggplot(aes(.pred)) +
  geom_histogram()


submit_df <-  augment(
  regression_fit,
  competition_df) %>%
  transmute(id, yield_tpha = .pred)


head(submit_df)  %>% 
     knitr::kable()      

submit_df  %>% 
  write_csv("submission.csv")
```     