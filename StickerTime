{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85723,"databundleVersionId":10652996,"sourceType":"competition"}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"---\ntitle: \"Stickers\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict sticker counts.\n\nThis kernel is builds on the excellent work that CABAXIOM [EDA and Linear Regression Baseline](https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline/notebook) did in Python.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm and the `poissonreg` interface to poisson models. The `stacks` package provides ensembling functions.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s5e1/\"\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2021-03-22\"))\nremotes::install_github(\"mitchelloharawild/fable.prophet\", quiet = TRUE)\n\n\ncores <- future::availableCores()\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n  orig_path <- stringr::str_c(here::here(\"data\"),\"/\")\n\n  cores <- future::availableCores(omit = 1)\n}\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\n  \nlibrary(finetune)\nlibrary(poissonreg)\n  \nlibrary(bonsai)  \nlibrary(stacks)\n # interface to lightgbm\n\n})\n\ntidymodels_prefer()\n\nconflicted::conflicts_prefer(purrr::is_null)\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(ggdark::dark_theme_minimal())\n\n\n```\n\n<p><img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F59561%2F48dd013ca109783d1abe5389c233f352%2Fkerneler.png?generation=1735682270467088&amp;alt=media\" alt=\"\"></p>\n\n## Load Data\n\nCredit to [Broccoli Beef for the World Bank feature idea](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349)\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\n\nraw_spec <- cols(\n id = col_integer(),\n date = col_date(),\n country = col_character(),\n store = col_character(),\n product = col_character(),\n num_sold = col_double()\n)\n\n\ncompetition_spec <- cols(\n id = col_integer(),\n date = col_date(),\n country = col_character(),\n store = col_character(),\n product = col_character()\n )\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   col_types = raw_spec,\n                   show_col_types = FALSE) \n\npreprocessor <- function(dataframe) {\n\n\n dataframe <- dataframe %>%\n    janitor::clean_names() %>%\n    \n    mutate(gdp_year = year(date)) |>\n\n    mutate(across(c(where(is.character)), \\(x) as.factor(x))) |>\n    \n    inner_join(WDI::WDI(\n      indicator = \"NY.GDP.PCAP.CD\",\n      country = c('CA', 'FI', 'IT', 'KE', 'NO', 'SG'),\n      start = 2010,\n      end = 2020\n    ) |> \n    select(year, NY.GDP.PCAP.CD, country) |> \n    mutate(gdp = NY.GDP.PCAP.CD/sum(NY.GDP.PCAP.CD),\n           .by = year),\n    by = join_by(country == country, gdp_year == year)) |> \n    select(-gdp_year, -NY.GDP.PCAP.CD)\n  \n\nreturn(dataframe)\n}\n\nraw_preprocessed_df <- raw_df %>%\n          preprocessor() \n\ntst_preprocessed_df <- read_csv(str_c(path, \"test.csv\"),\n                   col_types = competition_spec,\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nfeatures <- raw_preprocessed_df %>%\n  select(-id, -num_sold) %>%\n  names()\n\n# because we already know the test set, let's remove the train set factor levels that do not correspond with anything on the test set\nfor (col in names(raw_preprocessed_df)) {\n    if (is.factor(raw_preprocessed_df[[col]]) & col != \"depression\") {\n      # Get levels in train and test dataframes\n      raw_levels <- levels(raw_preprocessed_df[[col]])\n      tst_levels <- levels(tst_preprocessed_df[[col]])\n      \n      # Identify levels in train not in test\n      new_levels <- setdiff(raw_levels, tst_levels)\n      \n      # Set these levels to NA in train dataframe\n      raw_df[[col]] <- factor(raw_preprocessed_df[[col]], levels = c(tst_levels, new_levels))\n      raw_df[[col]][raw_df[[col]] %in% new_levels] <- NA_character_\n    }\n  }\n\nall_preprocessed_df <-\n  bind_rows(\n    raw_preprocessed_df %>% mutate(source = \"train\") %>%\n      distinct(pick(all_of(features)), .keep_all = TRUE),\n    tst_preprocessed_df %>% mutate(source = \"test\")\n  )\n\ntrain_load_df <- all_preprocessed_df %>% \n  filter(source == \"train\") %>% \n  select(-source) |> \n  left_join(\n    expand_grid(\n      date = seq(min(raw_df$date), max(raw_df$date), by = \"day\"),\n      country = levels(raw_df$country),\n      store = levels(raw_df$store),\n      product = levels(raw_df$product)\n    )\n  )            \n\ncompetition_load_df <- all_preprocessed_df %>% \n  filter(source == \"test\") %>% \n  select(-source, -num_sold)\n\nnom_features <- train_load_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\n```\n\nNominal features:\n\n`r nom_features`\n\nSize of the combined train and competition datasets:\n\n`r nrow(all_preprocessed_df)`\n\nSize of the split made available to machine learning\n\n`r nrow(train_load_df)`\n\n<p><img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1034305%2Faa58ad81cfc4cf4f0762b2687de48896%2Fkaggle.jpg?generation=1562598537059919&alt=media\" alt=\"\"></p>\n          \n           \n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Nominal features\n\nExplore the distribution of outcome class by factor level.\n\nThere are 3 categorical columns that together describe a univariate time series. `Country`, `Store` and `Product`.           \n\n```{r}\n#| label: nominal\n#| warning: false\n#| message: false\n#| fig.height: 9\n#| fig.width: 9\n\ntrain_load_df %>% \n  select(all_of(nom_features), num_sold) %>% \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-num_sold,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n    \n  ggplot(aes(x = num_sold, y = value)) +\n  geom_boxplot() +\n  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Nominal Feature Distributions\",\n       fill = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n```\n\n## Missing              \n\n```{r}\n#| label: series\nseries <- train_load_df |> \n  group_by(country, store, product) |> \n  summarise(n = n(),\n            .groups = \"drop\")\n\n```\n\nThere are `r nrow(series)` univariate time series with `r max(series$n)` time series events.\n\nHowever we have missing values in the number of sales in the training data set:\n\n```{r}\n#| label: counts of missingness\n\ntrain_load_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x))),\n            .groups = \"drop\") %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n\ntrain_load_df |> \n  mutate(`missing num_sold rows` = if_else(is.na(num_sold),\"missing num sold\", \"has num sold\")) |> \n  group_by(country, store, product, `missing num_sold rows`) |> \n  summarise(num_rows = n(),\n            .groups = \"drop\") |> \n  pivot_wider(names_from = `missing num_sold rows`,\n              values_from = num_rows) |> \n  filter(!is.na(`missing num sold`)) |> \n  arrange(desc(`missing num sold`))  %>% \n                   knitr::kable()\n\n```\n\nIn total 9 of the 90 time series (10%) have atleast some missing some data.\n\n2 of the time series are completely missing data: { `Canada`, `Discount Stickers`, `Holographic Goose` } and  { `Kenya`, `Discount Stickers`, `Holographic Goose` }\n\n2 of the time series are only missing a single day of data { `Canada`, `Discount Stickers`, `Kerneler` } and { `Kenya`, `Discount Stickers`, `Kernerler Dark Mode` }\n\nLets take a closer look at where the missing values occur in each of these time series:\n\n```{r}\n#| label: where missing values occur\n#| warning: false\n#| message: false\n#| fig.height: 9\n#| fig.width: 6\n\ntrain_load_df %>% \n  \n  select(all_of(nom_features), date, num_sold) |> \n\n  mutate(has_na = mean(num_sold),\n         .by = c(country, store, product)) |> \n  filter(is.na(has_na)) |> \n  select(-has_na) |> \n  \n  mutate(series = glue::glue(\"{country} {store} {product}\")) |> \n\n  ggplot(aes(x = date, y = num_sold)) +\n  geom_line(color = \"lightgreen\") +\n  geom_vline(data = .%>%filter(is.na(num_sold)), aes(xintercept = date), color = \"orange\", alpha = 0.1) +\n  facet_wrap(vars(series), scales = \"free\", ncol = 1) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Number of Stickers Sold Daily and Missing\",\n       subtitle = \"For Series with Missing Data\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n```\n\nThe missing data is not missing completely randomly (with respect to time), some periods contain more missing data than others.\n\nIt looks data is missing when the value for `num_sold < 200` for `Canada` and `< 5 for Kenya` (for some of the time series). \n\nThe training data starts on `r min(train_load_df$date)` and ends on `r max(train_load_df$date)`\n\nThe competition dataset starts on `r min(competition_load_df$date)` and ends on `r max(competition_load_df$date)`\n\nWe have 7 years of data to train on occuring at daily frequency.\n\nWe are required to forecast 3 years.\n\n\n## Country\n\nFirst we show that its a good idea to aggregate countries when we make the forecast.\n\nTo do this we need to show that the proportion of total sales for each country remains constant, regardless of time.\n\nIn the graph below, we are looking for straight lines for each country:\n\n```{r}\n#| label: country 1\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6\n\ntrain_load_df %>% \n\n  mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n  \n  summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(country,date)) |> \n\n  ggplot(aes(x = date, y = `Proportion of Sales`, color = country)) +\n  geom_line() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Proportion of Stickers Sold Daily\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n\n```\n\n\nThe lines are not perflectly straight, meaning a single constant does not explain the proportion of sales regardless of time.\n\nThe lines for each country do seem to have rises and falls each year (noteably exactly at the year markings) something artificially strange is going on here.\n\nThe link seems to be GDP per captia.\n\n\n```{r}\n#| label: country 2\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6\n\ntrain_load_df %>% \n\n  mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n  \n  mutate(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(country,date))  |> \n\n  ggplot(aes(x = date, y = `Proportion of Sales`, color = country)) +\n  geom_line() +\n  geom_line(aes(y = gdp, group = country), color = \"gray\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Proportion of Stickers Sold Daily\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n\n```\n\n\nThe gray line shows the ratio of GDP per captia for each year for that country compared to the sum of GDP per capita for all the other countries.\n\nNote that Canada and Kenya do not perfectly allign to these ratios, likely because of missing values, this is fine.\n\nThere might be some slight non-random noise here, so perhaps this method isnt quite perfect?\n\nThis means we can predict the proportion of sales between each country for each year that we have to forecast for, by considering the annual GDP per capita. This means we can aggregate the number of sales across countries for each product and store when making the forecast and then disagregate using the known annual GDP per capita ratios for the years we are predicting for.\n\n\n## Store\n\n```{r}\n#| label: store 1\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6\n\ntrain_load_df %>% \n\n  mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n  \n  summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(store,date)) |> \n\n  ggplot(aes(x = date, y = `Proportion of Sales`, color = store)) +\n  geom_line() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Proportion of Stickers Sold Daily\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n\n\n```\n\nThe ratios remain constant. This means we can generally predict the proportion of sales for each store, regardless of when it occurs.\n\n\n```{r}\n#| label: store 2\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6\n\nstore_ratio <- train_load_df %>% \n  mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n    summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(store,date)) |> \n   summarise(store_proportion = mean(`Proportion of Sales`), .by = store)\n\ntrain_load_df %>% \n\n  mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n  \n  summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(store,date)) |> \n  \n  left_join(store_ratio, by = join_by(store)) |>\n\n  ggplot(aes(x = date, y = `Proportion of Sales`, color = store)) +\n  geom_line() +\n  geom_line(aes(y = store_proportion, group = store), color = \"gray\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Proportion of Stickers Sold Daily\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n\n```\n\n\n## Product\n\nThe product ratio shows clear sinsidual lines for each product, with a period of something like 1 or 2 years.\n\n\n```{r}\n#| label: product 1\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6\n\ntrain_load_df %>% \n\n  mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n  \n  summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(product,date)) |> \n\n  ggplot(aes(x = date, y = `Proportion of Sales`, color = product)) +\n  geom_line() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Proportion of Stickers Sold Daily\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n\n```\n\nAs we have a clear seasonal pattern of the ratio of sales for each product, we do not need to forecast each product individually (or treat product as a categorical variable etc.). Instead we can forecast the sum of all sales each day, then afterwards convert the forecasted sum down to the forecast for each product, using the forecasted ratios for each date.\n\n```{r}\n#| label: product 2\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6\n\nproduct_ratio <- train_load_df |> \n    mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n  \n  summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(product,date)) |> \n  \n  mutate(date_label = glue::glue(\"year { year(date) %% 2 } {strftime(date, '%j') }\")) |> \n  \n  summarize(product_proportion = mean(`Proportion of Sales`, na.rm = TRUE), \n           .by = c(date_label, product))\n\ntrain_load_df %>% \n\n  mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n  \n  summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(product,date)) |> \n  \n  mutate(date_label = glue::glue(\"year { year(date) %% 2 } {strftime(date, '%j') }\")) |> \n  left_join(product_ratio, by = join_by(product, date_label)) |> \n\n  ggplot(aes(x = date, y = `Proportion of Sales`, color = product)) +\n  geom_line() +\n  geom_line(aes(y = product_proportion, group = product), color = \"gray\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Proportion of Stickers Sold Daily\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n```\n\n\n\nAll this together means we really only need to forecast 2 time series:\n\n- The ratio in number of sales for each product each day\n- The total sales each day\n\n\n## Fill Outcome            \n\nFor each day of the training set, as many as 7 observations are missing at the same time within the 54 time series.\n\nWe will use a simple hueristic for filling. \n\n\n```{r}\n#| label: fill outcome\n           \n## lets run a quick glmnet poisson model to fill in those missing num_sold values\n \ntrain_prefill_df <- train_load_df |> \n  \n  left_join(store_ratio, by = join_by(store)) |>\n  \n  mutate(date_label = glue::glue(\"year { year(date) %% 2 } {strftime(date, '%j') }\")) |> \n  left_join(product_ratio, by = join_by(product, date_label)) |> \n  select(-date_label) |>  \n  \n  mutate(num_sold = if_else(is.na(num_sold), gdp*store_proportion*product_proportion*70000, num_sold))\n\nstore_ratio <- train_prefill_df %>% \n  mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n    summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(store,date)) |> \n   summarise(store_proportion = mean(`Proportion of Sales`), .by = store)\n\nproduct_ratio <- train_prefill_df |> \n    mutate(`total sold by date` = sum(num_sold, na.rm = TRUE),\n         .by = date) |> \n  \n  summarise(`Proportion of Sales` = sum(num_sold / `total sold by date`, na.rm= TRUE),\n         .by = c(product,date)) |> \n  \n  mutate(date_label = glue::glue(\"year { year(date) %% 2 } {strftime(date, '%j') }\")) |> \n  \n  summarize(product_proportion = mean(`Proportion of Sales`, na.rm = TRUE), \n           .by = c(date_label, product))\n\ntrain_fill_df <- train_load_df |> \n  left_join(store_ratio, by = join_by(store)) |>\n  \n  mutate(date_label = glue::glue(\"year { year(date) %% 2 } {strftime(date, '%j') }\")) |> \n  left_join(product_ratio, by = join_by(product, date_label)) |> \n  select(-date_label) |> \n  \n  mutate(num_sold = if_else(is.na(num_sold), gdp*store_proportion*product_proportion*70000, num_sold))\n  \n\ncompetition_fill_df <- competition_load_df |> \n  left_join(store_ratio, by = join_by(store)) |>\n  \n  mutate(date_label = glue::glue(\"year { year(date) %% 2 } {strftime(date, '%j') }\")) |> \n  left_join(product_ratio, by = join_by(product, date_label)) |> \n  select(-date_label)\n\n```\n\n```{r}\n#| label: fill outcome confirmation\n#| warning: false\n#| message: false\n#| fig.height: 50\n#| fig.width: 6\n#| \ntrain_fill_df %>% \n  \n  select(all_of(nom_features), date, num_sold) |> \n\n  mutate(series = glue::glue(\"{country} {store} {product}\")) |> \n  \n  ggplot(aes(x = date, y = num_sold)) +\n  geom_line(color = \"lightgreen\") +\n  facet_wrap(vars(series), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Number of Stickers Sold Daily\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n```\n      \n\n## Prophet\n\nWe will assess several time series package algorithms to project the sales totals by day.\n\n```{r}\n#| label: total model\n#| warning: false\n#| message: false\n#| fig.width: 6\n\nlibrary(fable)\nlibrary(feasts)\nlibrary(fable.prophet)\n\ntotal_df <- train_fill_df |>   \n  summarize(num_sold = sum(num_sold, na.rm = TRUE),\n         .by = date)\n\ntotal_ts <- total_df |> \n  fable::as_tsibble()\n  \ntotal_ts |> \n  autoplot()\n\ntotal_ts |> \n  gg_season()\n\n# total_df |> \n#   mutate(DOY = strftime(date, \"%j\")) |> \n#   summarize(num_sold = mean(num_sold),\n#             .by = DOY) |> \n#   slice_max(num_sold, n = 20) |>\n#   pull(DOY)\n#   \n\ntotal_tr <- total_ts |> \n  tsibble::stretch_tsibble(.init = 365, .step = 365)\n\ntotal_tr |> \n  model(\n        stepwise = ARIMA(num_sold),\n        search = ARIMA(num_sold, stepwise=FALSE),\n        ets = ETS(num_sold ~ trend()),\n        prophet = fable.prophet::prophet(num_sold ),\n        croston = CROSTON(num_sold ),\n        tslm = TSLM(num_sold ~ trend() + season())\n  ) |>\n  forecast(h = \"1 year\") |>\n  fabletools::accuracy(total_ts)\n\nprophet_fit <- total_ts |> \n  model(prophet = fable.prophet::prophet(num_sold )) \n  \nprophet_fit |> \n  components() |> \n  autoplot()\n\nprophet_forecast <- prophet_fit |> \n  fabletools::forecast(total_ts, h = \"3 years\")\n\nprophet_forecast |> \n  fabletools::accuracy(total_ts)\n\nprophet_forecast |>\n    autoplot(level = NULL)\n\nprophet_forecast <- prophet_fit |>\n  fabletools::forecast(competition_fill_df |>\n                         count(date) |> as_tsibble(),\n                       h = \"3 years\") |> \n  as_tibble() |> \n  select(date, total_sold = .mean) \n\n\ncompetition_complete_df <- competition_fill_df |>\n  left_join(\n    prophet_forecast\n  )\n\ntrain_complete_df <- train_fill_df |> \n  mutate(total_sold = sum(num_sold), .by = date)\n\nfeatures <- train_complete_df %>%\n  select(-id, -num_sold) %>%\n  names()\n\n```\n# Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n<img height=\"auto\" width=\"600\" alt=\"\" src=\"https://i.ibb.co/g6YM0gK/0-1.jpg\">\n\n## What exactly are modeling then?\n \nWith the sum of sales projected, and the proportions, what is left is a sort of residual. Lets take a look to try to understand what features remain.\n\n```{r}\n#| label: what are we modeling\n#| warning: false\n#| message: false\n#| fig.width: 6\n#| fig.height: 6\n\ntrain_complete_df |> \n  mutate(est_num_sold = total_sold * store_proportion * product_proportion * gdp) |> \n  summarize(mean_residual = mean(est_num_sold - num_sold), .by = c(product,date)) |> \n  ggplot(aes(date, mean_residual, color = product)) +\n  geom_line() +\n  facet_wrap(vars(product)) +\n  theme(legend.position = \"bottom\")\n\ntrain_complete_df |> \n  mutate(est_num_sold = total_sold * store_proportion * product_proportion * gdp) |> \n  summarize(mean_residual = mean(est_num_sold - num_sold), .by = c(country,date)) |> \n  ggplot(aes(date, mean_residual, color = country)) +\n  geom_line() +\n  facet_wrap(vars(country)) +\n  theme(legend.position = \"bottom\")\n\ntrain_complete_df |> \n  mutate(est_num_sold = total_sold * store_proportion * product_proportion * gdp) |> \n  summarize(mean_residual = mean(est_num_sold - num_sold), .by = c(store,date)) |> \n  ggplot(aes(date, mean_residual, color = store)) +\n  geom_line() +\n  facet_wrap(vars(store)) +\n  theme(legend.position = \"bottom\")\n\n```\n\n\n                 \n                \n\n```{r}\n#| label: mape\n\nmetrics <- metric_set(mape)\n\n```\n\n## Recipes\n\n```{r}\n#| label: recipe\n#| warning: false\n#| message: false\n#| fig.width: 6\n                   \n                   \nlgbm_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_complete_df) %>%\n\n  step_mutate(EOY = factor(strftime(date, \"%j\"))) |>  \n  step_date(date, label = TRUE,   features = c(\"dow\", \"year\"), keep_original_cols = FALSE) %>% \n  \n  step_mutate(est_num_sold = total_sold * store_proportion * product_proportion * gdp) |> \n\n  step_zv(all_predictors())\n\n\nlinear_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_complete_df) %>%\n  \n  step_mutate(EOY = factor(strftime(date, \"%j\"))) |>  \n  step_date(date, label = TRUE,   features = c(\"dow\", \"year\"), keep_original_cols = FALSE) %>% \n  step_dummy(all_nominal_predictors()) |> \n  \n  step_mutate(est_num_sold = total_sold * store_proportion * product_proportion * gdp) |> \n  \n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nxgb_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_complete_df) %>%\n\n  step_mutate(EOY = factor(strftime(date, \"%j\"))) |>  \n  step_date(date, label = TRUE,   features = c(\"dow\", \"year\"), keep_original_cols = FALSE) %>% \n  \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> \n  \n  step_mutate(est_num_sold = total_sold * store_proportion * product_proportion * gdp) |> \n\n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nfolds <- rsample::sliding_period(\n  train_complete_df,\n  date,\n  period = \"month\",\n  lookback = 60L,\n  assess_start = 1L,\n  assess_stop = 3L,\n  complete = TRUE,\n  step = 3L,\n  skip = 0L\n)\n\nfolds |>\n  tidy() |>\n  ggplot(aes(x = Resample, y = factor(Row), fill = Data)) +\n  geom_tile() +\n  labs(y = \"Date\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n```\n\n## Workflowsets\n\n```{r}\n#| label: workflowsets\n#| warning: false\n#| message: false\n\npoisson_reg_glmnet_spec <-\n  poisson_reg(penalty = tune(), mixture =1) %>%\n  set_engine('glmnet')\n\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 100L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune()\n#   loss_reduction = tune()\n #  mtry = tune()\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n             num_threads = cores\n             ) %>%\n  set_mode(mode = \"regression\") \n\n\nboost_tree_xgb_spec <- \n  boost_tree(\n    trees = 100L,\n    min_n = tune(),\n    learn_rate = tune()\n  ) %>% \n  set_engine(engine = \"xgboost\", nthread = cores) %>%\n  set_mode(mode = \"regression\")    \n\n\ndep_models <- \n   workflow_set(\n      preproc = list(imputedhot = xgb_rec,\n                     linear = linear_rec,\n                     base = lgbm_rec),\n      models = list(xgb = boost_tree_xgb_spec,\n                    poisson = poisson_reg_glmnet_spec,\n                    lgbm = boost_tree_lgbm_spec),\n      cross = FALSE\n   ) %>% \n  option_add_parameters() |> \n  option_add(\n    control = finetune::control_sim_anneal( save_pred = TRUE, save_workflow = TRUE, verbose = TRUE),\n    metrics = metrics\n  )\n\nxgb_params <- dep_models |>\n  extract_workflow(\"imputedhot_xgb\") |>\n  parameters() |>\n  update(\n    learn_rate = learn_rate(range = c(-1.4,-1.0)),\n    min_n = min_n(range = c(7,90))\n    )\n\nlinear_params <- dep_models |>\n  extract_workflow(\"linear_poisson\") |>\n  parameters() |>\n  update(\n     penalty = penalty(range = c(-0.2,0))\n        )\n\nlgbm_params <- dep_models |> \n  extract_workflow(\"base_lgbm\") |> \n  parameters() |> \n  update(\n      min_n = min_n(range = c(10,100)),\n#      loss_reduction = loss_reduction(range = c(-1,0)),\n   #   mtry = mtry(range = c(27,unknown())),\n      num_leaves = num_leaves(range = c(70,200)),\n      tree_depth = tree_depth(range = c(9,100)),\n      learn_rate = learn_rate(range = c(-1.2,-0.9))\n         )\n\ndep_models <- dep_models |> \n  option_add(\n    param_info = lgbm_params,\n    id = \"base_lgbm\"\n  ) |> \n  option_add(\n    param_info = linear_params,\n    id = \"linear_poisson\"\n  ) |> \n  option_add(\n    param_info = xgb_params,\n    id = \"imputedhot_xgb\"\n  ) |>\n   workflow_map(\"tune_sim_anneal\", resamples = folds, iter = 12, \n                metrics = metrics, verbose = TRUE)\n                   \n\nautoplot(dep_models) +\n  geom_text(aes(y = mean -5, label = wflow_id), angle = 90, hjust = 1)+\n  scale_y_continuous(expand = expansion(mult = c(0.5,0)))+\n  theme(legend.position = \"none\")\n\nrank_results(dep_models, rank_metric = \"mape\", select_best = TRUE) %>% \n   select(rank, mean, model, wflow_id, .config)\n\ndep_models %>%\n  dplyr::filter(grepl(\"xgb\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean)\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"imputedhot_xgb\") |> \n  autoplot() +\n  labs(title = \"XGB Hyperparameter Search\")\n\ndep_models %>%\n  dplyr::filter(grepl(\"lgbm\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean)\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"base_lgbm\") |> \n  autoplot() +\n  labs(title = \"LGBM Hyperparameter Search\")\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"linear_poisson\") |> \n  autoplot() +\n  labs(title = \"LGBM Hyperparameter Search\")\n\ndep_stack <- stacks() %>%\n  add_candidates(dep_models) %>%\n  blend_predictions(  metric = metrics,\n      penalty = c(10^seq(-2.7, -0.4, 0.1)),\n      non_negative = TRUE,\n      control = tune::control_grid(allow_par = TRUE))\n\nautoplot(dep_stack)\n\nautoplot(dep_stack, type = \"members\")        \n                   \nautoplot(dep_stack, \"weights\")\n                   \nregression_fit <- dep_stack %>% \n    fit_members()\n\n\n```\n\n# Submission\n\nThe ensemble regression model is applied to labeled training data one last time to explore the greatest residuals, and then fit to competition / test data for submission to kaggle.                   \n\n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6                    \n                   \ntrain_result <- augment(regression_fit, train_complete_df) %>% \n  mutate(residual = abs(num_sold - .pred))\n\ntrain_result |> \n  group_by(country, store, product) |> \n  mape(num_sold, .pred) |> \n  arrange(desc(.estimate))\n\ntrain_result |> \n  ggplot(aes(.pred, residual , fill = country)) +\n  geom_hex()\n\ntrain_result |> \n  ggplot(aes(.pred, residual , fill = store)) +\n  geom_hex()\n\ntrain_result |> \n  ggplot(aes(.pred, residual , fill = product)) +\n  geom_hex() \n                   \nsubmit_df <-  augment(\n  regression_fit,\n  competition_complete_df \n) %>%\n  transmute(id, num_sold = if_else(.pred < 0, 0, .pred))\n\nhead(submit_df)  %>% \n     knitr::kable()      \n\nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n``` ","metadata":{"_uuid":"8299a615-fadb-4469-82a3-ea6eb284d75b","_cell_guid":"89e03c62-7d27-45e0-8d9e-5f4f5acac1d1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}