{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85723,"databundleVersionId":10652996,"sourceType":"competition"}],"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"---\ntitle: \"Stickers\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict sticker counts.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm and `brulee` interface to torch.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s5e1/\"\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2021-03-22\"))\n#install.packages(\"BradleyTerry2\", quiet = TRUE)\n# remotes::install_github(\"luisdva/hexsession\", quiet = TRUE)  had used previously, but recent addition of menu() makes it unworkable here\n\ncores <- future::availableCores()\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n  orig_path <- stringr::str_c(here::here(\"data\"),\"/\")\n\n  cores <- future::availableCores(omit = 1)\n}\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\n  \nlibrary(finetune)\n  \nlibrary(bonsai)  \nlibrary(stacks)\n # interface to lightgbm\n\n})\n\ntidymodels_prefer()\n\nconflicted::conflicts_prefer(purrr::is_null)\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(ggdark::dark_theme_minimal())\n\n\n```\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F59561%2F48dd013ca109783d1abe5389c233f352%2Fkerneler.png)\n\n## Load Data\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\n\nraw_spec <- cols(\n id = col_integer(),\n date = col_date(),\n country = col_character(),\n store = col_character(),\n product = col_character(),\n num_sold = col_double()\n)\n\n\ncompetition_spec <- cols(\n id = col_integer(),\n date = col_date(),\n country = col_character(),\n store = col_character(),\n product = col_character()\n )\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   col_types = raw_spec,\n                   show_col_types = FALSE) \n\n\npreprocessor <- function(dataframe) {\n\ndataframe <- dataframe %>%\n    janitor::clean_names() %>%\n\n#    mutate(across(c(\"insurance_duration\",\"number_of_dependents\"), \\(x) as.factor(x), .names = \"factor_{.col}\")) |>\n  \n    mutate(across(c(where(is.character)), \\(x) as.factor(x))) \n\n\nreturn(dataframe)\n}\n\nraw_df <- raw_df %>%\n          preprocessor() |> \n          na.omit(num_sold) \n\n\n\ntst_df <- read_csv(str_c(path, \"test.csv\"),\n                   col_types = competition_spec,\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nfeatures <- raw_df %>%\n  select(-id, -num_sold) %>%\n  names()\n\n# because we already know the test set, let's remove the train set factor levels that do not correspond with anything on the test set\nfor (col in names(raw_df)) {\n    if (is.factor(raw_df[[col]]) & col != \"depression\") {\n      # Get levels in train and test dataframes\n      raw_levels <- levels(raw_df[[col]])\n      tst_levels <- levels(tst_df[[col]])\n      \n      # Identify levels in train not in test\n      new_levels <- setdiff(raw_levels, tst_levels)\n      \n      # Set these levels to NA in train dataframe\n      raw_df[[col]] <- factor(raw_df[[col]], levels = c(tst_levels, new_levels))\n      raw_df[[col]][raw_df[[col]] %in% new_levels] <- NA_character_\n    }\n  }\n\nall_df <-\n  bind_rows(\n    raw_df %>% mutate(source = \"train\") %>%\n      distinct(pick(all_of(features)), .keep_all = TRUE),\n    tst_df %>% mutate(source = \"test\")\n  )\n\ntrain_df <- all_df %>% \n  filter(source == \"train\") %>% \n  select(-source) |> \n  left_join(\n    expand_grid(\n      date = seq(min(raw_df$date), max(raw_df$date), by = \"day\"),\n      country = levels(raw_df$country),\n      store = levels(raw_df$store),\n      product = levels(raw_df$product)\n    )\n  ) |> \n  replace_na(list(num_sold = 0))\n\ncompetition_df <- all_df %>% \n  filter(source == \"test\") %>% \n  select(-source, -num_sold)\n\nnom_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\nLogical features: \n\n`r logical_features`\n\n\nSize of the combined train and competition datasets:\n\n`r nrow(all_df)`\n\nSize of the split made available to machine learning\n\n`r nrow(train_df)`\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Numeric features\n\nConsider where features require univariate transformation, or clipping outliers.\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 18\n#| fig.width: 12\n\nif(length(num_features) >0){\n\ntrain_df %>% \n  select(all_of(num_features), num_sold ) %>% \n  pivot_longer(-num_sold,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = ggplot2::cut_number(num_sold, 5))) +\n  geom_histogram(bins = 200) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\") +\n  labs(color = NULL, fill = NULL,\n       title = \"Numeric Feature Univariate Distributions\",\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\ntrain_df %>% \n  select(all_of(num_features), num_sold ) %>% \n  pivot_longer(-num_sold,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, num_sold)) +\n  geom_point(shape = 20, alpha = 0.01) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\") +\n  labs(color = NULL, fill = NULL,\n       title = \"Numeric Features and Premium Amount\",\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n}\n```\n\n\n## Nominal features\n\nExplore the distribution of outcome class by factor level, and the factor levels that exist in test that do not exist in training.\n\n\n```{r}\n#| label: nominal\n#| warning: false\n#| message: false\n#| fig.height: 9\n#| fig.width: 9\n\n\nif(length(nom_features) >0){\n\ntrain_df %>% \n  select(all_of(nom_features), num_sold) %>% \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-num_sold,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n    \n  filter(!is.na(num_sold)) %>% \n    \n  summarise(n = n(),\n            .by = c(num_sold, metric, value)) %>%\n      \n  mutate(value = tidytext::reorder_within(value, n, metric)) %>%\n    \n  ggplot(aes(x = n, y = value, fill = ggplot2::cut_number(num_sold, 5))) +\n  geom_col() +\n  tidytext::scale_y_reordered() +\n  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Nominal Feature Counts\",\n       fill = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n}\n\n```\n\n## Time Series\n\n```{r}\n#| label: date\n#| warning: false\n#| message: false\n#| fig.height: 16\n#| fig.width: 9\n\ntrain_df %>% \n  \n  select(all_of(features), num_sold) |> \n  \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-c(num_sold,date),\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n    \n  summarise(n = sum(num_sold),\n            .by = c(num_sold, date, metric, value)) %>%\n  \n \n  ggplot(aes(x = date, y = n, color = value)) +\n  geom_point(alpha = 0.1) +\n#  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_grid(vars(metric, value), scales = \"free\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Number of Stickers Sold\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n\n```\n\n\n\n\n\n## Counts of Missingness\n                  \n```{r}\n#| label: counts of missingness\n\ntrain_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n                   \ntrain_df %>% \n    select(all_of(features)) %>%\n    slice_sample(n = 2000) %>%\n    naniar::vis_miss()          \n\nnaniar::gg_miss_var(train_df %>% select(all_of(features)), \n                   facet = store)    \n\n                  \n```\n\n## Counts of Distinct\n                   \n               \n```{r}\n#| label: counts of distinct\n               \ntrain_df %>%\n  summarize(across(all_of(features), n_distinct)) %>%\n  pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct train\") |>\n  left_join(\n    tst_df %>%\n      summarize(across(all_of(features), n_distinct)) %>%\n      pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct test\")\n  ) %>% \n                   knitr::kable()\n\n\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct label?\n\nIt looks like we had had a few car models with more than one selling price.\n\nThe test set has 81 entries that appear in train, and 24 that appear twice in test.\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\n\nall_df %>%\n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>% \n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>% \n    count(is_duplicated) %>% \n                   knitr::kable()\n\n```\n                   \n\n\n\n## Target\n\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 6\n\ntrain_df |> count(num_sold) |> arrange(desc(num_sold))                   \n                   \ntrain_df %>% \n  ggplot(aes(num_sold)) +\n  geom_histogram(bins = 100) +\n  scale_x_log10(labels = scales::comma) +\n  labs(title = \"Stickers Sold for each Training Observation\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n\n\n```\n \n           \n                \n# Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n\n```{r}\n#| label: mape\n\nmetrics <- metric_set(mape)\n\n```\n\n## Recipes\n\n```{r}\n#| label: recipe\n#| warning: false\n#| message: false\n#| fig.width: 6\n                   \nlgbm_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  \n  step_date(date, label = TRUE, features = c(\"dow\", \"month\", \"week\", \"year\"), keep_original_cols = FALSE) %>% \n\n  step_zv(all_predictors())\n\n\nlinear_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n\n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |> \n\n  step_date(date, label = TRUE,   features = c(\"dow\", \"month\", \"week\", \"year\"), keep_original_cols = FALSE) %>% \n  \n  step_dummy(all_nominal_predictors()) |> \n\n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nxgb_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |> \n  \n  step_date(date, label = TRUE,   features = c(\"dow\", \"month\", \"week\", \"year\"), keep_original_cols = FALSE) %>% \n  \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> \n  \n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nfolds <- vfold_cv(train_df,\n                  v = 3,\n                  repeats = 1,\n                  strata = num_sold)\n\n                   \n```\n\n## Linear Model\n\n```{r}\n#| label: linear\n#| warning: false\n#| message: false\n#| fig.height: 6\n\nlinear_reg_glmnet_spec <-\n   linear_reg(penalty = tune(), mixture = tune()) %>%\n   set_engine('glmnet')\n\nwf <- workflow(linear_rec,linear_reg_glmnet_spec)\n\nrs <- tune_grid(\n  wf,\n  folds,\n  grid = 6,\n  metrics = metrics\n)\n\nautoplot(rs)\n\nlinear_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)\n\nvip::vip(linear_fit, num_features = 30L)\n\nvip::vi(linear_fit)\n\n\n```\n\n## XGB\n\n```{r}\n#| label: xgb\n#| warning: false\n#| message: false\n#| fig.height: 6\n\nboost_tree_xgb_spec <- \n  boost_tree(\n    trees = 80L,\n    min_n = tune(),\n    learn_rate = tune()\n  ) %>% \n  set_engine(engine = \"xgboost\", nthread = cores) %>%\n  set_mode(mode = \"regression\")    \n\nwf <- workflow(xgb_rec,boost_tree_xgb_spec)\n\nrs <- tune_grid(\n  wf,\n  folds,\n  grid = 6,\n  metrics = metrics\n)\n\nautoplot(rs)\n\nxgb_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)\n\nvip::vip(xgb_fit, num_features = 30L)\n\nvip::vi(xgb_fit)\n\n\n```\n\n## LGBM\n\n```{r}\n#| label: lgbm\n#| warning: false\n#| message: false\n#| fig.height: 6\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 80L,\n   tree_depth = tune(),\n   learn_rate =  0.1,\n   min_n = tune(),\n#  loss_reduction = tune()\n #  mtry = tune()\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_threads = cores\n             ) %>%\n  set_mode(mode = \"regression\") \n\nwf <- workflow(lgbm_rec,boost_tree_lgbm_spec)\n\nrs <- tune_grid(\n  wf,\n  folds,\n  grid = 6,\n  metrics = metrics\n)\n\nautoplot(rs)\n\nlgbm_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)\n\nvip::vip(lgbm_fit, num_features = 30L)\n\nvip::vi(lgbm_fit)\n\n\n```\n\n\n## Workflowsets\n\n```{r}\n#| label: workflowsets\n#| warning: false\n#| message: false\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 180L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune()\n#   loss_reduction = tune()\n #  mtry = tune()\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_threads = cores\n             ) %>%\n  set_mode(mode = \"regression\") \n\n\nboost_tree_xgb_spec <- \n  boost_tree(\n    trees = 180L,\n    min_n = tune(),\n    learn_rate = tune()\n  ) %>% \n  set_engine(engine = \"xgboost\", nthread = cores) %>%\n  set_mode(mode = \"regression\")    \n\n\ndep_models <- \n   workflow_set(\n      preproc = list(imputedhot = xgb_rec,\n                     base = lgbm_rec),\n      models = list(xgb = boost_tree_xgb_spec,\n                    lgbm = boost_tree_lgbm_spec),\n      cross = FALSE\n   ) %>% \n  option_add_parameters() |> \n  option_add(\n    control = finetune::control_sim_anneal( save_pred = TRUE, save_workflow = TRUE, verbose = TRUE),\n    metrics = metrics\n  )\n\nxgb_params <- dep_models |>\n  extract_workflow(\"imputedhot_xgb\") |>\n  parameters() |>\n  update(\n    learn_rate = learn_rate(range = c(-1.2,-0.6)),\n    min_n = min_n(range = c(20,90))\n    )\n\nlgbm_params <- dep_models |> \n  extract_workflow(\"base_lgbm\") |> \n  parameters() |> \n  update(\n      min_n = min_n(range = c(10,100)),\n#      loss_reduction = loss_reduction(range = c(-1,0)),\n   #   mtry = mtry(range = c(27,unknown())),\n      tree_depth = tree_depth(range = c(9,45)),\n      learn_rate = learn_rate(range = c(-1,-0.6))\n         )\n\ndep_models <- dep_models |> \n  option_add(\n    param_info = lgbm_params,\n    id = \"base_lgbm\"\n  ) |> \n  option_add(\n    param_info = xgb_params,\n    id = \"imputedhot_xgb\"\n  ) |>\n   workflow_map(\"tune_sim_anneal\", resamples = folds, iter = 6, \n                metrics = metrics, verbose = TRUE)\n                   \n\nautoplot(dep_models) +\n  geom_text(aes(y = mean -1, label = wflow_id), angle = 90, hjust = 1)+\n  scale_y_continuous(expand = expansion(mult = c(0.5,0)))+\n  theme(legend.position = \"none\")\n\nrank_results(dep_models, rank_metric = \"mape\", select_best = TRUE) %>% \n   select(rank, mean, model, wflow_id, .config)\n\ndep_models %>%\n  dplyr::filter(grepl(\"xgb\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean)\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"imputedhot_xgb\") |> \n  autoplot() +\n  labs(title = \"XGB Hyperparameter Search\")\n\ndep_models %>%\n  dplyr::filter(grepl(\"lgbm\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean)\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"base_lgbm\") |> \n  autoplot() +\n  labs(title = \"LGBM Hyperparameter Search\")\n\n\ndep_stack <- stacks() %>%\n  add_candidates(dep_models) %>%\n  blend_predictions(  metric = metrics,\n      penalty = c(10^seq(-2.2, -0.8, 0.1)),\n      non_negative = TRUE,\n      control = tune::control_grid(allow_par = TRUE))\n\nautoplot(dep_stack)\n\nautoplot(dep_stack, type = \"members\")        \n                   \nautoplot(dep_stack, \"weights\")\n                   \nregression_fit <- dep_stack %>% \n    fit_members()\n\n\n```\n                   \n\n\n# Submission\n                   \n\n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6                    \n                   \naugment(regression_fit, train_df) %>% \n  ggplot(aes(.pred, num_sold)) +\n  geom_point(alpha = 0.1, shape = 20) +\n  geom_abline(color = \"green\") \n\n\nsubmit_df <-  augment(\n  regression_fit,\n  competition_df \n) %>%\n  transmute(id, num_sold = if_else(.pred < 0, 0, .pred))\n\nhead(submit_df)  %>% \n     knitr::kable()      \n\nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n```      ","metadata":{"_uuid":"8743fb2c-fd09-458e-a42d-a83e97a57efe","_cell_guid":"771d4a61-472d-49ca-b807-f971e7fb3116","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}