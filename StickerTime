{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85723,"databundleVersionId":10652996,"sourceType":"competition"}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"---\ntitle: \"Stickers\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict sticker counts.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm and the `poissonreg` interface to poisson models. The `stacks` package provides ensembling functions.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s5e1/\"\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2021-03-22\"))\n\ncores <- future::availableCores()\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n  orig_path <- stringr::str_c(here::here(\"data\"),\"/\")\n\n  cores <- future::availableCores(omit = 1)\n}\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\n  \nlibrary(finetune)\nlibrary(poissonreg)\n  \nlibrary(bonsai)  \nlibrary(stacks)\n # interface to lightgbm\n\n})\n\ntidymodels_prefer()\n\nconflicted::conflicts_prefer(purrr::is_null)\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(ggdark::dark_theme_minimal())\n\n\n```\n\n<p><img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F59561%2F48dd013ca109783d1abe5389c233f352%2Fkerneler.png?generation=1735682270467088&amp;alt=media\" alt=\"\"></p>\n\n## Load Data\n\nCredit to [Broccoli Beef for the World Bank feature idea](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349)\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\n\nraw_spec <- cols(\n id = col_integer(),\n date = col_date(),\n country = col_character(),\n store = col_character(),\n product = col_character(),\n num_sold = col_double()\n)\n\n\ncompetition_spec <- cols(\n id = col_integer(),\n date = col_date(),\n country = col_character(),\n store = col_character(),\n product = col_character()\n )\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   col_types = raw_spec,\n                   show_col_types = FALSE) \n\npreprocessor <- function(dataframe) {\n\n\n dataframe <- dataframe %>%\n    janitor::clean_names() %>%\n    \n    mutate(gdp_year = year(date)) |>\n\n    mutate(across(c(where(is.character)), \\(x) as.factor(x))) |>\n    \n    inner_join(WDI::WDI(\n      indicator = \"NY.GDP.PCAP.KD\",\n      country = c('CA', 'FI', 'IT', 'KE', 'NO', 'SG'),\n      start = 2010,\n      end = 2020\n    ) |> select(year, country, NY.GDP.PCAP.KD),\n    by = join_by(country == country, gdp_year == year)) |> \n    select(-gdp_year)\n  \n\nreturn(dataframe)\n}\n\nraw_df <- raw_df %>%\n          preprocessor() \n\ntst_df <- read_csv(str_c(path, \"test.csv\"),\n                   col_types = competition_spec,\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nfeatures <- raw_df %>%\n  select(-id, -num_sold) %>%\n  names()\n\n# because we already know the test set, let's remove the train set factor levels that do not correspond with anything on the test set\nfor (col in names(raw_df)) {\n    if (is.factor(raw_df[[col]]) & col != \"depression\") {\n      # Get levels in train and test dataframes\n      raw_levels <- levels(raw_df[[col]])\n      tst_levels <- levels(tst_df[[col]])\n      \n      # Identify levels in train not in test\n      new_levels <- setdiff(raw_levels, tst_levels)\n      \n      # Set these levels to NA in train dataframe\n      raw_df[[col]] <- factor(raw_df[[col]], levels = c(tst_levels, new_levels))\n      raw_df[[col]][raw_df[[col]] %in% new_levels] <- NA_character_\n    }\n  }\n\nall_df <-\n  bind_rows(\n    raw_df %>% mutate(source = \"train\") %>%\n      distinct(pick(all_of(features)), .keep_all = TRUE),\n    tst_df %>% mutate(source = \"test\")\n  )\n\ntrain_df <- all_df %>% \n  filter(source == \"train\") %>% \n  select(-source) |> \n  left_join(\n    expand_grid(\n      date = seq(min(raw_df$date), max(raw_df$date), by = \"day\"),\n      country = levels(raw_df$country),\n      store = levels(raw_df$store),\n      product = levels(raw_df$product)\n    )\n  )            \n\n```          \n\nQuite a few permutations of features and date in the training set have missing outcomes that exist over in the test set for competition. \n\nAt this early stage of the competition, we will fill them in with a poisson glmnet model.           \n\nIf this competition were handled as a multivariate time series on numeric predictors, we might a set of lag features or a difference in lags. \n\n```{r}\n#| label: median outcome\n           \n\ntrain_df |> \n  mutate(missing = if_else(is.na(num_sold),\"missing\",\"ok\")) |> \n  group_by(country, store, product) |>\n  count(missing) |> \n  pivot_wider(names_from = missing,\n              values_from = n) |> \n  replace_na(list(missing = 0, ok = 0)) |> \n  mutate(missing_prop = missing / (missing + ok)) |> \n  filter(missing_prop > 0) |> \n  arrange(desc(missing_prop))\n\n## lets run a quick glmnet poisson model to fill in those missing num_sold values\n           \npoisson_reg_glmnet_spec <-\n  poisson_reg(penalty = tune(), mixture = 0.01) %>%\n  set_engine('glmnet')\n\nfolds <- rsample::sliding_period(\n  train_df |> filter(!is.na(num_sold)),\n  date,\n  period = \"month\",\n  lookback = 60L,\n  assess_start = 1L,\n  assess_stop = 3L,\n  complete = TRUE,\n  step = 3L,\n  skip = 0L\n)\n\nlinear_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_df |> filter(!is.na(num_sold))) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n\n  step_date(date, label = TRUE,   features = c(\"dow\", \"year\",\"doy\"), keep_original_cols = FALSE) %>% \n\n  step_harmonic(\n  date_doy,  \n  frequency = c(1,7),\n  cycle_size =365,\n  keep_original_cols = TRUE\n  ) |> \n  \n  step_mutate(holiday_bump = factor(if_else(abs(date_doy_cos_1) > 0.98, \"holiday\",\"ordinary_day\"))) |> \n  \n  step_dummy(all_nominal_predictors()) |> \n  \n  step_interact(~ contains(\"store\"):contains(\"Kerneler\")) |> \n\n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\n\nwf <- workflow(linear_rec, poisson_reg_glmnet_spec)\n\nrs <- tune_grid(\n  wf,\n  folds,\n  grid = 5,\n  metrics = metric_set(mape)\n)\n\nautoplot(rs)\n\nlinear_fit <- fit(finalize_workflow(wf, select_best(rs, metric = \"mape\")), train_df |> filter(!is.na(num_sold)))\n\ntrain_df <- train_df |> \n  left_join(       \n       augment(linear_fit, train_df |> filter(is.na(num_sold))) |> transmute(id, num_sold = .pred),\n          by = join_by(id)\n  ) |> \n  mutate(num_sold = if_else(is.na(num_sold.x), num_sold.y, num_sold.x)) |> \n  select(-num_sold.x, -num_sold.y) \n\n```\n\n\n```{r}\n#| label: competition df\n\ncompetition_df <- all_df %>% \n  filter(source == \"test\") %>% \n  select(-source, -num_sold)\n\nnom_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\nLogical features: \n\n`r logical_features`\n\n\nSize of the combined train and competition datasets:\n\n`r nrow(all_df)`\n\nSize of the split made available to machine learning\n\n`r nrow(train_df)`\n\n<p><img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1034305%2Faa58ad81cfc4cf4f0762b2687de48896%2Fkaggle.jpg?generation=1562598537059919&alt=media\" alt=\"\"></p>\n          \n\n\n           \n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Nominal features\n\nExplore the distribution of outcome class by factor level, and the factor levels that exist in test that do not exist in training.\n\n\n```{r}\n#| label: nominal\n#| warning: false\n#| message: false\n#| fig.height: 9\n#| fig.width: 9\n\ntrain_df %>% \n  select(all_of(nom_features), num_sold) %>% \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-num_sold,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n    \n  filter(!is.na(num_sold)) %>% \n    \n  summarise(n = n(),\n            .by = c(num_sold, metric, value)) %>%\n      \n  mutate(value = tidytext::reorder_within(value, n, metric)) %>%\n    \n  ggplot(aes(x = n, y = value, fill = ggplot2::cut_number(num_sold, 5))) +\n  geom_col() +\n  tidytext::scale_y_reordered() +\n  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Nominal Feature Counts\",\n       fill = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\ntrain_df %>% \n  select(all_of(nom_features), num_sold) %>% \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-num_sold,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n    \n  ggplot(aes(x = num_sold, y = value)) +\n  geom_boxplot() +\n  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Nominal Feature Distributions\",\n       fill = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n```\n\n## Time Series\n\n```{r}\n#| label: date\n#| warning: false\n#| message: false\n#| fig.height: 16\n#| fig.width: 9\n\ntrain_df %>% \n  \n  select(all_of(nom_features), date, num_sold) |> \n  \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-c(num_sold,date),\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n    \n  summarise(mean_num_sold = mean(num_sold),\n            .by = c(date, metric, value)) %>%\n  \n  ggplot(aes(x = date, y = mean_num_sold, color = value)) +\n  geom_point() +\n#  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_grid(vars(metric, value), scales = \"free\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Mean Number of Stickers Sold Daily\",\n       fill = NULL, color = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n\n```\n\n\n\n\n\n## Counts of Missingness\n                  \n```{r}\n#| label: counts of missingness\n\ntrain_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n                   \ntrain_df %>% \n    select(all_of(features)) %>%\n    slice_sample(n = 2000) %>%\n    naniar::vis_miss()          \n\nnaniar::gg_miss_var(train_df %>% select(all_of(features)), \n                   facet = store)    \n\n                  \n```\n\n## Counts of Distinct\n                   \n               \n```{r}\n#| label: counts of distinct\n               \ntrain_df %>%\n  summarize(across(all_of(features), n_distinct)) %>%\n  pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct train\") |>\n  left_join(\n    tst_df %>%\n      summarize(across(all_of(features), n_distinct)) %>%\n      pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct test\")\n  ) %>% \n                   knitr::kable()\n\n\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct label?\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\n\nall_df %>%\n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>% \n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>% \n    count(is_duplicated) %>% \n                   knitr::kable()\n\n```\n                   \n\n\n\n## Target\n\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 6\n                  \ntrain_df %>% \n  ggplot(aes(num_sold)) +\n  geom_histogram(bins = 100) +\n  scale_x_log10(labels = scales::comma) +\n  labs(title = \"Stickers Sold for each Training Observation\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n\n\n```\n \n<img height=\"auto\" width=\"600\" alt=\"\" src=\"https://i.ibb.co/g6YM0gK/0-1.jpg\">\n                 \n                \n# Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n```{r}\n#| label: mape\n\nmetrics <- metric_set(mape)\n\n```\n\n## Recipes\n\n```{r}\n#| label: recipe\n#| warning: false\n#| message: false\n#| fig.width: 6\n                   \n                   \nlgbm_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  \n  step_date(date, label = TRUE,   features = c(\"dow\", \"month\", \"year\",\"doy\"), keep_original_cols = FALSE) %>% \n  \n  step_harmonic(\n  date_doy,  \n  frequency = c(1,7),\n  cycle_size =365,\n  keep_original_cols = TRUE\n  ) |> \n  \n  step_mutate(holiday_bump = factor(if_else(abs(date_doy_cos_1) > 0.98, \"holiday\",\"ordinary_day\"))) |> \n  \n  step_zv(all_predictors())\n\n\nlinear_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n\n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |> \n\n  step_date(date, label = TRUE,   features = c(\"dow\", \"year\",\"doy\"), keep_original_cols = FALSE) %>% \n  \n  step_harmonic(\n  date_doy,  \n  frequency = c(1,7),\n  cycle_size =365,\n  keep_original_cols = TRUE\n  ) |> \n  \n  step_mutate(holiday_bump = factor(if_else(abs(date_doy_cos_1)> 0.98, \"holiday\",\"ordinary_day\"))) |> \n  \n  step_dummy(all_nominal_predictors()) |> \n  \n  step_interact(~ contains(\"store\"):contains(\"Kerneler\")) |> \n\n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nxgb_rec <- recipe(formula(paste0(\"num_sold ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |> \n  \n  step_date(date, label = TRUE,   features = c(\"dow\", \"year\",\"doy\"), keep_original_cols = FALSE) %>% \n  \n  step_harmonic(\n  date_doy,  \n  frequency = c(1,7),\n  cycle_size =365,\n  keep_original_cols = TRUE\n  ) |> \n\n  step_mutate(holiday_bump = factor(if_else(abs(date_doy_cos_1) > 0.98, \"holiday\",\"ordinary_day\"))) |> \n  \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> \n\n  step_interact(~ contains(\"store\"):contains(\"Kerneler\")) |> \n\n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nfolds <- rsample::sliding_period(\n  train_df,\n  date,\n  period = \"month\",\n  lookback = 60L,\n  assess_start = 1L,\n  assess_stop = 3L,\n  complete = TRUE,\n  step = 3L,\n  skip = 0L\n)\n\nfolds |>\n  tidy() |>\n  ggplot(aes(x = Resample, y = factor(Row), fill = Data)) +\n  geom_tile() +\n  labs(y = \"Date\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\nlgbm_rec |> \n    prep() |> \n    bake(new_data = NULL) |> \n  summarize(num_sold = mean(num_sold), sin_doy = mean(date_doy_sin_1), .by = c(date_doy, product, country)) |>  \n  ggplot(aes(sin_doy, num_sold, color = product)) + \n  geom_point() + \n  facet_wrap(vars(factor(country))) +\n  labs(title = \"Harmonic for 365 Day Cycle / Frequency of 1\")\n\nlgbm_rec |> \n    prep() |> \n    bake(new_data = NULL) |> \n  summarize(num_sold = mean(num_sold), sin_doy = mean(date_doy_sin_1), .by = c(date_doy, store, product)) |>  \n  ggplot(aes(sin_doy, num_sold, color = product)) + \n  geom_point() + \n  facet_wrap(vars(factor(store)))+\n  labs(title = \"Harmonic for 365 Day Cycle / Frequency of 1\")\n\nlgbm_rec |> \n    prep() |> \n    bake(new_data = NULL) |> \n  summarize(num_sold = mean(num_sold), cos_doy = mean(date_doy_cos_1), .by = c(date_doy, store, product)) |>  \n  ggplot(aes(cos_doy, num_sold, color = product)) + \n  geom_point() + \n  facet_wrap(vars(factor(store))) +\n  labs(title = \"Harmonic for 365 Day Cycle / Frequency of 1\")\n\nlgbm_rec |> \n    prep() |> \n    bake(new_data = NULL) |>\n  summarize(num_sold = mean(num_sold), sin_doy = mean(date_doy_sin_2), .by = c(date_doy, product, country)) |>  \n  ggplot(aes(sin_doy, num_sold, color = product)) + \n  geom_point() + \n  facet_wrap(vars(factor(country))) +\n  labs(title = \"Harmonic for 365 Day Cycle / Frequency of 7\")\n\nlgbm_rec |> \n    prep() |> \n    bake(new_data = NULL) |> \n  summarize(num_sold = mean(num_sold), sin_doy = mean(date_doy_sin_2), .by = c(date_doy, store, product)) |>  \n  ggplot(aes(sin_doy, num_sold, color = product)) + \n  geom_point() + \n  facet_wrap(vars(factor(store))) +\n  labs(title = \"Harmonic for 365 Day Cycle / Frequency of 7\")\n\nlgbm_rec |> \n    prep() |> \n    bake(new_data = NULL) |> \n  summarize(num_sold = mean(num_sold), cos_doy = mean(date_doy_cos_2), .by = c(date_doy, store, product)) |>  \n  ggplot(aes(cos_doy, num_sold, color = product)) + \n  geom_point() + \n  facet_wrap(vars(factor(store))) +\n  labs(title = \"Harmonic for 365 Day Cycle / Frequency of 7\")\n\n```\n\n## Poisson Model\n\n```{r}\n#| label: poisson\n#| warning: false\n#| message: false\n#| fig.height: 6\n\npoisson_reg_glmnet_spec <-\n  poisson_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine('glmnet')\n\nwf <- workflow(linear_rec, poisson_reg_glmnet_spec)\n\nrs <- tune_grid(\n  wf,\n  folds,\n  grid = 5,\n  metrics = metrics\n)\n\nautoplot(rs)\n\nlinear_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)\n\nvip::vip(linear_fit, num_features = 30L)\n\n```\n\n## XGB\n\n```{r}\n#| label: xgb\n#| warning: false\n#| message: false\n#| fig.height: 6\n\nboost_tree_xgb_spec <- \n  boost_tree(\n    trees = 80L,\n    min_n = tune(),\n    learn_rate = tune()\n  ) %>% \n  set_engine(engine = \"xgboost\", nthread = cores) %>%\n  set_mode(mode = \"regression\")    \n\nwf <- workflow(xgb_rec,boost_tree_xgb_spec)\n\nrs <- tune_grid(\n  wf,\n  folds,\n  grid = 5,\n  metrics = metrics\n)\n\nautoplot(rs)\n\nxgb_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)\n\nvip::vip(xgb_fit, num_features = 30L)\n\n```\n\n## LGBM\n\n```{r}\n#| label: lgbm\n#| warning: false\n#| message: false\n#| fig.height: 6\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 80L,\n   tree_depth = tune(),\n   learn_rate =  0.1,\n   min_n = tune(),\n#  loss_reduction = tune()\n #  mtry = tune()\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n             num_threads = cores\n             ) %>%\n  set_mode(mode = \"regression\") \n\nwf <- workflow(lgbm_rec,boost_tree_lgbm_spec)\n\nrs <- tune_grid(\n  wf,\n  folds,\n  grid = 5,\n  metrics = metrics\n)\n\nautoplot(rs)\n\nlgbm_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)\n\nvip::vip(lgbm_fit, num_features = 30L)\n\n```\n\n\n## Workflowsets\n\n```{r}\n#| label: workflowsets\n#| warning: false\n#| message: false\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 500L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune()\n#   loss_reduction = tune()\n #  mtry = tune()\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n             num_threads = cores\n             ) %>%\n  set_mode(mode = \"regression\") \n\n\nboost_tree_xgb_spec <- \n  boost_tree(\n    trees = 500L,\n    min_n = tune(),\n    learn_rate = tune()\n  ) %>% \n  set_engine(engine = \"xgboost\", nthread = cores) %>%\n  set_mode(mode = \"regression\")    \n\n\ndep_models <- \n   workflow_set(\n      preproc = list(imputedhot = xgb_rec,\n                     linear = linear_rec,\n                     base = lgbm_rec),\n      models = list(xgb = boost_tree_xgb_spec,\n                    poisson = poisson_reg_glmnet_spec,\n                    lgbm = boost_tree_lgbm_spec),\n      cross = FALSE\n   ) %>% \n  option_add_parameters() |> \n  option_add(\n    control = finetune::control_sim_anneal( save_pred = TRUE, save_workflow = TRUE, verbose = TRUE),\n    metrics = metrics\n  )\n\nxgb_params <- dep_models |>\n  extract_workflow(\"imputedhot_xgb\") |>\n  parameters() |>\n  update(\n    learn_rate = learn_rate(range = c(-1.4,-0.9)),\n    min_n = min_n(range = c(7,90))\n    )\n\nlgbm_params <- dep_models |> \n  extract_workflow(\"base_lgbm\") |> \n  parameters() |> \n  update(\n      min_n = min_n(range = c(10,100)),\n#      loss_reduction = loss_reduction(range = c(-1,0)),\n   #   mtry = mtry(range = c(27,unknown())),\n      num_leaves = num_leaves(range = c(70,200)),\n      tree_depth = tree_depth(range = c(9,100)),\n      learn_rate = learn_rate(range = c(-1.3,-1.0))\n         )\n\ndep_models <- dep_models |> \n  option_add(\n    param_info = lgbm_params,\n    id = \"base_lgbm\"\n  ) |> \n  option_add(\n    param_info = xgb_params,\n    id = \"imputedhot_xgb\"\n  ) |>\n   workflow_map(\"tune_sim_anneal\", resamples = folds, iter = 12, \n                metrics = metrics, verbose = TRUE)\n                   \n\nautoplot(dep_models) +\n  geom_text(aes(y = mean -5, label = wflow_id), angle = 90, hjust = 1)+\n  scale_y_continuous(expand = expansion(mult = c(0.5,0)))+\n  theme(legend.position = \"none\")\n\nrank_results(dep_models, rank_metric = \"mape\", select_best = TRUE) %>% \n   select(rank, mean, model, wflow_id, .config)\n\ndep_models %>%\n  dplyr::filter(grepl(\"xgb\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean)\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"imputedhot_xgb\") |> \n  autoplot() +\n  labs(title = \"XGB Hyperparameter Search\")\n\ndep_models %>%\n  dplyr::filter(grepl(\"lgbm\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean)\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"base_lgbm\") |> \n  autoplot() +\n  labs(title = \"LGBM Hyperparameter Search\")\n\n\ndep_stack <- stacks() %>%\n  add_candidates(dep_models) %>%\n  blend_predictions(  metric = metrics,\n      penalty = c(10^seq(-2.7, -1.1, 0.1)),\n      non_negative = TRUE,\n      control = tune::control_grid(allow_par = TRUE))\n\nautoplot(dep_stack)\n\nautoplot(dep_stack, type = \"members\")        \n                   \nautoplot(dep_stack, \"weights\")\n                   \nregression_fit <- dep_stack %>% \n    fit_members()\n\n\n```\n\n# Submission\n\nThe ensemble regression model is applied to labeled training data one last time to explore the greatest residuals, and then fit to competition / test data for submission to kaggle.                   \n\n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6                    \n                   \ntrain_result <- augment(regression_fit, train_df) %>% \n  mutate(residual = abs(num_sold - .pred))\n\ntrain_result |> \n  group_by(country, store, product) |> \n  mape(num_sold, .pred) |> \n  arrange(desc(.estimate))\n\ntrain_result |> \n  ggplot(aes(.pred, residual , fill = country)) +\n  geom_hex()\n\ntrain_result |> \n  ggplot(aes(.pred, residual , fill = store)) +\n  geom_hex()\n\ntrain_result |> \n  ggplot(aes(.pred, residual , fill = product)) +\n  geom_hex() \n                   \nsubmit_df <-  augment(\n  regression_fit,\n  competition_df \n) %>%\n  transmute(id, num_sold = if_else(.pred < 0, 0, .pred))\n\nhead(submit_df)  %>% \n     knitr::kable()      \n\nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n```      ","metadata":{"_uuid":"7b35a9f4-3d29-4cf8-93ce-761a5e41a13f","_cell_guid":"6ae0e4d6-ab95-47bf-b7c0-74af7b2bb8f4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}