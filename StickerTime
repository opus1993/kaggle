---
title: "Stickers"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---
  
# Introduction  {.tabset .tabset-fade .tabset-pills}

The goal of this competition is to predict sticker counts.

My notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.

Lets dive right in.

The Kaggle kernels have many of the common r packages built in.  

## Load libraries

In addition to `tidymodels` we will load the `bonsai` interface to lightgbm and `brulee` interface to torch.

```{r }
#| label: setup
#| warning: false
#| message: false

if (dir.exists("/kaggle")){
  path <- "/kaggle/input/playground-series-s5e1/"

options(repos = c(CRAN = "https://packagemanager.posit.co/cran/2021-03-22"))

cores <- future::availableCores()

} else {
  path <- stringr::str_c(here::here("data"),"/")
  orig_path <- stringr::str_c(here::here("data"),"/")

  cores <- future::availableCores(omit = 1)
}
 
suppressPackageStartupMessages({
library(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages
library(tidymodels) # metapackage see https://www.tidymodels.org/
  
library(finetune)
library(poissonreg)
  
library(bonsai)  
library(stacks)
 # interface to lightgbm

})

tidymodels_prefer()

conflicted::conflicts_prefer(purrr::is_null)

options(tidymodels.dark = TRUE)

theme_set(ggdark::dark_theme_minimal())


```

<p><img src="https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F59561%2F48dd013ca109783d1abe5389c233f352%2Fkerneler.png?generation=1735682270467088&amp;alt=media" alt=""></p>

## Load Data

Credit to [Broccoli Beef for the World Bank feature idea](https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349)

```{r }
#| label: load data
#| warning: false
#| message: false


raw_spec <- cols(
 id = col_integer(),
 date = col_date(),
 country = col_character(),
 store = col_character(),
 product = col_character(),
 num_sold = col_double()
)


competition_spec <- cols(
 id = col_integer(),
 date = col_date(),
 country = col_character(),
 store = col_character(),
 product = col_character()
 )

raw_df <- read_csv(str_c(path, "train.csv"),
                   col_types = raw_spec,
                   show_col_types = FALSE) 

preprocessor <- function(dataframe) {


 dataframe <- dataframe %>%
    janitor::clean_names() %>%
    
    mutate(gdp_year = year(date)) |>

    mutate(across(c(where(is.character)), \(x) as.factor(x))) |>
    
    inner_join(WDI::WDI(
      indicator = "NY.GDP.PCAP.KD",
      country = c('CA', 'FI', 'IT', 'KE', 'NO', 'SG'),
      start = 2010,
      end = 2020
    ) |> select(year, country, NY.GDP.PCAP.KD),
    by = join_by(country == country, gdp_year == year)) |> 
    select(-gdp_year)
  

return(dataframe)
}

raw_df <- raw_df %>%
          preprocessor() 

tst_df <- read_csv(str_c(path, "test.csv"),
                   col_types = competition_spec,
                   show_col_types = FALSE)  %>% 
  preprocessor() 

features <- raw_df %>%
  select(-id, -num_sold) %>%
  names()

# because we already know the test set, let's remove the train set factor levels that do not correspond with anything on the test set
for (col in names(raw_df)) {
    if (is.factor(raw_df[[col]]) & col != "depression") {
      # Get levels in train and test dataframes
      raw_levels <- levels(raw_df[[col]])
      tst_levels <- levels(tst_df[[col]])
      
      # Identify levels in train not in test
      new_levels <- setdiff(raw_levels, tst_levels)
      
      # Set these levels to NA in train dataframe
      raw_df[[col]] <- factor(raw_df[[col]], levels = c(tst_levels, new_levels))
      raw_df[[col]][raw_df[[col]] %in% new_levels] <- NA_character_
    }
  }

all_df <-
  bind_rows(
    raw_df %>% mutate(source = "train") %>%
      distinct(pick(all_of(features)), .keep_all = TRUE),
    tst_df %>% mutate(source = "test")
  )

train_df <- all_df %>% 
  filter(source == "train") %>% 
  select(-source) |> 
  left_join(
    expand_grid(
      date = seq(min(raw_df$date), max(raw_df$date), by = "day"),
      country = levels(raw_df$country),
      store = levels(raw_df$store),
      product = levels(raw_df$product)
    )
  )            

```          

Quite a few permutations of features and date have missing outcomes. We will fill them in as the minimum by date.           

```{r}
#| label: median outcome
           
train_df |> 
  mutate(missing = if_else(is.na(num_sold),"missing","ok")) |> 
  group_by(country, store, product) |>
  count(missing) |> 
  pivot_wider(names_from = missing,
              values_from = n) |> 
  replace_na(list(missing = 0, ok = 0)) |> 
  mutate(missing_prop = missing / (missing + ok)) |> 
  filter(missing_prop > 0) |> 
  arrange(desc(missing_prop))

train_df <- train_df |> 
    # fill in 64 missing Kernelers with the median, across countries
    mutate(num_sold = if_else(is.na(num_sold) & str_detect(product, "Kerneler"), median(num_sold, na.rm = TRUE), num_sold),
         .by = c(date, product, store)) |> 
    # fill in 380 + 646 Premium Store with median, across countries
    mutate(num_sold = if_else(is.na(num_sold) & str_detect(store, "Premium"), median(num_sold, na.rm = TRUE), num_sold),
         .by = c(date, product, store)) |> 
    # fill in 1358 + 1308 Premium Store with median, across countries
    mutate(num_sold = if_else(is.na(num_sold) & str_detect(store, "for Less"), median(num_sold, na.rm = TRUE), num_sold),
         .by = c(date, product, store)) |> 
    # fill in the rest with the median for that date
    mutate(num_sold = if_else(is.na(num_sold), median(num_sold, na.rm = TRUE), num_sold),
         .by = c(date, product, country))  
    

```


```{r}
#| label: competition df

competition_df <- all_df %>% 
  filter(source == "test") %>% 
  select(-source, -num_sold)

nom_features <- train_df %>%
  select(all_of(features)) %>%
  select(where(is.character), where(is.factor)) %>%
  names() 

logical_features <- train_df %>%
  select(all_of(features)) %>%
  select(where(is.logical)) %>%
  names() 

num_features <- train_df %>%
  select(all_of(features)) %>%
  select(where(is.numeric)) %>%
  names()

```

Nominal features:

`r nom_features`

Numeric features: 

`r num_features`

Logical features: 

`r logical_features`


Size of the combined train and competition datasets:

`r nrow(all_df)`

Size of the split made available to machine learning

`r nrow(train_df)`


# EDA {.tabset .tabset-fade .tabset-pills}

## Nominal features

Explore the distribution of outcome class by factor level, and the factor levels that exist in test that do not exist in training.


```{r}
#| label: nominal
#| warning: false
#| message: false
#| fig.height: 9
#| fig.width: 9

train_df %>% 
  select(all_of(nom_features), num_sold) %>% 
  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%
  pivot_longer(-num_sold,
    names_to = "metric",
    values_to = "value"
  ) %>%
    
  filter(!is.na(num_sold)) %>% 
    
  summarise(n = n(),
            .by = c(num_sold, metric, value)) %>%
      
  mutate(value = tidytext::reorder_within(value, n, metric)) %>%
    
  ggplot(aes(x = n, y = value, fill = ggplot2::cut_number(num_sold, 5))) +
  geom_col() +
  tidytext::scale_y_reordered() +
  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +
  facet_wrap(vars(metric), scales = "free", ncol = 2) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
       legend.position = "bottom") +
  labs(title = "Nominal Feature Counts",
       fill = NULL,
       caption = "Data: Kaggle | Visual: Jim Gruman")

train_df %>% 
  select(all_of(nom_features), num_sold) %>% 
  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%
  pivot_longer(-num_sold,
    names_to = "metric",
    values_to = "value"
  ) %>%
    
  ggplot(aes(x = num_sold, y = value)) +
  geom_boxplot() +
  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +
  facet_wrap(vars(metric), scales = "free", ncol = 2) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
       legend.position = "bottom") +
  labs(title = "Nominal Feature Distributions",
       fill = NULL,
       caption = "Data: Kaggle | Visual: Jim Gruman")

```

## Time Series

```{r}
#| label: date
#| warning: false
#| message: false
#| fig.height: 16
#| fig.width: 9

train_df %>% 
  
  select(all_of(nom_features), date, num_sold) |> 
  
  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%
  pivot_longer(-c(num_sold,date),
    names_to = "metric",
    values_to = "value"
  ) %>%
    
  summarise(n = sum(num_sold),
            .by = c(num_sold, date, metric, value)) %>%
  
 
  ggplot(aes(x = date, y = n, color = value)) +
  geom_point(alpha = 0.1) +
#  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +
  facet_grid(vars(metric, value), scales = "free") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
       legend.position = "bottom") +
  labs(title = "Number of Stickers Sold",
       fill = NULL, color = NULL,
       caption = "Data: Kaggle | Visual: Jim Gruman")


```





## Counts of Missingness
                  
```{r}
#| label: counts of missingness

train_df %>% 
  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% 
  pivot_longer(everything(),
              names_to = "feature",
              values_to = "Count of Missing") %>% 
                   knitr::kable()
                   
train_df %>% 
    select(all_of(features)) %>%
    slice_sample(n = 2000) %>%
    naniar::vis_miss()          

naniar::gg_miss_var(train_df %>% select(all_of(features)), 
                   facet = store)    

                  
```

## Counts of Distinct
                   
               
```{r}
#| label: counts of distinct
               
train_df %>%
  summarize(across(all_of(features), n_distinct)) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "Count of distinct train") |>
  left_join(
    tst_df %>%
      summarize(across(all_of(features), n_distinct)) %>%
      pivot_longer(everything(), names_to = "feature", values_to = "Count of distinct test")
  ) %>% 
                   knitr::kable()


               
```

## Duplicated

Is this competition transaction already in the training data with a correct label?

```{r}
#| label: duplicates
#| warning: false
#| message: false

all_df %>%
    group_by_at(features) %>%
    mutate(num_dups = n(),
           dup_id = row_number()) %>% 
    ungroup() %>%
    group_by(source) %>%
    mutate(is_duplicated = dup_id > 1) %>% 
    count(is_duplicated) %>% 
                   knitr::kable()

```
                   



## Target


```{r}
#| label: outcome 
#| warning: false
#| message: false
#| fig.width: 6
                  
train_df %>% 
  ggplot(aes(num_sold)) +
  geom_histogram(bins = 100) +
  scale_x_log10(labels = scales::comma) +
  labs(title = "Stickers Sold for each Training Observation",
       caption = "Data: Kaggle.com | Visual: Jim Gruman")



```
 
           
                
# Machine Learning {.tabset .tabset-fade .tabset-pills}


```{r}
#| label: mape

metrics <- metric_set(mape)

```

## Recipes

```{r}
#| label: recipe
#| warning: false
#| message: false
#| fig.width: 6
                   
                   
lgbm_rec <- recipe(formula(paste0("num_sold ~ ", str_c(features, collapse = " + "))), data = train_df) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  
  step_date(date, label = TRUE,   features = c("dow", "month", "year","doy"), keep_original_cols = FALSE) %>% 
  
  step_mutate(sin_doy = sin(2 * pi * date_doy/365),
              cos_doy = cos(pi * date_doy/365)) |> 
  
  step_mutate(holiday_bump = factor(if_else(cos_doy > 0.98, "holiday","ordinary_day"))) |> 
  
  step_zv(all_predictors())


linear_rec <- recipe(formula(paste0("num_sold ~ ", str_c(features, collapse = " + "))), data = train_df) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%

  step_impute_median(all_numeric_predictors()) |> 
  step_impute_mode(all_nominal_predictors()) |> 

  step_date(date, label = TRUE,   features = c("dow", "year","doy"), keep_original_cols = FALSE) %>% 
  
  step_mutate(sin_doy = sin(2 *pi * date_doy/365),
              cos_doy = cos(pi * date_doy/365)) |> 
  
  step_mutate(holiday_bump = factor(if_else(cos_doy > 0.98, "holiday","ordinary_day"))) |> 
  
  step_dummy(all_nominal_predictors()) |> 
  
  step_interact(~ contains("store"):contains("Kerneler")) |> 

  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

xgb_rec <- recipe(formula(paste0("num_sold ~ ", str_c(features, collapse = " + "))), data = train_df) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  
  step_impute_median(all_numeric_predictors()) |> 
  step_impute_mode(all_nominal_predictors()) |> 
  
  step_date(date, label = TRUE,   features = c("dow", "year","doy"), keep_original_cols = FALSE) %>% 
  
  step_mutate(sin_doy = sin(2 *pi * date_doy/365),
              cos_doy = cos(pi * date_doy/365)) |> 

  step_mutate(holiday_bump = factor(if_else(abs(cos_doy) > 0.98, "holiday","ordinary_day"))) |> 
  
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> 

  step_interact(~ contains("store"):contains("Kerneler")) |> 

  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

folds <- rsample::sliding_period(
  train_df,
  date,
  period = "month",
  lookback = 60L,
  assess_start = 1L,
  assess_stop = 3L,
  complete = TRUE,
  step = 3L,
  skip = 0L
)

folds |>
  tidy() |>
  ggplot(aes(x = Resample, y = factor(Row), fill = Data)) +
  geom_tile() +
  labs(y = "Date") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

lgbm_rec |> 
    prep() |> 
    bake(new_data = NULL) |> 
  summarize(num_sold = mean(num_sold), .by = c(date_doy, date_year, product)) |>  
  ggplot(aes(date_doy, num_sold, color = product)) + 
  geom_point() + 
  facet_wrap(vars(factor(date_year)))

lgbm_rec |> 
    prep() |> 
    bake(new_data = NULL) |> 
  summarize(num_sold = mean(num_sold), sin_doy = mean(sin_doy), .by = c(date_doy, store, product)) |>  
  ggplot(aes(sin_doy, num_sold, color = product)) + 
  geom_point() + 
  facet_wrap(vars(factor(store)))

lgbm_rec |> 
    prep() |> 
    bake(new_data = NULL) |> 
  summarize(num_sold = mean(num_sold), cos_doy = mean(cos_doy), .by = c(date_doy, store, product)) |>  
  ggplot(aes(cos_doy, num_sold, color = product)) + 
  geom_point() + 
  facet_wrap(vars(factor(store)))
                   
```

## Poisson Model

```{r}
#| label: poisson
#| warning: false
#| message: false
#| fig.height: 6

poisson_reg_glmnet_spec <-
  poisson_reg(penalty = tune(), mixture = tune()) %>%
  set_engine('glmnet')

wf <- workflow(linear_rec, poisson_reg_glmnet_spec)

rs <- tune_grid(
  wf,
  folds,
  grid = 6,
  metrics = metrics
)

autoplot(rs)

linear_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)

vip::vip(linear_fit, num_features = 30L)

```

## XGB

```{r}
#| label: xgb
#| warning: false
#| message: false
#| fig.height: 6

boost_tree_xgb_spec <- 
  boost_tree(
    trees = 80L,
    min_n = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine(engine = "xgboost", nthread = cores) %>%
  set_mode(mode = "regression")    

wf <- workflow(xgb_rec,boost_tree_xgb_spec)

rs <- tune_grid(
  wf,
  folds,
  grid = 6,
  metrics = metrics
)

autoplot(rs)

xgb_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)

vip::vip(xgb_fit, num_features = 30L)

```

## LGBM

```{r}
#| label: lgbm
#| warning: false
#| message: false
#| fig.height: 6

boost_tree_lgbm_spec <- 
  boost_tree(
    trees = 80L,
   tree_depth = tune(),
   learn_rate =  0.1,
   min_n = tune(),
#  loss_reduction = tune()
 #  mtry = tune()
  ) %>% 
  set_engine(engine = "lightgbm",
             is_unbalance = TRUE,
             num_leaves = tune(),
             num_threads = cores
             ) %>%
  set_mode(mode = "regression") 

wf <- workflow(lgbm_rec,boost_tree_lgbm_spec)

rs <- tune_grid(
  wf,
  folds,
  grid = 6,
  metrics = metrics
)

autoplot(rs)

lgbm_fit <- fit(finalize_workflow(wf, select_best(rs)), train_df)

vip::vip(lgbm_fit, num_features = 30L)

```


## Workflowsets

```{r}
#| label: workflowsets
#| warning: false
#| message: false

boost_tree_lgbm_spec <- 
  boost_tree(
    trees = 220L,
   tree_depth = tune(),
   learn_rate =  tune(),
   min_n = tune()
#   loss_reduction = tune()
 #  mtry = tune()
  ) %>% 
  set_engine(engine = "lightgbm",
             is_unbalance = TRUE,
             num_leaves = tune(),
             num_threads = cores
             ) %>%
  set_mode(mode = "regression") 


boost_tree_xgb_spec <- 
  boost_tree(
    trees = 220L,
    min_n = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine(engine = "xgboost", nthread = cores) %>%
  set_mode(mode = "regression")    


dep_models <- 
   workflow_set(
      preproc = list(imputedhot = xgb_rec,
                     linear = linear_rec,
                     base = lgbm_rec),
      models = list(xgb = boost_tree_xgb_spec,
                    poisson = poisson_reg_glmnet_spec,
                    lgbm = boost_tree_lgbm_spec),
      cross = FALSE
   ) %>% 
  option_add_parameters() |> 
  option_add(
    control = finetune::control_sim_anneal( save_pred = TRUE, save_workflow = TRUE, verbose = TRUE),
    metrics = metrics
  )

xgb_params <- dep_models |>
  extract_workflow("imputedhot_xgb") |>
  parameters() |>
  update(
    learn_rate = learn_rate(range = c(-1.2,-0.6)),
    min_n = min_n(range = c(7,90))
    )

lgbm_params <- dep_models |> 
  extract_workflow("base_lgbm") |> 
  parameters() |> 
  update(
      min_n = min_n(range = c(10,100)),
#      loss_reduction = loss_reduction(range = c(-1,0)),
   #   mtry = mtry(range = c(27,unknown())),
      num_leaves = num_leaves(range = c(10,200)),
      tree_depth = tree_depth(range = c(9,100)),
      learn_rate = learn_rate(range = c(-1.3,-1.0))
         )

dep_models <- dep_models |> 
  option_add(
    param_info = lgbm_params,
    id = "base_lgbm"
  ) |> 
  option_add(
    param_info = xgb_params,
    id = "imputedhot_xgb"
  ) |>
   workflow_map("tune_sim_anneal", resamples = folds, iter = 12, 
                metrics = metrics, verbose = TRUE)
                   

autoplot(dep_models) +
  geom_text(aes(y = mean -5, label = wflow_id), angle = 90, hjust = 1)+
  scale_y_continuous(expand = expansion(mult = c(0.5,0)))+
  theme(legend.position = "none")

rank_results(dep_models, rank_metric = "mape", select_best = TRUE) %>% 
   select(rank, mean, model, wflow_id, .config)

dep_models %>%
  dplyr::filter(grepl("xgb", wflow_id)) %>%
  mutate(metrics = map(result, collect_metrics)) %>%
  dplyr::select(wflow_id, metrics) %>%
  tidyr::unnest(cols = metrics) |> 
  arrange(mean)

dep_models |> 
  workflowsets::extract_workflow_set_result("imputedhot_xgb") |> 
  autoplot() +
  labs(title = "XGB Hyperparameter Search")

dep_models %>%
  dplyr::filter(grepl("lgbm", wflow_id)) %>%
  mutate(metrics = map(result, collect_metrics)) %>%
  dplyr::select(wflow_id, metrics) %>%
  tidyr::unnest(cols = metrics) |> 
  arrange(mean)

dep_models |> 
  workflowsets::extract_workflow_set_result("base_lgbm") |> 
  autoplot() +
  labs(title = "LGBM Hyperparameter Search")


dep_stack <- stacks() %>%
  add_candidates(dep_models) %>%
  blend_predictions(  metric = metrics,
      penalty = c(10^seq(-2.2, -0.8, 0.1)),
      non_negative = TRUE,
      control = tune::control_grid(allow_par = TRUE))

autoplot(dep_stack)

autoplot(dep_stack, type = "members")        
                   
autoplot(dep_stack, "weights")
                   
regression_fit <- dep_stack %>% 
    fit_members()


```
                   


# Submission
                   

```{r}
#| label: submission
#| warning: false
#| message: false
#| fig.height: 6
#| fig.width: 6                    
                   
train_result <- augment(regression_fit, train_df) %>% 
  mutate(residual = abs(num_sold - .pred))

train_result |> 
  mape(num_sold, .pred)

train_result |> 
  ggplot(aes(.pred, residual , fill = country)) +
  geom_hex()

train_result |> 
  ggplot(aes(.pred, residual , fill = store)) +
  geom_hex()

train_result |> 
  ggplot(aes(.pred, residual , fill = product)) +
  geom_hex() 
                   
submit_df <-  augment(
  regression_fit,
  competition_df 
) %>%
  transmute(id, num_sold = if_else(.pred < 0, 0, .pred))

head(submit_df)  %>% 
     knitr::kable()      

submit_df  %>% 
  write_csv("submission.csv")
```      