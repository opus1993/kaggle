{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"}],"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jimgruman/flooding-probability?scriptVersionId=175484694\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"---\ntitle: \"Regression with a Flood Prediction Dataset\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict the predict the probability of a region flooding.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nIf you fork this on kaggle, be sure to choose the kernel Environment setting for \"Always use latest environment\"\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\"))\n\n#install.packages(\"tidymodels\", quiet = TRUE, verbose = FALSE, dependencies = TRUE)\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\n  \nlibrary(bonsai)\n    \n})\n\ntidymodels_prefer()\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(cowplot::theme_minimal_grid())\n\n```\n\n\n## Interchangeability\n\nI prefer to be able to run the same code locally and on a Kaggle kernel.\n\n```{r}\n#| label: interchangeability\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s4e5/\"\n} else {\n  path <- str_c(here::here(\"data\"),\"/\")\n}\n\npath\n\n```\n\n## Load Data\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\nfuture::plan(\"multisession\", workers = future::availableCores())\n\npreprocessor <- function(dataframe) {\n    \nmax_col_names <- colnames(dataframe)[max.col(select(dataframe, -id), ties.method = \"first\") + 1]\n\ndataframe$max_predictor <- max_col_names    \n    \ndataframe <- dataframe %>%\n    janitor::clean_names() %>%\n    mutate(across(c(where(is.character)), ~ as.factor(.x)), ) %>%\n#    mutate(flood_median = furrr::future_pmap_dbl(select(., monsoon_intensity:political_factors), .f = lift_vd(median))) %>%\n    mutate(flood_mean = furrr::future_pmap_dbl(select(., monsoon_intensity:political_factors), .f = lift_vd(mean))) %>%\n    mutate(flood_max = furrr::future_pmap_dbl(select(., monsoon_intensity:political_factors), .f = lift_vd(max))) %>%\n    mutate(flood_total = furrr::future_pmap_dbl(select(., monsoon_intensity:political_factors), .f = lift_vd(sum))) %>%\n    mutate(flood_max_prop = flood_max / flood_total)\n\n# dataframe$flood_0 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 0)\n# dataframe$flood_1 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 1)\n# dataframe$flood_2 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 2)\n# dataframe$flood_3 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 3)\n# dataframe$flood_4 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 4)\ndataframe$flood_5 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 5)\n# dataframe$flood_6 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 6)\n# dataframe$flood_7 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 7)\n# dataframe$flood_8 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 8)\n# dataframe$flood_9 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 9)\n# dataframe$flood_10 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 10)\n# dataframe$flood_11 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 11)\n# dataframe$flood_12 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 12)\n# dataframe$flood_13 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 13)\n# dataframe$flood_14 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 14)\n# dataframe$flood_15 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 15)\n# dataframe$flood_16 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) == 16)\n    \n# dataframe$flood_over10 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) > 10)\n# dataframe$flood_over08 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) > 8)\ndataframe$flood_over06 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) > 6)\n# dataframe$flood_over04 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) > 4)\n# dataframe$flood_over02 <- rowSums(select(dataframe,-id, -starts_with(\"flood\"), -max_predictor) > 2)\n    \nreturn(dataframe)\n}\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   show_col_types = FALSE) %>%\n\n  preprocessor() %>% \n  mutate(outcome_residual = flood_mean/10 - flood_probability)\n\nfeatures <- raw_df %>%\n  select(-id, -flood_probability, -outcome_residual, -flood_mean) %>%\n  names()\n\nraw_df <- raw_df %>% \n  distinct(pick(all_of(features)), .keep_all = TRUE)\n\nnom_features <- raw_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- raw_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- raw_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n\ncompetition_df <- read_csv(str_c(path, \"test.csv\"),\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nall_df <-\n    bind_rows(raw_df %>% mutate(source = \"train\"),\n            competition_df %>% mutate(source = \"test\"))\n\nfuture::plan(\"sequential\")  \n\n\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Numeric features\n\nConsider where features require univariate transformation, or clipping outliers.\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 24\n#| fig.width: 12\n\nraw_df %>% \n  select(all_of(num_features), outcome_residual ) %>% \n  pivot_longer(-outcome_residual,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = ggplot2::cut_number(outcome_residual,3))) +\n  geom_histogram(alpha = 0.6, bins = 50) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 3) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  labs(color = NULL, fill = \"Outcome Residual\",\n       title = \"Numeric Feature Univariate Distributions\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\nraw_df %>% \n  select(all_of(num_features), outcome_residual) %>% \n  pivot_longer(-outcome_residual,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, outcome_residual)) +\n  geom_jitter(alpha = 0.01, shape = 21) +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 4) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  labs(color = NULL, \n       title = \"Numeric Feature versus Outcome Residual\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n```\n\n\n## Nominal features\n\n```{r}\n#| label: nominal\n#| warning: false\n#| message: false\n#| fig.height: 12\n#| fig.width: 12\n\nif(length(nom_features) >0){\n\nraw_df %>% \n  select(all_of(nom_features), outcome_residual) %>% \n  pivot_longer(-outcome_residual,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value)) +\n  geom_bar() +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 4) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  labs(title = \"Nominal Feature Counts\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\nraw_df %>% \n  select(all_of(nom_features), outcome_residual) %>% \n  pivot_longer(-outcome_residual,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(outcome_residual, value)) +\n  geom_boxplot() +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 4) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  labs(title = \"Nominal Feature Versus Outcome\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n }   \n\n```\n\n## Counts of Missingness\n                  \n```{r}\n#| label: counts of missingness\n\nraw_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n```\n\n## Counts of Distinct\n               \n```{r}\n#| label: counts of distinct\n               \nraw_df %>% \n  summarize(\n    across(all_of(features), n_distinct)\n  ) %>%\n  pivot_longer(everything(),\n               names_to = \"feature\",\n               values_to = \"Count of distinct\") %>% \n                   knitr::kable()\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct label?\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\nall_df %>%\n    select(all_of(features), source) %>% \n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>%\n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>%\n    count(is_duplicated) %>% \n                   knitr::kable()\n               \n\n```\n                   \n## Pairwise Correlations\n                   \n`ggcorrplot` provides a quick look at numeric features where the correlation may be significant. \n                         \n\n```{r}\n#| label: pairwise correlations\n# Leave blank on no significant coefficient\n#| fig.height: 12\n#| fig.width: 12\n                   \ncorr <- raw_df %>%\n  select(all_of(num_features), outcome_residual) %>%\n  cor()\n\np.mat <-raw_df %>%\n  select(all_of(num_features), outcome_residual) %>%\n  ggcorrplot::cor_pmat() \n\nggcorrplot::ggcorrplot(\n    corr,\n    hc.order = TRUE,\n    lab_size = 1.5,\n    tl.cex = 4,\n    pch.cex = 8,\n    digits = 2,\n    lab = TRUE,\n    type = \"lower\",\n    show.diag = TRUE,\n    insig = \"blank\",\n    p.mat = p.mat\n  ) +\n  labs(title = \"Pairwise Correlations Training Set\")\n\nggcorrplot::ggcorrplot(\n   competition_df %>%\n    select(all_of(num_features)) %>%\n    cor(),\n    hc.order = TRUE,\n    lab_size = 1.5,\n    tl.cex = 4, \n    pch.cex = 8,\n    digits = 2,\n    lab = TRUE,\n    type = \"lower\",\n   show.diag = TRUE,\n    insig = \"blank\",\n    p.mat = competition_df %>%\n  select(all_of(num_features)) %>%\n  ggcorrplot::cor_pmat() \n  ) +\n  labs(title = \"Pairwise Correlations Competition Set\")\n\n```      \n\n\n## Target\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 12\n\nraw_df %>% \n ggplot(aes(outcome_residual)) +\n  geom_histogram(binwidth = 0.005) +\n  labs(title = \"Outcome: Residual from the Mean of the Original Features\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n```\n                              \n           \n                \n# Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n## Recipe\n                \n\n```{r}\n#| label: recipe\n#| fig.height: 30\n#| fig.width: 30\n\nrec <- recipe(\n    \n    formula(paste0(\"outcome_residual ~ \", \n               str_c(features,  collapse = \" + \"))),\n    data = raw_df \n  ) \n                                     \n```\n\n## Lightgbm Engine\n\n```{r}\n#| label: lightgbm engine\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 10000L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune(),\n    loss_reduction = 0\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n          #   boosting = \"goss\",\n          #   num_threads = future::availableCores()\n             ) %>%\n  set_mode(mode = \"regression\") \n                   \nfolds <- vfold_cv(raw_df, \n                  v = 9,\n                  repeats = 1,\n                  strata = outcome_residual)\n\n```\n\n## LightGBM Fit                                                                     \n\n```{r}\n#| label: lightgbm fit \n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12\ntictoc::tic()\n                   \nfuture::plan(\"multisession\", workers = future::availableCores())\n                   \nwf <- workflow(rec,\n               boost_tree_lgbm_spec) \n\nset.seed(42)\n\nctrl <- finetune::control_sim_anneal(     \n     verbose = FALSE,\n     verbose_iter = TRUE,\n     parallel_over = \"everything\",\n     save_pred = TRUE,\n     save_workflow = TRUE)\n\nparam <- wf %>%\n   extract_parameter_set_dials() %>%\n   recipes::update(\n      min_n = min_n(range = c(500, 1000)),\n      tree_depth = tree_depth(range = c(5, 20)),\n      learn_rate = learn_rate(range = c(-3, -1.5)),\n      num_leaves = num_leaves(range = c(150, 350))\n   ) %>%\n   dials::finalize(raw_df)                   \n                   \nburnin <- tune_grid(\n  wf,\n  grid = 4,\n  resamples = folds,\n  control = ctrl,\n  metrics = metric_set(rmse),\n  param_info = param)\n\nlgbm_rs <- finetune::tune_sim_anneal(\n  wf,\n  resamples = folds,\n  iter = 4,\n  initial = burnin,\n  control = ctrl,\n  metrics =  metric_set(rmse),\n  param_info = param) \n                   \nfuture::plan(\"sequential\")  \n                   \nshow_best(lgbm_rs)  \n\nautoplot(lgbm_rs)\n\nregression_fit <- wf %>%\n  finalize_workflow(select_best(lgbm_rs)) %>%\n  fit(raw_df)\n\naugment(regression_fit, raw_df) %>% \n  mutate(pred = flood_mean/10 - .pred) %>% \n  rsq(flood_probability, pred)\n\nregression_fit %>% \n  extract_fit_engine() %>% \n  lightgbm::lgb.importance() %>% \n  lightgbm::lgb.plot.importance()  \n\n```\n                   \n## Submission\n                 \n             \n                   \n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n                   \nsubmit_df <- augment(regression_fit, competition_df) %>% \n  transmute(id, FloodProbability = flood_mean/10 - .pred) \n\n\nhead(submit_df)  %>% \n     knitr::kable()      \n                   \n submit_df %>% ggplot(aes(FloodProbability)) + geom_histogram(bins = 500)                   \n                   \nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n```          ","metadata":{"_uuid":"8c7bec1d-f08c-45ad-ba78-f11a70e88005","_cell_guid":"e8d24220-3f01-4b1d-b183-b0055ca4c8e4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}