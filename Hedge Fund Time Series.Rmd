{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":105581,"databundleVersionId":15242733,"sourceType":"competition"}],"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"---\ntitle: \"Hedge Fund Time Series Forecasting\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n\n![](https://www.visualcapitalist.com/wp-content/uploads/2022/12/HedgeFunds_MAINNov30.jpg)\n\n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict `y_target` on a hierarchical time series. The sponsor of the competition is *The Data Company*.\n\nLets dive right in.\n\n\n## Load libraries\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/ts-forecasting/\"\n  \n# options(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2021-03-22\"))\n\ninstall.packages(\"pak\")\npak::pak(\"bonsai\")\npak::pak(\"ggplot2\")\n    \nremotes::install_github(\"luisdva/hexsession\", quiet = TRUE)    \n\ncores <- future::availableCores()\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n  orig_path <- stringr::str_c(here::here(\"data\"),\"/\")\n\n  cores <- future::availableCores(omit = 2)\n  \n}\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels, quietly = TRUE) # metapackage see https://www.tidymodels.org/\n\nlibrary(bonsai)  # Tidymodels access to lightgbm\n\nlibrary(modeltime)\nlibrary(thief)\n\n# library(mirai)\n# daemons(cores)\n    \n})\n\nconflicted::conflict_prefer(\"select\", \"dplyr\")\nconflicted::conflict_prefer(\"where\", \"dplyr\")\n\n\ntidymodels_prefer()\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(theme_minimal())\n\nhexsession::make_tile(packages = c(\"lightgbm\",\"bonsai\",\"DALEXtra\", \"DALEX\",  \"correlationfunnel\",\"yardstick\",\"workflows\",\n                                   \"tune\",\"rsample\",\"recipes\",\"parsnip\",\"dials\",\"broom\",\"tidymodels\",\"lubridate\",\n                                   \"forcats\",\"stringr\",\"dplyr\",\"purrr\",\"readr\",\"tidyr\",\"tibble\",\"ggplot2\",\"tidyverse\"),\n                     highlight_mode = TRUE\n                     )\n\n\n```\n\n\n## Load Data\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n#| skimr_digits: 3\n#| width: 90\n#| echo: true\n\npreprocessor <- function(dataframe) {\n\ndataframe <- dataframe |>\n    janitor::clean_names() |> \n    \n    mutate(across(c(where(is.character)), \\(x) as.factor(x)))  \n\nreturn(dataframe)\n}\n\ntrain_df <- arrow::read_parquet(str_c(path, \"train.parquet\")) |> \n  preprocessor()\n\nskimr::skim(train_df)\n\n```\n\n\n## Features\n\n\n```{r}\n#| label: Features\n#| warning: false\n#| message: false\n\n\nfeatures <- train_df %>%\n  dplyr::select(starts_with(\"feature\")) |> \n  names()\n\nnom_features <- train_df |> \n  dplyr::select(dplyr::all_of(features)) |> \n  dplyr::select(dplyr::where(is.factor)) |> \n  names() \n\nlogical_features <- train_df |> \n  dplyr::select(dplyr::all_of(features)) |> \n  dplyr::select(dplyr::where(is.logical)) |> \n  names() \n\nnum_features <- train_df |> \n  dplyr::select(dplyr::all_of(features)) |> \n  dplyr::select(dplyr::where(is.numeric)) |> \n  names()\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\nSize of the train dataset:\n\n`r nrow(train_df)`\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Time Series\n\n```{r}\n#| label: time series\n#| warning: false\n#| message: false\n#| fig.height: 12\n#| fig.width: 12\n\ntsviz_df <- train_df |> \n  dplyr::select(all_of(num_features), y_target, ts_index) %>% \n  slice_sample(prop = 0.1) |> \n  pivot_longer(-c(y_target, ts_index),\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) \n\ntsviz_df |> \n  ggplot(aes(value, y_target)) + \n  geom_bin_2d() +\n  geom_smooth() +\n  geom_rug() +\n  scale_fill_distiller(type = \"seq\", palette = \"YlOrRd\") +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2)\n\ntsviz_df |> \n  ggplot(aes(ts_index, value)) + \n  geom_bin_2d() +\n  geom_smooth() +\n  geom_rug() +\n  scale_fill_distiller(type = \"seq\", palette = \"YlOrRd\") +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2)\n```\n\n\n## Numeric\n\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 12\n#| fig.width: 12\n\ntsviz_df |> \n  ggplot(aes(value, y_target)) + \n  geom_bin_2d() +\n  geom_smooth() +\n  geom_rug() +\n  scale_fill_distiller(type = \"seq\", palette = \"YlOrRd\") +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2)\n\n```\n\n\n# Evaluation Function\n\n```{r}\n\n\nweighted_rmse_score_vec <- function(truth, estimate, na_rm = TRUE, ...) {\n  \n  weighted_rmse_score_impl <- function(truth, estimate) {\n\n    clip01 <- function(x) {\n             # Ensure the input is treated as a numeric vector\n             x <- as.numeric(x)\n             # Use vectorized functions pmax and pmin\n            return(pmin(pmax(x, 0.0), 1.0))\n    }\n\n    denom <- sum(w * truth^2)\n    ratio <- sum(w * (truth - estimate)^2) / denom\n    clipped <- clip01(ratio)\n    val <- 1.0 - clipped\n  \n     return(sqrt(val))\n    \n    \n  }\n  \n  metric_vec_template(\n    metric_impl = weighted_rmse_score_impl,\n    truth = truth, \n    estimate = estimate,\n    na_rm = na_rm,\n    cls = \"numeric\",\n    ...\n  )\n  \n}\n\n\nweighted_rmse_score <- function(data, ...) {\n  UseMethod(\"weighted_rmse_score\")\n}\n\nweighted_rmse_score <- new_numeric_metric(weighted_rmse_score, direction = \"minimize\")\n\nweighted_rmse_score.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {\n  \n  metric_summarizer(\n    metric_nm = \"weighted_rmse_score\",\n    metric_fn = mse_vec,\n    data = data,\n    truth = !! enquo(truth),\n    estimate = !! enquo(estimate), \n    na_rm = na_rm,\n    ...\n  )\n  \n}\n\n```\n\n\n# make a time series tibble\n\n# make initial_time_split\n\n# model specification\n\n```{r}\nmodel_spec_thief <- temporal_hierarchy() %>%\n  set_engine(\"thief\") %>%\n  set_mode(\"regression\")\n\n# You can specify parameters such as combination_method (e.g., \"struc\", \"mse\", \"ols\") or\n# use_model (e.g., \"ets\" or \"arima\") within the temporal_hierarchy() function, although defaults often work well.\n\n```\n\n# Fit the model\n\n```{r}\nmodel_fit_thief <- model_spec_thief %>%\n  fit(log(value) ~ date, data = training(splits))\n\n\n```\n\n# use in a modeltime workflow\n\n```{r}\nmodel_tbl <- modeltime_table(model_fit_thief) %>%\n  calibrate(testing(splits))\n\n# View test accuracy\nmodel_tbl %>%\n  modeltime_accuracy() %>%\n  table_modeltime_accuracy(.interactive = FALSE)\n\n```\n\n# Forecast and refit\n\n```{r}\n# Test set forecast plot\nmodel_tbl %>%\n  modeltime_forecast(\n    new_data = testing(splits),\n    actual_data = data,\n    .interactive = FALSE\n  ) %>%\n  plot_modeltime_forecast(.interactive = FALSE)\n\n# Refit to full data and forecast forward\n# model_tbl %>%\n#   modeltime_refit(data) %>%\n#   modeltime_forecast(h = \"1 year\")\n```\n","metadata":{"_uuid":"9420d9c4-89f1-47a9-bd1a-33e601760bf0","_cell_guid":"3cb1cbf9-6ba3-48e0-af89-e8c2b4b966c8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}