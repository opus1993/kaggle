---
title: "Media Campaign Cost Season3 Episode11"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

# Introduction  

<div 
    style="background-image: url('https://storage.googleapis.com/kaggle-datasets-images/new-version-temp-images/default-backgrounds-81.png-4760409/dataset-cover.png'); 
    width:100%; 
    height:600px; 
    background-position:center;">&nbsp;
</div>

The dataset for this competition (both train and test) was derived from a deep learning model trained on the [media campaign cost](https://www.kaggle.com/datasets/gauravduttakiit/media-campaign-cost-prediction) dataset. Our challenge is to to predict `cost`.

My notebook serves as a clean demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.

Lets dive right in.

# Preparation {.tabset .tabset-fade .tabset-pills}

## Load libraries

```{r }
#| label: setup

suppressPackageStartupMessages({
library(tidyverse) # metapackage of all tidyverse packages
library(tidymodels) # metapackage see https://www.tidymodels.org/

library(ggforce)

library(corrr)
library(GGally)

})
    
tidymodels_prefer()

options(tidymodels.dark = TRUE)

theme_set(theme_bw())


```

## Root Mean Squared Log Error


```{r}
#| label: custom RMSLE metric

rmsle_vec <- function(truth, estimate, na_rm = TRUE, ...) {
  
  rmsle_impl <- function(truth, estimate) {

      sqrt(mean(log(1 + truth) - log(1 + estimate))^2)
    
  }
  
  metric_vec_template(
    metric_impl = rmsle_impl,
    truth = truth, 
    estimate = estimate,
    na_rm = na_rm,
    cls = "numeric",
    ...
  )
  
}

rmsle <- function(data, ...) {
  UseMethod("mse")
}

rmsle <- new_numeric_metric(rmsle, direction = "minimize")

rmsle.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
  
  metric_summarizer(
    metric_nm = "rmsle",
    metric_fn = rmsle_vec,
    data = data,
    truth = !! enquo(truth),
    estimate = !! enquo(estimate), 
    na_rm = na_rm,
    ...
  )
  
}

metrics <- metric_set(rmsle)


```




## Interchangeability

```{r}
if (dir.exists("/kaggle")){
  path <- "/kaggle/input/playground-series-s3e11/"
} else {
  path <- str_c(here::here("data"),"/")
}
```

## Load Data

```{r }
#| label: load data

# factors <- c("florist","prepared_food","salad_bar","video_store","coffee_bar","recyclable_package","low_fat")

preprocessor <- function(rawdata){
  rawdata |> 
     janitor::clean_names() |> 
     mutate(across(-any_of(c("id", "cost")), factor))
}


raw_df <- read_csv(str_c(path, "train.csv"),
                   show_col_types = FALSE) |> 
          distinct(across(-id), .keep_all = TRUE) |> 
          preprocessor()

competition_df <- read_csv(str_c(path, "test.csv"),
                   show_col_types = FALSE) |> 
                  preprocessor()

all_df <- bind_rows(
  raw_df |> mutate(source = "train"),
  competition_df |> mutate(source = "competition")
) 

```

```{r}
#| label: skimr
skimr::skim(raw_df)
```

```{r}
#| label: skimr competition
skimr::skim(competition_df)
```

Food Mart (CFM) is a chain of convenience stores in the United States. Headquartered in Mentor, Ohio, the privately held company currently has approximately 325 stores in the United States. Convenience food supermarkets adopt a franchise system.
Food Mart was the third largest convenience store chain in the country in 1988.

The Nasdaq stock market fell Convenience Food Mart that same year after the company failed to meet financial reporting requirements.
Carden & Cherry used the character Ernest to advertise the convenience food market in the 1980s.
Your task is to build a machine learning model that will help us predict the cost of grocery store media campaigns based on the provided features.

    
* ```store_sales(in millions)``` - store_sales(in million dollars)
* ```unit_sales(in millions)``` - unit_sales(in millions) in stores Quantity
* ```Total_children``` - TOTAL CHILDREN IN HOME
* ```avg_cars_at home(approx)``` - avg_cars_at home(approx)
* ```Num_children_at_home``` - num_children_at_home AS PER CUSTOMERS FILLED DETAILS
* ```Gross_weight``` - gross_weight OF ITEM
* ```Recyclable_package``` - FOOD ITEM IS recyclable_package
* ```Low_fat``` - LOW_FAT FOOD ITEM IS LOW FAT
* ```Units_per_case``` - UNITS/CASE UNITS AVAILABLE IN EACH STORE SHELVES
* ```Store_sqft``` - STORE AREA AVAILABLE IN SQFT
* ```Coffee_bar``` - COFFEE BAR available in store
* ```Video_store``` - VIDEO STORE/gaming store available
* ```Salad_bar``` - SALAD BAR available in store
* ```Prepared_food``` - food prepared available in store
* ```Florist``` - flower shelves available in store
* ```Cost``` - COST ON ACQUIRING A CUSTOMERS in dollars


## Duplicated Values

Is this competition transaction already in the training data?

```{r}
#| label: duplicates
#| warning: false
#| message: false
#| fig.height: 10
#| fig.width: 12

all_df |> 
  group_by(-id) |>  
  mutate(num_dups = n(), 
         dup_id = row_number()) |>  
  ungroup() |> 
  group_by(source) |> 
  mutate(is_duplicated = dup_id > 1) |> 
  count(is_duplicated)
```


There are 3 SKUs that only exist in the competition set, based on gross_weight.


```{r}
all_df |> 
  count(gross_weight, source) |> 
  pivot_wider(names_from = source,
              values_from = n) |> 
  filter(is.na(competition) | is.na(train))


```

We can begin to identify the "real" SKUs when we count the gross_weights against recyclable packaging. In some cases the neural net has invented new combinations, and the new ones may only exist in the competition set.  These are the first 30 of 387 combinations.

```{r}
all_df |> 
  count(gross_weight, recyclable_package, source) |> 
  pivot_wider(names_from = c(source, recyclable_package),
              values_from = n) |> 
  slice_head(n = 30)


```

It's a bit of a relief that the number of children at home is often less than the total children in a household demographic. But not always.

```{r}
all_df |> 
  count(num_children_at_home, total_children, source) |> 
  pivot_wider(names_from = c(source, total_children),
              values_from = n) |> 
  slice_head(n = 30)


```


## Numerics

Lets zoom in more closely on the training set only.


```{r}
#| label: numeric density plots
#| warning: false
#| message: false
#| fig.height: 10
#| fig.width: 12
# raw_df |> 
#   select(where(is.numeric)) |>
#   mutate(cost = ggplot2::cut_number(cost, 5)) |> 
#   pivot_longer(- c(id, cost),
#     names_to = "metric",
#     values_to = "value"
#   ) |> 
#   ggplot(aes(value, fill = cost, color = NULL)) +
#   geom_density(alpha = 0.6, show.legend = FALSE) +
#   facet_wrap(vars(metric), scales = "free", ncol = 3) +
#   theme(panel.grid.major = element_blank(),
#         panel.grid.minor = element_blank())
```


## Q-Q


```{r}
#| label: qq plots
#| warning: false
#| message: false
#| fig.height: 10
#| fig.width: 12

# all_df |> 
#   select(where(is.numeric)) |>
#   mutate(cost = ggplot2::cut_number(cost, 5)) |> 
#   pivot_longer(- c(id, cost),
#     names_to = "metric",
#     values_to = "value"
#   ) |> 
#   ggplot(aes(sample = value)) + 
#   stat_qq(aes(color = cost)) + 
#   stat_qq_line(show.legend = FALSE) +
#   facet_wrap(vars(metric), scales = "free", ncol = 3) +
#   theme(panel.grid.major = element_blank(),
#         panel.grid.minor = element_blank(),
#         legend.position = c(0.8, 0.15))
```

## Categorical counts

```{r }
#| fig.height: 5
#| fig.width: 9
#| label: categorical features1
#| fig.height: 10
#| fig.width: 12

raw_df %>% 
  select(where(is.factor), cost) |> 
  group_by(across(-any_of(c("id", "cost")))) |> 
  summarize(cost = mean(cost),
            .groups = "drop") |> 
  gather("type", "value", -cost) |> 
  # pivot_longer(cols = -price,
  #              names_to = "type",
  #              values_to = "value") |> 
  ggplot(aes(value, fill = ggplot2::cut_number(cost, 5))) +
  geom_bar(position = "dodge") +
  facet_wrap(~ type, scales = "free", ncol = 2)+
  scale_fill_brewer(type = "seq") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "top") +
  labs(title = "Target impact - categorical features",
       fill = "Cost", x = NULL)
```

## Target: Cost

On to the target itself. There is a pretty big imbalance in this dataset.

```{r }
#| label: booking status target
#| warning: false
#| message: false
#| fig.height: 10
#| fig.width: 12
raw_df %>% 
  ggplot(aes(cost)) +
  geom_histogram(bins = 50) +
  labs(y = "Count of Samples", x = NULL, title = "Target: Cost")
```

# Feature-target interactions {.tabset .tabset-fade .tabset-pills}

## ggpairs

```{r}
#| label: interesting feature interactions
#| warning: false
#| message: false
#| fig.height: 10
#| fig.width: 12

# foo <- raw_df %>% 
#   select(where(is.numeric), -id) %>% 
#   mutate(cost = ggplot2::cut_number(cost, 5)) |> 
#   drop_na() 
# 
# foo %>% 
#   ggpairs(
#     columns = 1:(ncol(foo)-1),
#     mapping = aes(color = cost, alpha = 0.5),
#     lower = list(continuous = wrap("points", alpha = 0.3, size=0.01)),
#     upper = list(continuous = wrap("smooth", alpha = 0.005, size = 0.1)),
#     progress = FALSE) +
#   theme(axis.text = element_blank(), axis.ticks = element_blank()) +
#   labs(title = "Pair plots: lower: scatter; upper: linear fit - color by target")
```

## Feature Interaction Search

```{r}
# 
# boost_tree_xgboost_spec <-
#   boost_tree() |>
#   set_engine('xgboost',
#              tree_method = "hist") |>
#   set_mode('classification')
# 
# (rec_selector <- recipe(Class ~ ., data = raw_df )  %>% 
#     step_rm(id) %>%
#     step_range(Skewness, Skewness_DMSNR_Curve) %>%
#     step_log(Skewness, Mean_DMSNR_Curve, SD_DMSNR_Curve, Skewness_DMSNR_Curve, offset = 0.001) %>%
#     step_sqrt(Mean_Integrated) %>%
#     
#     step_interact(~ all_numeric_predictors():all_numeric_predictors()) |> 
#     step_ratio(all_numeric_predictors() ,
#                denom = denom_vars(all_predictors())) |> 
#     
#     step_nzv(all_numeric_predictors())
# )
# 
# 
# train_selector_df <- rec_selector |> prep() |> bake(new_data = NULL)
# 
# formulas <- leave_var_out_formulas(Class ~ ., data = train_selector_df)
# 
# folds <- vfold_cv(train_selector_df,
#                   v = 5,
#                   strata = Class)
# 
# all_cores <- parallelly::availableCores(omit = 1)
# all_cores
# 
# # future::plan("multisession", workers = all_cores) 
# 
# doFuture::registerDoFuture()
# cl <- parallel::makeCluster(all_cores)
# future::plan(future::cluster, workers = cl)
# 
# quality_workflows <-
#   workflow_set(
#     preproc = formulas,
#     models = list(xg = boost_tree_xgboost_spec)
#   )
# 
# quality_workflows
# 
# quality_workflows <- quality_workflows %>%
#   workflow_map("fit_resamples",
#                resamples = folds,
#                metrics = metrics)
# 
# quality_workflows
# 
# log_loss_values <-
#   quality_workflows %>%
#   collect_metrics(summarize = FALSE)  %>%
#   filter(.metric == "mn_log_loss") |> 
#   mutate(wflow_id = gsub("_xg", "", wflow_id))
# 
# full_model <-
#   log_loss_values %>%
#   filter(wflow_id == "everything") %>%
#   select(full_model = .estimate, id)
# 
# differences <-
#   log_loss_values %>%
#   filter(wflow_id != "everything") %>%
#   full_join(full_model, by = "id") %>%
#   mutate(performance_drop = full_model - .estimate)
# 
# summary_stats <-
#   differences %>%
#   group_by(wflow_id) %>%
#   summarize(
#     std_err = sd(performance_drop)/sum(!is.na(performance_drop)),
#     performance_drop = mean(performance_drop),
#     lower = performance_drop - qnorm(0.975) * std_err,
#     upper = performance_drop + qnorm(0.975) * std_err,
#     .groups = "drop"
#   ) %>%
#   mutate(
#     wflow_id = factor(wflow_id),
#     wflow_id = reorder(wflow_id, performance_drop)
#   )
# 
# summary_stats %>% filter(lower > 0)
# 
# ggplot(summary_stats, aes(x = performance_drop, y = wflow_id)) +
#   geom_point() +
#   geom_errorbar(aes(xmin = lower, xmax = upper), width = .25) +
#   ylab("")
```



# Machine Learning {.tabset .tabset-fade .tabset-pills}

## CV

```{r}
#| label: make resample folds

set.seed(42)

folds <- vfold_cv(raw_df, 
                  v = 5,
                  repeats = 2,
                  strata = cost)

```

## Parallel

Too speed computation, we will load up a parallel backend.

```{r}
#| label: assign a parallel backend

all_cores <- parallelly::availableCores(omit = 1)
all_cores

# future::plan("multisession", workers = all_cores) 

doFuture::registerDoFuture()
cl <- parallel::makeCluster(all_cores)
future::plan(future::cluster, workers = cl)

```

## Parsnip engines

```{r}
#| label: model specifications

boost_tree_xgboost_spec <-
  boost_tree(
    trees = 200L,
    min_n = tune(),  #26
    mtry = tune(),     #27
    tree_depth = tune(),   # 2
    learn_rate = 0.1
  ) %>%
  set_engine('xgboost') %>%
  set_mode('regression')

```

## Preprocessing Recipe


```{r}
#| label: set the recipe
#| warning: false
#| message: false
#| fig.height: 10
#| fig.width: 12

(rec <- recipe(Class ~ ., data = raw_df )  %>% 
  step_rm(id) %>%
  step_range(Skewness, Skewness_DMSNR_Curve) %>%
  step_log(Skewness, Mean_DMSNR_Curve, SD_DMSNR_Curve, Skewness_DMSNR_Curve, offset = 0.001) %>%
  step_sqrt(Mean_Integrated) %>%

  step_interact(~ EK_DMSNR_Curve:Skewness_DMSNR_Curve) |> 
  step_interact(~ EK:Skewness_DMSNR_Curve) |>   #0.03136 
  step_interact(~ EK:EK_DMSNR_Curve) |>    #0.03139
   
  step_interact(~ Mean_Integrated:SD) |> 
   
   step_ratio(EK_DMSNR_Curve,SD,
              denom = denom_vars(Skewness)) |>
   step_ratio(Mean_DMSNR_Curve ,Skewness_DMSNR_Curve,
              denom = denom_vars(EK)) |>
   step_ratio(Mean_Integrated,Skewness_DMSNR_Curve, Skewness,EK,
              denom = denom_vars(SD)) |>
   step_ratio(SD_DMSNR_Curve,
              denom = denom_vars(Mean_DMSNR_Curve)) |>
   step_ratio(SD_DMSNR_Curve,Mean_DMSNR_Curve,
              denom = denom_vars(Skewness_DMSNR_Curve)) |>
   step_ratio(SD,
              denom = denom_vars(SD_DMSNR_Curve)) |>
   step_ratio(Skewness_DMSNR_Curve,
              denom = denom_vars(EK_DMSNR_Curve)) |>
   

#  step_interact(~ Mean_DMSNR_Curve:SD_DMSNR_Curve) |> 
  # step_interact(~ SD:EK_DMSNR_Curve) |> 
  #  step_interact(~ SD:EK)  |>
  #  step_interact(~ SD:Mean_DMSNR_Curve) |>
  #  step_interact(~ SD:Skewness_DMSNR_Curve) |>
  #  step_interact(~ Skewness:Mean_DMSNR_Curve) |>
  #  step_interact(~ Skewness:SD_DMSNR_Curve) |>
  #  step_interact(~ Skewness:Skewness_DMSNR_Curve) |>
   
 step_poly(all_predictors(), degree = tune()) %>%
  step_ns(starts_with("EK_poly"), deg_free = 3) 
)

```

## UMAP View

```{r}
#| label:  UMAP view
#| warning: false
#| message: false
#| fig.height: 10
#| fig.width: 12

# umap_rec <- rec
# umap_rec$steps[[15]] <- update(umap_rec$steps[[15]], degree = 2)
# umap_rec <- umap_rec %>%
#   embed::step_umap(all_predictors(), 
#                    epochs = 100,
#                    neighbors = 50,
#                    seed = c(1,42),
#                    keep_original_cols = FALSE,
#                    outcome = vars(Class), 
#                    num_comp = 3) 
# 
# plot_validation_results <- function(recipe, dat = raw_df, target = Class) {
#   recipe %>%
#     # Estimate any additional steps
#     prep() %>%
#     # Process the data (the validation set by default)
#     bake(new_data = dat) %>%
#     # Create the scatterplot matrix
#     ggplot(aes(x = .panel_x, y = .panel_y, color = {{target}}, fill = {{target}})) +
#     geom_point(alpha = 0.4, size = 0.5) +
#     geom_autodensity(alpha = .3) +
#     facet_matrix(vars(-{{target}}), layer.diag = 2) + 
#     scale_color_brewer(palette = "Dark2") + 
#     scale_fill_brewer(palette = "Dark2")
# }
# 
# 
# umap_rec %>% 
#   plot_validation_results() +
#   ggtitle("UMAP (supervised)")


```

## LightGBM

```{r}
#| label: xgboost
#| warning: false
#| fig.height: 10
#| fig.width: 12

ctrlg <- finetune::control_sim_anneal(
     verbose = FALSE,
     verbose_iter = TRUE,
     save_pred = FALSE,
     event_level = "second",
     save_workflow = FALSE,
     parallel_over = "resamples")

wf <- workflow(rec, boost_tree_lgbm_spec)

lgbm_param <- wf %>%
  extract_parameter_set_dials() %>%
  recipes::update(
    min_n = min_n(), # range = c(44L,57L)
    mtry = mtry(), # range = c(20L, 26L)
    tree_depth = tree_depth(), # range = c(18L, 23L)
    degree = degree(range = c(1.7,3.2))
  ) %>%
  dials::finalize(raw_df)

lgbm_burnin <- tune_grid(
  wf,
  resamples = folds,
  grid = 12,
  control = ctrlg,
  metrics = metrics,
  param_info = lgbm_param
  )

autoplot(lgbm_burnin)
```


```{r}

results <- finetune::tune_sim_anneal(
  wf,
  resamples = folds,
  iter = 200,
  initial = lgbm_burnin,
  control = ctrlg,
  metrics = metrics,
  param_info = lgbm_param
  )

autoplot(results) 

collect_metrics(results, metric = "mn_log_loss") %>% 
  select(-.metric, -.estimator) %>%
  arrange(mean)

best_fit <- wf %>%
     finalize_workflow(select_best(results)) %>% 
     fit(raw_df) 

```


# Submission


```{r}

submission_df <- predict(best_fit, competition_df, type = "prob") |>
  bind_cols(competition_df) |>
  mutate(Class = .pred_pulsar, id = as.integer(id)) |> 
  select(id,Class)

submission_df |> 
  ggplot(aes(Class)) +
  geom_histogram()

submission_df %>%
  write_csv(str_c(path, "submission.csv"))
```




