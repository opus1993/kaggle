{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"---\ntitle: \"Insurance Premium\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict insurance premiums from a synthetic datset.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nIf you fork this on kaggle, be sure to choose the kernel Environment setting for \"Always use latest environment\"\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm and `brulee` interface to torch.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s4e12/\"\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2021-03-22\"))\n#install.packages(\"BradleyTerry2\", quiet = TRUE)\n# remotes::install_github(\"luisdva/hexsession\", quiet = TRUE)  had used previously, but recent addition of menu() makes it unworkable here\n\ncores <- future::availableCores()\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n  orig_path <- stringr::str_c(here::here(\"data\"),\"/\")\n\n  cores <- future::availableCores(omit = 1)\n}\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\n  \nlibrary(finetune)\n  \nlibrary(bonsai)  \nlibrary(stacks)\n # interface to lightgbm\n\n})\n\ntidymodels_prefer()\n\nconflicted::conflicts_prefer(purrr::is_null)\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(ggdark::dark_theme_minimal())\n\n\n```\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-kcvrLr1r8k?si=-dg94gULpjJwk33c\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n## Load Data\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\n\ntrain_spec <- cols(\n id = col_integer(),\n Age = col_double(),\n Gender = col_character(),\n `Annual Income` = col_double(),\n `Marital Status` = col_character(),\n `Number of Dependents` = col_double(),\n `Education Level` = col_character(),\n Occupation = col_character(),\n `Health Score` = col_double(),\n Location = col_character(),\n `Policy Type` = col_character(),\n `Previous Claims` = col_double(),\n  `Vehicle Age` = col_double(),\n  `Credit Score`  = col_double(),\n `Insurance Duration` = col_double(),\n  `Policy Start Date` = col_datetime(),\n  `Customer Feedback` = col_character(),\n  `Smoking Status` = col_character(),\n  `Exercise Frequency` = col_character(),\n  `Property Type` = col_character(),\n  `Premium Amount`  = col_double()\n)\n\n\ncompetition_spec <- cols(\n id = col_integer(),\n Age = col_double(),\n Gender = col_character(),\n `Annual Income` = col_double(),\n `Marital Status` = col_character(),\n `Number of Dependents` = col_double(),\n `Education Level` = col_character(),\n Occupation = col_character(),\n `Health Score` = col_double(),\n Location = col_character(),\n `Policy Type` = col_character(),\n `Previous Claims` = col_double(),\n  `Vehicle Age` = col_double(),\n  `Credit Score`  = col_double(),\n `Insurance Duration` = col_double(),\n  `Policy Start Date` = col_datetime(),\n  `Customer Feedback` = col_character(),\n  `Smoking Status` = col_character(),\n  `Exercise Frequency` = col_character(),\n  `Property Type` = col_character()\n )\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   col_types = train_spec,\n                   show_col_types = FALSE) \n\n\npreprocessor <- function(dataframe) {\n\ndataframe <- dataframe %>%\n    janitor::clean_names() %>%\n  \n    mutate(health_score = round(health_score),\n           policy_start_date = floor_date(policy_start_date, unit = \"day\")) %>% \n  \n    group_by(policy_start_date) |> \n    add_count(name = \"n_policies_today\") |> \n  \n    mutate(across(c(annual_income, credit_score, health_score), \\(x) (x - min(x, na.rm = TRUE))/(max(x, na.rm = TRUE)-min(x, na.rm=TRUE)), .names = \"scale_day_{.col}\")) |> \n  \n    ungroup() |> \n\n    replace_na(list(previous_claims = 0)) |> \n   \n    mutate(across(c(\"previous_claims\",\"insurance_duration\",\"number_of_dependents\"), \\(x) as.factor(x), .names = \"factor_{.col}\")) |>\n  \n    mutate(across(c(where(is.character)), \\(x) as.factor(x))) \n\n\nreturn(dataframe)\n}\n\nraw_df <- raw_df %>%\n          preprocessor() \n\ntst_df <- read_csv(str_c(path, \"test.csv\"),\n                   col_types = competition_spec,\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nfeatures <- raw_df %>%\n  select(-id, -premium_amount) %>%\n  names()\n\n# because we already know the test set, let's remove the train set factor levels that do not correspond with anything on the test set\nfor (col in names(raw_df)) {\n    if (is.factor(raw_df[[col]]) & col != \"depression\") {\n      # Get levels in train and test dataframes\n      raw_levels <- levels(raw_df[[col]])\n      tst_levels <- levels(tst_df[[col]])\n      \n      # Identify levels in train not in test\n      new_levels <- setdiff(raw_levels, tst_levels)\n      \n      # Set these levels to NA in train dataframe\n      raw_df[[col]] <- factor(raw_df[[col]], levels = c(tst_levels, new_levels))\n      raw_df[[col]][raw_df[[col]] %in% new_levels] <- NA_character_\n    }\n  }\n\nall_df <-\n  bind_rows(\n    raw_df %>% mutate(source = \"train\") %>%\n      distinct(pick(all_of(features)), .keep_all = TRUE),\n    tst_df %>% mutate(source = \"test\")\n  )\n\ntrain_df <- all_df %>% \n  filter(source == \"train\") %>% \n  select(-source) \n\ncompetition_df <- all_df %>% \n  filter(source == \"test\") %>% \n  select(-source, -premium_amount)\n\nnom_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\nLogical features: \n\n`r logical_features`\n\n\nSize of the combined train and competition datasets:\n\n`r nrow(all_df)`\n\nSize of the split made available to machine learning\n\n`r nrow(train_df)`\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Numeric features\n\nConsider where features require univariate transformation, or clipping outliers.\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 18\n#| fig.width: 12\n\ntrain_df %>% \n  select(all_of(num_features), premium_amount ) %>% \n  pivot_longer(-premium_amount,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = ggplot2::cut_number(premium_amount, 5))) +\n  geom_histogram(bins = 200) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\") +\n  labs(color = NULL, fill = NULL,\n       title = \"Numeric Feature Univariate Distributions\",\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\ntrain_df %>% \n  select(all_of(num_features), premium_amount ) %>% \n  pivot_longer(-premium_amount,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, premium_amount)) +\n  geom_point(shape = 20, alpha = 0.01) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\") +\n  labs(color = NULL, fill = NULL,\n       title = \"Numeric Features and Premium Amount\",\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n```\n\n\n## Nominal features\n\nExplore the distribution of outcome class by factor level, and the factor levels that exist in test that do not exist in training.\n\n\n```{r}\n#| label: nominal\n#| warning: false\n#| message: false\n#| fig.height: 18\n#| fig.width: 18\n\n\nif(length(nom_features) >0){\n\ntrain_df %>% \n  select(all_of(nom_features), premium_amount) %>% \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-premium_amount,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n    \n  filter(!is.na(premium_amount)) %>% \n    \n  summarise(n = n(),\n            .by = c(premium_amount, metric, value)) %>%\n      \n  mutate(value = tidytext::reorder_within(value, n, metric)) %>%\n    \n  ggplot(aes(x = n, y = value, fill = ggplot2::cut_number(premium_amount, 5))) +\n  geom_col() +\n  tidytext::scale_y_reordered() +\n  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Nominal Feature Counts\",\n       fill = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n}\n\n```\n\n## Counts of Missingness\n                  \n```{r}\n#| label: counts of missingness\n\ntrain_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n                   \ntrain_df %>% \n    select(all_of(features)) %>%\n    slice_sample(n = 2000) %>%\n    naniar::vis_miss()          \n\nnaniar::gg_miss_var(train_df %>% select(all_of(features), policy_type), \n                   facet = policy_type)    \n\n                  \n```\n\n## Counts of Distinct\n                   \n               \n```{r}\n#| label: counts of distinct\n               \ntrain_df %>%\n  summarize(across(all_of(features), n_distinct)) %>%\n  pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct train\") |>\n  left_join(\n    tst_df %>%\n      summarize(across(all_of(features), n_distinct)) %>%\n      pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct test\")\n  ) %>% \n                   knitr::kable()\n\n\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct label?\n\nIt looks like we had had a few car models with more than one selling price.\n\nThe test set has 81 entries that appear in train, and 24 that appear twice in test.\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\n\nall_df %>%\n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>% \n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>% \n    count(is_duplicated) %>% \n                   knitr::kable()\n\n```\n                   \n\n\n\n## Target\n\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 6\n\n train_df |> count(premium_amount) |> arrange(desc(premium_amount))                   \n                   \ntrain_df %>% \n  ggplot(aes(premium_amount)) +\n  geom_histogram(bins = 100) +\n  scale_x_log10(labels = scales::comma) +\n  labs(title = \"Premium Amounts\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n\n\n```\n \n           \n                \n# Regression Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n## Root Mean Squared Log Error\n\n\n```{r}\n#| label: custom RMSLE metric\n\nrmsle_impl <- function(truth, estimate, case_weights = NULL) {\n\n      sqrt( mean(  ((log1p(truth) - log1p(estimate))^2 )))\n    \n}\n\n\nrmsle_vec <- function(truth, estimate, na_rm = TRUE, case_weights = NULL, ...) {\n  check_numeric_metric(truth, estimate, case_weights)\n\n  if (na_rm) {\n    result <- yardstick_remove_missing(truth, estimate, case_weights)\n\n    truth <- result$truth\n    estimate <- result$estimate\n    case_weights <- result$case_weights\n  } else if (yardstick_any_missing(truth, estimate, case_weights)) {\n    return(NA_real_)\n  }\n\n  rmsle_impl(truth, estimate, case_weights = case_weights)\n}\n\nrmsle <- function(data, ...) {\n  UseMethod(\"rmsle\")\n}\n\nrmsle <- new_numeric_metric(rmsle, direction = \"minimize\")\n\nrmsle.data.frame <- function(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...) {\n\n  numeric_metric_summarizer(\n    name = \"rmsle\",\n    fn = rmsle_vec,\n    data = data,\n    truth = !!enquo(truth),\n    estimate = !!enquo(estimate),\n    na_rm = na_rm,\n    case_weights = !!enquo(case_weights)\n  )\n}\n\nmetrics <- metric_set(rmsle)\n\n```\n\n## Recipe\n\n```{r}\n#| label: recipe\n#| warning: false\n#| message: false\n#| fig.width: 6\n                   \nlgbm_rec <- recipe(formula(paste0(\"premium_amount ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  \n  step_date(policy_start_date, label = TRUE,   features = c(\"month\", \"week\"), keep_original_cols = FALSE) %>% \n\n  step_zv(all_predictors())\n\n\nlinear_rec <- recipe(formula(paste0(\"premium_amount ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |> \n  \n  step_date(policy_start_date, label = TRUE,   features = c(\"dow\", \"month\", \"week\", \"year\"), keep_original_cols = FALSE) %>% \n\n  step_ratio(annual_income, credit_score, vehicle_age, denom = denom_vars(age)) |> \n  step_ratio(annual_income, denom = denom_vars(health_score)) |> \n  \n                   \n  step_dummy(all_nominal_predictors()) |> \n  \n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nxgb_rec <- recipe(formula(paste0(\"premium_amount ~ \", str_c(features, collapse = \" + \"))), data = train_df) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  \n  step_impute_median(all_numeric_predictors()) |> \n  step_impute_mode(all_nominal_predictors()) |> \n  \n  step_date(policy_start_date, label = TRUE, features = c(\"dow\", \"month\", \"week\", \"year\"), keep_original_cols = FALSE) %>% \n  \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> \n  \n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nfolds <- vfold_cv(train_df,\n                  v = 5,\n                  repeats = 1,\n                  strata = premium_amount)\n\n                   \n```\n\n## Workflowsets\n\n```{r}\n#| label: workflowsets\n#| warning: false\n#| message: false\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 300L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune(),\n   loss_reduction = tune()\n #  mtry = tune()\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_threads = cores\n             ) %>%\n  set_mode(mode = \"regression\") \n\n\nboost_tree_xgb_spec <- \n  boost_tree(\n    trees = 300L,\n    min_n = tune(),\n    learn_rate = tune()\n  ) %>% \n  set_engine(engine = \"xgboost\", nthread = cores) %>%\n  set_mode(mode = \"regression\")    \n\nlinear_reg_glmnet_spec <-\n   linear_reg(penalty = tune(), mixture = tune()) %>%\n   set_engine('glmnet')\n\ndep_models <- \n   workflow_set(\n      preproc = list(imputedhot = xgb_rec,\n                     imputed = linear_rec,\n                     base = lgbm_rec),\n      models = list(xgb = boost_tree_xgb_spec,\n                    linear = linear_reg_glmnet_spec,\n                    lgbm = boost_tree_lgbm_spec),\n      cross = FALSE\n   ) %>% \n  option_add_parameters() |> \n  option_add(\n    control = finetune::control_sim_anneal( save_pred = TRUE, save_workflow = TRUE, verbose = TRUE),\n    metrics = metrics\n  )\n\nxgb_params <- dep_models |>\n  extract_workflow(\"imputedhot_xgb\") |>\n  parameters() |>\n  update(\n    learn_rate = learn_rate(range = c(-2.0,-1.2)),\n    min_n = min_n(range = c(10,25))\n    )\n\nlgbm_params <- dep_models |> \n  extract_workflow(\"base_lgbm\") |> \n  parameters() |> \n  update(\n      min_n = min_n(range = c(3,50)),\n      loss_reduction = loss_reduction(range = c(-1,0)),\n   #   mtry = mtry(range = c(27,unknown())),\n      tree_depth = tree_depth(range = c(20,200)),\n      learn_rate = learn_rate(range = c(-1.4,-0.9))\n         )\n\ndep_models <- dep_models |> \n  option_add(\n    param_info = lgbm_params,\n    id = \"base_lgbm\"\n  ) |> \n  option_add(\n    param_info = xgb_params,\n    id = \"imputed_xgb\"\n  ) |>\n   workflow_map(\"tune_sim_anneal\", resamples = folds, iter = 8, \n                metrics = metrics, verbose = TRUE)\n                   \n\nautoplot(dep_models) +\n  geom_text(aes(y = mean -0.01, label = wflow_id), angle = 90, hjust = 1)+\n  scale_y_continuous(expand = expansion(mult = c(0.5,0)))+\n  theme(legend.position = \"none\")\n\nrank_results(dep_models, rank_metric = \"rmsle\", select_best = TRUE) %>% \n   select(rank, mean, model, wflow_id, .config)\n\n\n\n```\n\n\n```{r }                   \n#| label: xgb results\n#| warning: false\n#| message: false\n\n\ndep_models %>%\n  dplyr::filter(grepl(\"xgb\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean) \n                   \ndep_models |> \n  workflowsets::extract_workflow_set_result(\"imputedhot_xgb\") |> \n  autoplot()\n```\n\n\n```{r }                   \n#| label: lgbm results\n#| warning: false\n#| message: false               \n                   \ndep_models %>%\n  dplyr::filter(grepl(\"lgbm\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean)\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"base_lgbm\") |> \n  autoplot()\n\n```\n\n\n```{r }                   \n#| label: linear results\n#| warning: false\n#| message: false                    \n\ndep_models %>%\n  dplyr::filter(grepl(\"linear\", wflow_id)) %>%\n  mutate(metrics = map(result, collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(mean)\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"imputed_linear\") |> \n  autoplot()\n\n```\n\n# Ensemble Stack                   \n\n```{r }                   \n#| label: stacks\n#| warning: false\n#| message: false                    \n                   \n\ndep_stack <- stacks() %>%\n  add_candidates(dep_models) %>%\n  blend_predictions(  metric = metrics,\n      penalty = c(10^seq(-2.2, -0.8, 0.1)),\n      non_negative = TRUE,\n      control = tune::control_grid(allow_par = TRUE))\n\nautoplot(dep_stack)\n\nautoplot(dep_stack, type = \"members\")        \n                   \nautoplot(dep_stack, \"weights\")\n                   \nregression_fit <- dep_stack %>% \n    fit_members()\n\n\n```\n                   \n\n# Submission\n                   \n\n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6                    \n                   \naugment(regression_fit, train_df) %>% \n  ggplot(aes(.pred, premium_amount)) +\n  geom_point(alpha = 0.1, shape = 20) +\n  geom_abline(color = \"green\") \n\n\nsubmit_df <-  augment(\n  regression_fit,\n  competition_df \n) %>%\n  transmute(id, `Premium Amount` = if_else(.pred < 0, 0, .pred))\n\nhead(submit_df)  %>% \n     knitr::kable()      \n\nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n```    ","metadata":{"_uuid":"1799c4e5-8edb-4cf3-a37f-1c61d277a605","_cell_guid":"2b606bb6-560e-426f-813c-3a4074fab0ec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}