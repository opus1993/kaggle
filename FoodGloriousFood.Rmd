---
title: "Calorie Expenditure"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---
  
# Introduction  {.tabset .tabset-fade .tabset-pills}

The goal of this competition is to predict the calorie expenditure from a synthetic datset.

My notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.

Lets dive right in.

The Kaggle kernels have many of the common r packages built in.  

## Load libraries

```{r }
#| label: setup
#| warning: false
#| message: false

if (dir.exists("/kaggle")){
  path <- "/kaggle/input/playground-series-s5e5/"

options(repos = c(CRAN = "https://packagemanager.posit.co/cran/2021-03-22"))

cores <- future::availableCores()

} else {
  path <- stringr::str_c(here::here("data"),"/")
  orig_path <- stringr::str_c(here::here("data"),"/")

  cores <- future::availableCores(omit = 2)
}
 
suppressPackageStartupMessages({
library(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages
library(tidymodels) # metapackage see https://www.tidymodels.org/

library(DALEXtra) # Tidymodels Explainability         
    
library(bonsai)   # interface to torch
library(stacks)   # model ensembling

})

tidymodels_prefer()
conflicted::conflicts_prefer(brulee::coef)

options(tidymodels.dark = TRUE)

theme_set(ggdark::dark_theme_minimal())

```

![](https://www.forksoverknives.com/wp-content/uploads/FOK_CalorieDensity_Final.jpg)


## Load Data

```{r }
#| label: load data
#| warning: false
#| message: false

preprocessor <- function(dataframe) {

dataframe <- dataframe %>%
    janitor::clean_names() %>%
  
    mutate(across(c(where(is.character)), ~ as.factor(.x))) 

return(dataframe)
}

raw_df <- read_csv(str_c(path, "train.csv"),
                   show_col_types = FALSE) |> 
          preprocessor() 

tst_df <- read_csv(str_c(path, "test.csv"),
                   show_col_types = FALSE)  |>  
  preprocessor() 


# because we already know the test set, let's remove the train set factor levels that do not correspond with anything on the test set
for (col in names(raw_df)) {
    if (is.factor(raw_df[[col]]) & col != "rainfall") {
      # Get levels in train and test dataframes
      raw_levels <- levels(raw_df[[col]])
      tst_levels <- levels(tst_df[[col]])
      
      # Identify levels in train not in test
      new_levels <- setdiff(raw_levels, tst_levels)
      
      # Set these levels to NA in train dataframe
      raw_df[[col]] <- factor(raw_df[[col]], levels = c(tst_levels, new_levels))
      raw_df[[col]][raw_df[[col]] %in% new_levels] <- NA_character_
    }
  }

# the synthetic playground competitions seem to perform better when numerics are also included as factors
all_df <-
    bind_rows(raw_df %>% mutate(source = "train"),
              tst_df %>% mutate(source = "test")) 


train_df <- all_df %>% 
  filter(source == "train") %>% 
  select(-source) 

competition_df <- all_df %>% 
  filter(source == "test") %>% 
  select(-source, -calories)


```
# EDA {.tabset .tabset-fade .tabset-pills}

## Features

We will de-duplicate the training set and calculate a mean calories figure.

```{r}
#| label: Features
#| warning: false
#| message: false
#| fig.width: 6

conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("where", "dplyr")

features <- train_df %>%
  dplyr::select(-id, -calories) |> 
  names()

train_df <- train_df %>% 
  dplyr::group_by_at(all_of(features)) |> 
  dplyr::summarise(id = min(id),
            calories = mean(calories),
            .groups = "drop")

nom_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.factor)) |> 
  names() 

logical_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.logical)) |> 
  names() 

num_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.numeric)) |> 
  names()
```

Nominal features:

`r nom_features`

Numeric features: 

`r num_features`

Logical features: 

`r logical_features`


Size of the combined train and competition datasets:

`r nrow(all_df)`

Size of training set after de-duplication

`r nrow(train_df)`


## Numeric features



```{r}
#| label: numeric
#| warning: false
#| message: false
#| fig.height: 6
#| fig.width: 12

train_df %>% 
  select(all_of(num_features), calories ) %>% 
  pivot_longer(-calories,
    names_to = "metric",
    values_to = "value"
  ) %>%
  ggplot(aes(value, fill = ggplot2::cut_number(calories, 5))) +
  geom_histogram(bins = 200) +
  facet_wrap(vars(metric), scales = "free", ncol = 2) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "top") +
  labs(color = NULL, fill = "Calories",
       title = "Numeric Feature Univariate Distributions",
       caption = "Data: Kaggle | Visual: Jim Gruman")

train_df %>% 
  select(all_of(num_features), calories ) %>% 
  pivot_longer(-calories,
    names_to = "metric",
    values_to = "value"
  ) %>%
  ggplot(aes(value, calories)) +
  geom_point(shape = 20, alpha = 0.01) +
   facet_wrap(vars(metric), scales = "free", ncol = 2) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "top") +
  labs(color = NULL, fill = "Calories",
       title = "Numeric Features and Calories",
       caption = "Data: Kaggle | Visual: Jim Gruman")

```


## Nominal features

Explore the distribution of outcome class by factor level.


```{r}
#| label: nominal
#| warning: false
#| message: false
#| fig.height: 6
#| fig.width: 6


if(length(nom_features) >0){

train_df %>% 
  select(all_of(nom_features), calories) %>% 
  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%
  pivot_longer(-calories,
    names_to = "metric",
    values_to = "value"
  ) %>%
    
  filter(!is.na(calories)) %>% 
    
  summarise(n = n(),
            .by = c(calories, metric, value)) %>%
      
  mutate(value = tidytext::reorder_within(value, n, metric)) %>%
    
  ggplot(aes(x = n, y = value, fill = ggplot2::cut_number(calories, 5))) +
  geom_col() +
  tidytext::scale_y_reordered() +
  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +
  facet_wrap(vars(metric), scales = "free", ncol = 2) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
       legend.position = "bottom") +
  labs(title = "Nominal Feature Counts",
       fill = NULL,
       caption = "Data: Kaggle | Visual: Jim Gruman")

}

```


## Counts of Missingness

                  
```{r}
#| label: counts of missingness

train_df %>% 
  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% 
  pivot_longer(everything(),
              names_to = "feature",
              values_to = "Count of Missing") %>% 
                   knitr::kable()

competition_df %>% 
  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% 
  pivot_longer(everything(),
              names_to = "feature",
              values_to = "Count of Missing") %>% 
                   knitr::kable()

```

## Counts of Distinct
                   
               
```{r}
#| label: counts of distinct
               
train_df %>%
  summarize(across(all_of(features), n_distinct)) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "Count of distinct train") |>
  left_join(
    competition_df %>%
      summarize(across(all_of(features), n_distinct)) %>%
      pivot_longer(everything(), names_to = "feature", values_to = "Count of distinct test"),
    by = join_by(feature)
  ) %>% 
                   knitr::kable()
               
```

## Duplicated

Is this competition transaction already in the training data with a correct label?

It looks as though the answer may be yes

```{r}
#| label: duplicates
#| warning: false
#| message: false

bind_rows(train_df %>% mutate(source = "train"),
              competition_df %>% mutate(source = "test")) |> 
    group_by_at(features) %>%
    mutate(num_dups = n(),
           dup_id = row_number()) %>% 
    ungroup() %>% 
    group_by(source) %>%
    mutate(is_duplicated = dup_id > 1) %>% 
    count(is_duplicated) %>% 
                   knitr::kable()


bind_rows(train_df %>% mutate(source = "train"),
              competition_df %>% mutate(source = "test")) |> 
    group_by_at(features) %>%
    mutate(num_dups = n(),
           dup_id = row_number()) %>% 
    filter(num_dups > 1 ) |> 
    arrange(age, height, weight, duration) |> 
    slice_head(n = 15)
               

```


## Pairwise Correlations
                   
`ggcorrplot` provides a quick look at numeric features where the correlation may be significant. 

```{r}
#| label: pairwise correlations
#| fig.width: 12
#| fig.height: 12                   
                   
train_df %>% 
  select(all_of(num_features)) %>% 
  ggcorrplot::cor_pmat() %>% 
  ggcorrplot::ggcorrplot(hc.order = TRUE, lab = TRUE,
    type = "lower", insig = "blank") +
  labs(title = "Pairwise Correlations Training Set")
                   
competition_df %>% 
  select(all_of(num_features)) %>% 
  ggcorrplot::cor_pmat() %>% 
  ggcorrplot::ggcorrplot(hc.order = TRUE, lab = TRUE,
    type = "lower", insig = "blank") +
  labs(title = "Pairwise Correlations Competition Set")

```                    
               
## Correlation Funnel

```{r}

library(correlationfunnel)

calories_df <- as_tibble(train_df$calories)

train_df %>% 
  select(all_of(features)) %>% 
    binarize(one_hot = TRUE) %>%
    bind_cols(calories_df) |> 
    mutate(value = as.numeric(value) -1) |> 
    rename(calories = value) |> 
    correlate(target = calories ) %>% 
    plot_correlation_funnel()

```

                   
## Target

The outcome variable distribution is skewed with values between 1 and 314.

```{r}
#| label: outcome 
#| warning: false
#| message: false
#| fig.width: 6

train_df |> count(calories) |> arrange(desc(calories))    

train_df |> count(calories) |> arrange(calories)    

train_df %>% 
  ggplot(aes(calories)) +
  geom_histogram(bins = 100) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Calories",
       caption = "Data: Kaggle.com | Visual: Jim Gruman")



```
 
           
                
# Regression Machine Learning {.tabset .tabset-fade .tabset-pills}

## Root Mean Squared Log Error


```{r}
#| label: custom RMSLE metric

rmsle_impl <- function(truth, estimate, case_weights = NULL) {

      sqrt( mean(  ((log1p(truth) - log1p(estimate))^2 )))
    
}


rmsle_vec <- function(truth, estimate, na_rm = TRUE, case_weights = NULL, ...) {
  check_numeric_metric(truth, estimate, case_weights)

  if (na_rm) {
    result <- yardstick_remove_missing(truth, estimate, case_weights)

    truth <- result$truth
    estimate <- result$estimate
    case_weights <- result$case_weights
  } else if (yardstick_any_missing(truth, estimate, case_weights)) {
    return(NA_real_)
  }

  rmsle_impl(truth, estimate, case_weights = case_weights)
}

rmsle <- function(data, ...) {
  UseMethod("rmsle")
}

rmsle <- new_numeric_metric(rmsle, direction = "minimize")

rmsle.data.frame <- function(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...) {

  numeric_metric_summarizer(
    name = "rmsle",
    fn = rmsle_vec,
    data = data,
    truth = !!enquo(truth),
    estimate = !!enquo(estimate),
    na_rm = na_rm,
    case_weights = !!enquo(case_weights)
  )
}

metrics <- metric_set(rmsle)

```

## Recipe

```{r}
#| label: recipe
#| warning: false
#| message: false
#| fig.width: 6
                   
lgbm_rec <- recipe(formula(paste0("calories ~ ", str_c(features, collapse = " + "))), data = train_df) 

xgb_rec <- lgbm_rec |> 
           step_dummy(all_nominal_predictors(), one_hot = TRUE) 


folds <- vfold_cv(train_df,
                  v = 5,
                  repeats = 1,
                  strata = calories)

                   
```



## Workflowsets

```{r}
#| label: workflowsets
#| warning: false
#| message: false

boost_tree_lgbm_spec <- 
  boost_tree(
    trees = 200L,
   tree_depth = tune(),
   learn_rate =  tune(),
   min_n = tune(),
   loss_reduction = tune()
  ) %>% 
  set_engine(engine = "lightgbm",
             is_unbalance = TRUE,
             num_leaves = tune(),
             num_threads = cores
             ) %>%
  set_mode(mode = "regression") 


boost_tree_xgb_spec <- 
  boost_tree(
    trees = 200L,
    min_n = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine(engine = "xgboost", nthread = cores) %>%
  set_mode(mode = "regression")                    

dep_models <- 
   workflow_set(
      preproc = list(imputed = xgb_rec,
                     base = lgbm_rec),
      models = list(xgb = boost_tree_xgb_spec,
                    lgbm = boost_tree_lgbm_spec),
      cross = FALSE
   ) %>% 
  option_add_parameters() |> 
  option_add(
    control = finetune::control_sim_anneal( save_pred = TRUE, save_workflow = TRUE, verbose = TRUE),
    metrics = metrics
  )

xgb_params <- dep_models |>
  extract_workflow("imputed_xgb") |>
  parameters() |>
  update(
    learn_rate = learn_rate(range = c(-2.8,-1.0)),
    min_n = min_n(range = c(10,35))
    )

lgbm_params <- dep_models |> 
  extract_workflow("base_lgbm") |> 
  parameters() |> 
  update(
      min_n = min_n(range = c(10,70)),
      loss_reduction = loss_reduction(range = c(-1,0)),
      tree_depth = tree_depth(range = c(20,100)),
      learn_rate = learn_rate(range = c(-1.9,-1.0)),
      num_leaves = num_leaves(range = c(100,400))       
         )

dep_models <- dep_models |> 
  option_add(
    param_info = lgbm_params,
    id = "base_lgbm"
  ) |> 
  option_add(
    param_info = xgb_params,
    id = "imputed_xgb"
  ) |>
   workflow_map("tune_sim_anneal", resamples = folds, iter = 6, 
                metrics = metrics, verbose = TRUE)
                   

autoplot(dep_models) +
  geom_text(aes(y = mean -0.01, label = wflow_id), angle = 90, hjust = 1)+
  scale_y_continuous(limits = c(0,NA)) |>
  theme(legend.position = "none")

rank_results(dep_models, rank_metric = "rmsle", select_best = TRUE) %>% 
   select(rank, mean, model, wflow_id, .config)

dep_models %>%
  dplyr::filter(grepl("xgb", wflow_id)) %>%
  mutate(metrics = map(result, collect_metrics)) %>%
  dplyr::select(wflow_id, metrics) %>%
  tidyr::unnest(cols = metrics) |> 
  arrange(mean)

dep_models %>%
  dplyr::filter(grepl("lgbm", wflow_id)) %>%
  mutate(metrics = map(result, collect_metrics)) %>%
  dplyr::select(wflow_id, metrics) %>%
  tidyr::unnest(cols = metrics) |> 
  arrange(mean)


dep_stack <- stacks() %>%
  add_candidates(dep_models) %>%
  blend_predictions(  metric = metrics,
      penalty = c(10^seq(-1.7, -0.3, 0.1)),
      non_negative = TRUE,
      control = tune::control_grid(allow_par = TRUE))

autoplot(dep_stack)

autoplot(dep_stack, type = "members")        
                   
autoplot(dep_stack, "weights")
                   
regression_fit <- dep_stack %>% 
    fit_members()


```
                   
# Submission
                   

```{r}
#| label: submission
#| warning: false
#| message: false
#| fig.height: 6
#| fig.width: 6                    
                   
augment(regression_fit, train_df) %>% 
  ggplot(aes(.pred, calories)) +
  geom_point(alpha = 0.1, shape = 20) +
  geom_abline(color = "green") 

submit_df <-  augment(
  regression_fit,
  competition_df %>%
    left_join(train_df %>% select(-id), by = all_of(features)) %>%
    rename(train_calories = calories)
) %>%
  transmute(id, calories = if_else(is.na(train_calories), .pred, train_calories)) 


head(submit_df)  %>% 
     knitr::kable()      

submit_df  %>% 
  write_csv("submission.csv")
```    