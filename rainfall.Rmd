{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91714,"databundleVersionId":11251744,"sourceType":"competition"}],"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jimgruman/rainfall?scriptVersionId=225555535\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"---\ntitle: \"Rainfall\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict rainfall for each day of the year.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s5e3/\"\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2021-03-22\"))\nremotes::install_github(\"tidyverts/fable.binary\", quiet = TRUE)\nremotes::install_github(\"tidymodels/plsmod\", quiet = TRUE)    \n    \ncores <- future::availableCores()\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n  orig_path <- stringr::str_c(here::here(\"data\"),\"/\")\n\n  cores <- future::availableCores(omit = 7)\n}\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\n\nlibrary(plsmod)\n    \nlibrary(bonsai)  \nlibrary(stacks)\n # interface to lightgbm\n\n})\n\ntidymodels_prefer()\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(ggdark::dark_theme_minimal())\n\n```\n\n## Load Data\n\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\npreprocessor <- function(dataframe) {\n\ndataframe <- dataframe %>%\n    janitor::clean_names() |>\n  \n    mutate(year = id %/% 365 ,\n           day = id - year * 365 ) |> \n\n    mutate(across(c(where(is.character)), ~ as.factor(.x))) \n\nreturn(dataframe)\n}\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   show_col_types = FALSE) |> \n          mutate(rainfall= as.character(rainfall)) |> \n          preprocessor() \n\ntst_df <- read_csv(str_c(path, \"test.csv\"),\n                   show_col_types = FALSE)  |>  \n  preprocessor() \n\n# because we already know the test set, let's remove the train set factor levels that do not correspond with anything on the test set\nfor (col in names(raw_df)) {\n    if (is.factor(raw_df[[col]]) & col != \"rainfall\") {\n      # Get levels in train and test dataframes\n      raw_levels <- levels(raw_df[[col]])\n      tst_levels <- levels(tst_df[[col]])\n      \n      # Identify levels in train not in test\n      new_levels <- setdiff(raw_levels, tst_levels)\n      \n      # Set these levels to NA in train dataframe\n      raw_df[[col]] <- factor(raw_df[[col]], levels = c(tst_levels, new_levels))\n      raw_df[[col]][raw_df[[col]] %in% new_levels] <- NA_character_\n    }\n  }\n\n# the synthetic playground competitions seem to perform better when numerics are also included as factors\nall_df <-\n    bind_rows(raw_df %>% mutate(source = \"train\"),\n              tst_df %>% mutate(source = \"test\")) \n\n\ntrain_df <- all_df %>% \n  filter(source == \"train\") %>% \n  select(-source) \n\ncompetition_df <- all_df %>% \n  filter(source == \"test\") %>% \n  select(-source, -rainfall)\n\n\n```\n\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Time Series\n\nThere is a seasonal component. \n\n```{r}\n#| label: time series\n#| warning: false\n#| message: false\n#| fig.width: 6\n\nlibrary(fable)\nlibrary(feasts)\nlibrary(fable.binary)\n\ntrain_df |> \n  group_by(year, week = day %/% 7) |> \n  summarize(rainfall = mean(rainfall == 1)) |> \n  ggplot(aes(week, rainfall, color = factor(year), group = year)) +\n    geom_smooth(se = FALSE) +\n    scale_color_brewer(direction = 1)\n\nrainfall_ts <- train_df |> \n  mutate(Date = as.Date(\"2010-01-01\") + day + year*365, rainfall = rainfall == 1) |> \n  select(Date, rainfall, sunshine, cloud) |> \n  fable.binary::as_tsibble()\n\nrainfall_fit <- rainfall_ts |> \n  model(\n        nn = BINNET(rainfall ~ fourier(K = 1, period = \"year\"))\n  ) \n\ntrain_df <- train_df |> \n  mutate(Date = as.Date(\"2010-01-01\") + day + year*365) |> \n  left_join(fitted(rainfall_fit), by = join_by(Date)) |> \n  rename(seasonal_chance = .fitted) |> \n  select(-Date, -.model)\n\nfc <- forecast(rainfall_fit, \n               new_data =  competition_df |> \n  mutate(Date = as.Date(\"2010-01-01\") + day + year*365) |> \n      select(Date, sunshine,cloud) |> \n  fable.binary::as_tsibble(),\n               h = \"2 years\")\n\nas_tibble(fc) |>\n    ggplot(aes(x = Date, y = .mean, col = .model)) +\n    geom_line(show.legend = FALSE) +\n    labs(y = \"Probability of rain\")\n\ncompetition_df <- competition_df |> \n  bind_cols(as_tibble(fc) |>  select(.mean)) |> \n  rename(seasonal_chance = .mean) \n\n```\n\n## Other Features\n\n```{r}\n#| label: Features\n#| warning: false\n#| message: false\n#| fig.width: 6\n\n\nfeatures <- train_df %>%\n  select(-id, -rainfall) |> \n  names()\n\ntrain_df <- train_df %>% \n  distinct(pick(all_of(features)), .keep_all = TRUE)\n\nnom_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\nLogical features: \n\n`r logical_features`\n\n\nSize of the combined train and competition datasets:\n\n`r nrow(all_df)`\n\nSize of the split made available to machine learning\n\n`r nrow(train_df)`\n\n\n\n## Numeric features\n\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 16\n#| fig.width: 6\n\ntrain_df %>% \n  select(all_of(num_features), rainfall) %>% \n  pivot_longer(-rainfall,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = rainfall)) +\n  geom_histogram(show.legend = FALSE, bins = 200) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.title.position = \"plot\") +\n  labs(color = NULL, fill = \"Loan Status\",\n       title = \"Numeric Feature Univariate Distributions\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n```\n\n\n\n## Counts of Missingness\n\nThe test set is missing one wind direction. We will fill it in with the median of all of the data.\n                  \n```{r}\n#| label: counts of missingness\n\ntrain_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n\ncompetition_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n\ncompetition_df <- competition_df %>% \n  mutate(across(where(is.numeric), ~replace_na(., median(., na.rm=TRUE))))          \n\n```\n\n## Counts of Distinct\n                   \n               \n```{r}\n#| label: counts of distinct\n               \ntrain_df %>%\n  summarize(across(all_of(features), n_distinct)) %>%\n  pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct train\") |>\n  left_join(\n    competition_df %>%\n      summarize(across(all_of(features), n_distinct)) %>%\n      pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct test\")\n  ) %>% \n                   knitr::kable()\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct rainfall?\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\n\nbind_rows(train_df %>% mutate(source = \"train\"),\n              competition_df %>% mutate(rainfall= NA_character_, source = \"test\")) |> \n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>% \n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>% \n    count(is_duplicated) %>% \n                   knitr::kable()\n               \n\n```\n                   \n\n\n\n## Target\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 6\n\n\ntrain_df %>%\n  summarize(outcome_sum = n(), .by = rainfall) %>%\n  arrange(-outcome_sum) %>%\n  mutate(prop = outcome_sum / nrow(train_df)) %>%\n  mutate(ypos = cumsum(prop) - 0.5 * prop) %>%\n  ggplot(aes(x = \"\", y = prop, fill = rainfall)) +\n  geom_bar(stat = \"identity\",\n           width = 1,\n           show.legend = FALSE) +\n  geom_text(\n    aes(\n      y = ypos,\n      label= paste0(rainfall\n                     , \"\\n\", round(prop, 2) * 100, \"%\")\n    ),\n    color = \"white\",\n    nudge_x = 0,\n    size = 3\n  ) +\n  coord_polar(\"y\", start = 0) +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank()  ) +\n  labs(title = \"Rainfall\", caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n```                            \n           \n                \n# Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n## Recipe\n\n```{r}\n#| label: recipe\n\nbase_rec <- recipe(\n    \n    formula(paste0(\"rainfall ~ \", \n               str_c(features,  collapse = \" + \"))),\n    data = train_df\n  ) %>% \n\n  step_harmonic(winddirection, \n                cycle_size = 300, \n                frequency = c(2,5,6,7,8),\n                starting_val = 10,\n                keep_original_cols = TRUE) |>                       \n\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n\nfolds <- vfold_cv(train_df, \n                  v = 17,\n                  strata = rainfall)                           \n                                     \n```\n\n\n\n## Workflowset Ensemble\n\n```{r}\n#| label: workflowset\n#| warning: false\n#| message: false\n#| fig.width: 12\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 600L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune(),\n   sample_size = 0.85\n#   mtry = tune(),\n#   loss_reduction = 9e-9\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n             num_threads = cores\n       #      boosting = \"goss\"   # this may slow the runtime\n             ) %>%\n  set_mode(mode = \"classification\") \n\nboost_tree_xgb_spec <- \n  boost_tree(\n    trees = 600L,\n    min_n = tune(),\n    learn_rate = tune(),\n   sample_size = 0.85\n  ) %>% \n  set_engine(engine = \"xgboost\", nthread = cores) %>%\n  set_mode(mode = \"classification\")        \n\nsvm_rbf_kernlab_spec <-\n  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%\n  set_engine('kernlab') %>%\n  set_mode('classification')\n\npls_mixOmics_spec <-\n  pls(predictor_prop = tune(), num_comp = tune()) %>%\n  set_engine('mixOmics') %>%\n  set_mode('classification')\n\ndep_models <- \n   workflow_set(\n      preproc = list(base = base_rec,\n                     base = base_rec,\n                     base = base_rec,\n                     base = base_rec),\n      models = list(xgb = boost_tree_xgb_spec,\n                    lgbm = boost_tree_lgbm_spec,\n                    svm = svm_rbf_kernlab_spec,\n                    pls = pls_mixOmics_spec),\n      cross = FALSE\n   ) %>% \n  option_add_parameters() |> \n  option_add(\n    control = control_stack_grid(),\n    metrics = metric_set(mn_log_loss)\n  )\n                   \nxgb_params <- dep_models |> \n  extract_workflow(\"base_xgb\") |> \n  parameters() |> \n  update(min_n = min_n(range = c(10,120)),\n         learn_rate = learn_rate(range = c(-2,-1)))\n\nlgbm_params <- dep_models |> \n  extract_workflow(\"base_lgbm\") |> \n  parameters() |> \n  update(tree_depth = tree_depth(range = c(20,90)),\n         num_leaves = num_leaves(),\n         learn_rate = learn_rate(range = c(-2,-1)),\n          min_n = min_n(range = c(40,120)))\n\nsvm_params <- dep_models |> \n    extract_workflow(\"base_svm\") |> \n  parameters() |> \n  update(\n    cost = cost(),\n    rbf_sigma = dials::rbf_sigma())\n\ndep_models <- dep_models |> \n  option_add(\n    param_info = lgbm_params,\n    id = \"base_lgbm\"\n  ) |> \n  option_add(\n    param_info = xgb_params,\n    id = \"base_xgb\"\n  ) |> \n  option_add(\n    param_info = svm_params,\n    id = \"base_svm\"\n  ) |> \n   workflow_map(\"tune_grid\", resamples = folds, grid = 11, \n                metrics = metric_set(roc_auc), verbose = TRUE)\n\nrank_results(dep_models, rank_metric = \"roc_auc\", select_best = TRUE)                    \n\nautoplot(dep_models) +\n  geom_text(aes(y = mean -0.03, label= wflow_id), angle = 90, hjust = 1)+\n  theme(legend.position = \"none\")\n\n```\n\n## Hyperparameters and Feature Importance\n\n```{r }                   \n#| label: XGB Details\n#| warning: false\n#| message: false                   \n                   \n\ndep_models %>%\n  dplyr::filter(grepl(\"xgb\", wflow_id)) %>%\n  dplyr::mutate(metrics = purrr::map(result, tune::collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(desc(mean))\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"base_xgb\") |> \n  autoplot() +\n  labs(title = \"XGB Hyperparameter Search\")\n\nbest_xgb_params <- dep_models |> \n  workflowsets::extract_workflow_set_result(\"base_xgb\") |> \n  select_best(metric = \"roc_auc\")  \n  \ndep_models |> \n  workflowsets::extract_workflow(\"base_xgb\") |> \n  finalize_workflow(best_xgb_params) |> \n  fit(train_df) |> \n  vip::vip(num_features = 30L)\n```\n\n\n```{r }                   \n#| label: lgbm Details\n#| warning: false\n#| message: false                   \n                                      \n\n\ndep_models %>%\n  dplyr::filter(grepl(\"lgbm\", wflow_id)) %>%\n  dplyr::mutate(metrics = purrr::map(result, tune::collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(desc(mean))\n\ndep_models |> \n  workflowsets::extract_workflow_set_result(\"base_lgbm\") |> \n  autoplot() +\n  labs(title = \"LGBM Hyperparameter Search\")\n\nbest_lgbm_params <- dep_models |> \n  workflowsets::extract_workflow_set_result(\"base_lgbm\") |> \n  select_best(metric = \"roc_auc\")  \n  \ndep_models |> \n  workflowsets::extract_workflow(\"base_lgbm\") |> \n  finalize_workflow(best_lgbm_params) |> \n  fit(train_df) |> \n  vip::vip(num_features = 30L)\n\n```\n\n\n```{r }                   \n#| label: svm Details\n#| warning: false\n#| message: false \n                   \ndep_models %>%\n  dplyr::filter(grepl(\"svm\", wflow_id)) %>%\n  dplyr::mutate(metrics = purrr::map(result, tune::collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(desc(mean))\n\n```\n\n\n```{r }                   \n#| label: pls Details\n#| warning: false\n#| message: false                    \ndep_models %>%\n  dplyr::filter(grepl(\"pls\", wflow_id)) %>%\n  dplyr::mutate(metrics = purrr::map(result, tune::collect_metrics)) %>%\n  dplyr::select(wflow_id, metrics) %>%\n  tidyr::unnest(cols = metrics) |> \n  arrange(desc(mean))\n```\n\n## Ensemble Stack                   \n\n```{r }                   \n#| label: Stack\n#| warning: false\n#| message: false \n\ndep_stack <- stacks() %>%\n  add_candidates(dep_models) %>%\n  blend_predictions(  metric = metric_set(roc_auc),\n      penalty = c(10^seq(-1.7, -0.3, 0.1)),\n      non_negative = TRUE,\n      control = tune::control_grid(allow_par = TRUE))\n\nautoplot(dep_stack)\nautoplot(dep_stack, type = \"weights\")\nautoplot(dep_stack, type = \"members\")                   \n\nclassification_fit <- dep_stack %>% \n    fit_members()\n\n```\n\n# Performance {.tabset .tabset-fade .tabset-pills}\n\n## Submission\n```{r }                   \n#| label: submission\n#| warning: false\n#| message: false\n                   \naugment(classification_fit, train_df) %>% \n  conf_mat(rainfall, .pred_class) %>%\n  yardstick:::autoplot.conf_mat(type = \"heatmap\")\n\nsubmit_df <-  augment(classification_fit , competition_df, type= \"prob\") %>%\n       transmute(id = round(id), rainfall = .pred_1)\n\nhead(submit_df)  %>% \n     knitr::kable()      \n\nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n```  ","metadata":{"_uuid":"d7508e7e-1415-4cf5-8d55-be725a2508b8","_cell_guid":"4d50f059-4837-4432-b32a-42c4ba709dfe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}