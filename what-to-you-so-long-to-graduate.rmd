{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73290,"databundleVersionId":8710574,"sourceType":"competition"}],"dockerImageVersionId":30618,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"---\ntitle: \"Classification with an Academic Success Dataset\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to build classification models to predict students dropout and academic sucess.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nIf you fork this on kaggle, be sure to choose the kernel Environment setting for \"Always use latest environment\"\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s4e6/\"\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\"))\n# install.packages(\"xrf\", quiet = TRUE, verbose = FALSE, dependencies = TRUE)\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n}\n\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\n\nlibrary(baguette)    \nlibrary(bonsai)\nlibrary(catboost)    \nlibrary(stacks)    # ensemble stack of many models, using glmnet\n    \n})\n\ntidymodels_prefer()\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(cowplot::theme_minimal_grid())\n\n```\n\n\n## Load Data\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\n\ntrain_spec <- cols(\n  id = col_double(),\n  `Marital status` = col_character(),\n  `Application mode` = col_character(),\n  `Application order` = col_double(),\n  Course = col_character(),\n  `Daytime/evening attendance` = col_character(),\n  `Previous qualification` = col_character(),\n  `Previous qualification (grade)` = col_double(),\n  Nacionality = col_character(),\n  `Mother's qualification` = col_character(),\n  `Father's qualification` = col_character(),\n  `Mother's occupation` = col_character(),\n  `Father's occupation` = col_character(),\n  `Admission grade` = col_double(),\n  Displaced = col_character(),\n  `Educational special needs` = col_character(),\n  Debtor = col_character(),\n  `Tuition fees up to date` = col_character(),\n  Gender = col_character(),\n  `Scholarship holder` = col_character(),\n  `Age at enrollment` = col_double(),\n  International = col_character(),\n  `Curricular units 1st sem (credited)` = col_double(),\n  `Curricular units 1st sem (enrolled)` = col_double(),\n  `Curricular units 1st sem (evaluations)` = col_double(),\n  `Curricular units 1st sem (approved)` = col_double(),\n  `Curricular units 1st sem (grade)` = col_double(),\n  `Curricular units 1st sem (without evaluations)` = col_double(),\n  `Curricular units 2nd sem (credited)` = col_double(),\n  `Curricular units 2nd sem (enrolled)` = col_double(),\n  `Curricular units 2nd sem (evaluations)` = col_double(),\n  `Curricular units 2nd sem (approved)` = col_double(),\n  `Curricular units 2nd sem (grade)` = col_double(),\n  `Curricular units 2nd sem (without evaluations)` = col_double(),\n  `Unemployment rate` = col_double(),\n  `Inflation rate` = col_double(),\n  GDP = col_double(),\n  Target = col_character()\n)\n\ncompetition_spec <- cols(\n  id = col_double(),\n  `Marital status` = col_character(),\n  `Application mode` = col_character(),\n  `Application order` = col_double(),\n  Course = col_character(),\n  `Daytime/evening attendance` = col_character(),\n  `Previous qualification` = col_character(),\n  `Previous qualification (grade)` = col_double(),\n  Nacionality = col_character(),\n  `Mother's qualification` = col_character(),\n  `Father's qualification` = col_character(),\n  `Mother's occupation` = col_character(),\n  `Father's occupation` = col_character(),\n  `Admission grade` = col_double(),\n  Displaced = col_character(),\n  `Educational special needs` = col_character(),\n  Debtor = col_character(),\n  `Tuition fees up to date` = col_character(),\n  Gender = col_character(),\n  `Scholarship holder` = col_character(),\n  `Age at enrollment` = col_double(),\n  International = col_character(),\n  `Curricular units 1st sem (credited)` = col_double(),\n  `Curricular units 1st sem (enrolled)` = col_double(),\n  `Curricular units 1st sem (evaluations)` = col_double(),\n  `Curricular units 1st sem (approved)` = col_double(),\n  `Curricular units 1st sem (grade)` = col_double(),\n  `Curricular units 1st sem (without evaluations)` = col_double(),\n  `Curricular units 2nd sem (credited)` = col_double(),\n  `Curricular units 2nd sem (enrolled)` = col_double(),\n  `Curricular units 2nd sem (evaluations)` = col_double(),\n  `Curricular units 2nd sem (approved)` = col_double(),\n  `Curricular units 2nd sem (grade)` = col_double(),\n  `Curricular units 2nd sem (without evaluations)` = col_double(),\n  `Unemployment rate` = col_double(),\n  `Inflation rate` = col_double(),\n  GDP = col_double()\n)\n\n\npreprocessor <- function(dataframe) {\n\ndataframe <- dataframe %>%\n    janitor::clean_names() %>%\n    mutate(across(c(where(is.character)), ~ as.factor(.x))) \n\nreturn(dataframe)\n}\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   col_types = train_spec,\n                   show_col_types = FALSE) %>%\n          preprocessor() \n\ncompetition_df <- read_csv(str_c(path, \"test.csv\"),\n                   col_types = competition_spec,\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nall_df <-\n    bind_rows(raw_df %>% mutate(source = \"train\"),\n            competition_df %>% mutate(source = \"test\"))\n\ntrain_df <- all_df %>% \n  filter(source == \"train\") %>% \n  select(-source)\n\ncompetition_df <- all_df %>% \n  filter(source == \"test\") %>% \n  select(-source)\n\n\nfeatures <- train_df %>%\n  select(-id, target) %>%\n  names()\n\nraw_df <- train_df %>% \n  distinct(pick(all_of(features)), .keep_all = TRUE)\n\nnom_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n\n\n\n\n\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Numeric features\n\nConsider where features require univariate transformation, or clipping outliers.\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 24\n#| fig.width: 12\n\ntrain_df %>% \n  select(all_of(num_features), target ) %>% \n  pivot_longer(-target,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = target)) +\n  geom_histogram(bins = 50) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 3) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  labs(color = NULL, fill = \"Target\",\n       title = \"Numeric Feature Univariate Distributions\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n\n\n```\n\n\n## Nominal features\n\n```{r}\n#| label: nominal\n#| warning: false\n#| message: false\n#| fig.height: 18\n#| fig.width: 12\n\n\nif(length(nom_features) >0){\n\ntrain_df %>% \n  select(all_of(nom_features), target) %>% \n  pivot_longer(-target,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = target)) +\n  geom_bar() +\n   coord_flip() +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 4) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Nominal Feature Counts\",\n       fill = \"Target\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n}\n\n```\n\n## Counts of Missingness\n                  \n```{r}\n#| label: counts of missingness\n\ntrain_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n```\n\n## Counts of Distinct\n               \n```{r}\n#| label: counts of distinct\n               \ntrain_df %>% \n  summarize(\n    across(all_of(features), n_distinct)\n  ) %>%\n  pivot_longer(everything(),\n               names_to = \"feature\",\n               values_to = \"Count of distinct\") %>% \n                   knitr::kable()\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct label?\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\nall_df %>%\n    select(all_of(features), source) %>% \n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>%\n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>%\n    count(is_duplicated) %>% \n                   knitr::kable()\n               \n\n```\n                   \n## Pairwise Correlations\n                   \n`ggcorrplot` provides a quick look at numeric features where the correlation may be significant. \n                         \n\n```{r}\n#| label: pairwise correlations\n# Leave blank on no significant coefficient\n#| fig.height: 12\n#| fig.width: 12\n                   \ncorr <- train_df %>%\n  select(all_of(num_features)) %>%\n  cor()\n\np.mat <-train_df %>%\n  select(all_of(num_features)) %>%\n  ggcorrplot::cor_pmat() \n\nggcorrplot::ggcorrplot(\n    corr,\n    hc.order = TRUE,\n    lab_size = 1.5,\n    tl.cex = 4,\n    pch.cex = 8,\n    digits = 2,\n    lab = TRUE,\n    type = \"lower\",\n    show.diag = TRUE,\n    insig = \"blank\",\n    p.mat = p.mat\n  ) +\n  labs(title = \"Pairwise Correlations Training Set\")\n\nggcorrplot::ggcorrplot(\n   competition_df %>%\n    select(all_of(num_features)) %>%\n    cor(),\n    hc.order = TRUE,\n    lab_size = 1.5,\n    tl.cex = 4, \n    pch.cex = 8,\n    digits = 2,\n    lab = TRUE,\n    type = \"lower\",\n   show.diag = TRUE,\n    insig = \"blank\",\n    p.mat = competition_df %>%\n  select(all_of(num_features)) %>%\n  ggcorrplot::cor_pmat() \n  ) +\n  labs(title = \"Pairwise Correlations Competition Set\")\n\n```      \n\n\n## Target\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 12\n\ntrain_df %>% \n  summarize(outcome_sum = n(),\n            .by = target) %>%\n  arrange(desc(outcome_sum)) %>%\n  mutate(prop = outcome_sum / nrow(raw_df)) %>%\n  mutate(ypos = cumsum(prop) - 0.5 * prop) %>%\n  ggplot(aes(x = \"\", y = prop, fill = target)) +\n  geom_bar(stat = \"identity\", width = 1, show.legend = FALSE) +\n  geom_text(aes(y = ypos, label = paste0(target\n                                  ,\"\\n\",round(prop,2)*100,\"%\")),\n            color = \"white\",\n            nudge_x = 0.3,\n            size = 4) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() + \n  labs(title = \"Target\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n```\n                              \n           \n                \n# Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n## Recipe\n\n```{r}\n#| label: recipe\n\nrec <- recipe(\n    \n    formula(paste0(\"target ~ \", \n               str_c(features,  collapse = \" + \"))),\n    data = train_df \n  ) %>% \n  step_novel(all_nominal_predictors()) %>%\n  step_other(all_nominal_predictors(), threshold = 0.007) %>% \n  step_zv(all_predictors())\n                   \nfolds <- vfold_cv(train_df, \n                  v = 10,\n                  repeats = 1,\n                  strata = target)    \n                   \nctrl <- finetune::control_sim_anneal(     \n     verbose = FALSE,\n     verbose_iter = TRUE,\n     parallel_over = \"everything\",\n     save_pred = TRUE,\n     save_workflow = TRUE)                   \n                                     \n```\n\n\n## Ranger RF\n\n```{r}\n#| label: ranger\n                   \nrf_spec <- rand_forest(trees = 1000) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"ranger\")\n\nwf <- workflow(rec, rf_spec)\n\nrf_rs <- fit_resamples(\n  wf,\n  resamples = folds,\n  control = ctrl,\n  metrics = metric_set(mn_log_loss)\n)\n\ncollect_metrics(rf_rs)\n\n``` \n                   \n                   \n\n## Catboost\n\n```{r}\n#| label: catboost \n\nboost_tree_spec <- \n  boost_tree(\n    trees = 1000L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune(),\n   mtry = tune()\n  ) %>% \n  set_engine(engine = \"xgboost\", method = \"catboost\",\n             nthread = future::availableCores()\n             ) %>%\n  set_mode(mode = \"classification\") \n                   \n\ntictoc::tic()\nwf <- workflow(rec %>%                \n               step_dummy(all_nominal_predictors()) %>% \n               step_zv(all_predictors()) ,\n               boost_tree_spec) \n\nparam <- wf %>%\n   extract_parameter_set_dials() %>%\n   recipes::update(\n      mtry = mtry(range = c(1,10)),\n      min_n = min_n(range = c(20, 45)),\n      tree_depth = tree_depth(range = c(3, 90)),\n      learn_rate = learn_rate(range = c(-1.4, 0.3))\n   ) %>%\n   dials::finalize(raw_df)                 \n\nburnin <- tune_grid(\n  wf,\n  grid = 4,\n  resamples = folds,\n  control = ctrl,\n  metrics = metric_set(mn_log_loss),\n  param_info = param)\n\ncat_rs <- finetune::tune_sim_anneal(\n  wf,\n  resamples = folds,\n  iter = 12,\n  initial = burnin,\n  control = ctrl,\n  metrics =  metric_set(mn_log_loss),\n  param_info = param) \n\nshow_best(cat_rs)  \n\nautoplot(cat_rs)\n\nclassification_fit <- wf %>%\n  finalize_workflow(select_best(cat_rs)) %>%\n  fit(raw_df)\n\naugment(classification_fit, train_df) %>% \n  accuracy(target, .pred_class)\n                   \naugment(classification_fit, train_df) %>% \n  conf_mat(target, .pred_class) %>%\n  autoplot( type = \"heatmap\")\n                   \n\n```                            \n                   \n                   \n## GLMNet\n                   \n                   \n```{r}\n#| label: glmnet \n   \nlogistic_reg_glmnet_spec <-\n  multinom_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine('glmnet')\n\nwf <- workflow(rec %>% \n               step_dummy(all_nominal_predictors()) %>% \n               step_zv(all_predictors()) %>% \n               step_normalize(all_numeric_predictors()), logistic_reg_glmnet_spec)\n\nparam <- wf %>%\n   extract_parameter_set_dials() %>%\n   recipes::update(\n     penalty = penalty(),\n     mixture = mixture()\n   ) %>%\n   dials::finalize(raw_df)                 \n\nglmnet_rs <- tune_grid(\n  wf,\n  grid = 5,\n  resamples = folds,\n  control = ctrl,\n  metrics = metric_set(mn_log_loss),\n  param_info = param)\n\nshow_best(glmnet_rs)  \n\nautoplot(glmnet_rs)\n\nclassification_fit <- wf %>% \n  finalize_workflow(select_best(glmnet_rs)) %>% \n  fit(raw_df)\n\naugment(classification_fit, train_df) %>% \n  accuracy(target, .pred_class)\n\naugment(classification_fit, train_df) %>% \n  conf_mat(target, .pred_class) %>%\n  autoplot( type = \"heatmap\")\n```                  \n                   \n\n## Lightgbm \n\n```{r}\n#| label: lightgbm \n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 2500L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune(),\n   loss_reduction = 0\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n          #   boosting = \"goss\",\n             num_threads = future::availableCores()\n             ) %>%\n  set_mode(mode = \"classification\") \n                   \n\ntictoc::tic()\nwf <- workflow(rec,\n               boost_tree_lgbm_spec) \n\nparam <- wf %>%\n   extract_parameter_set_dials() %>%\n   recipes::update(\n      min_n = min_n(range = c(1, 7)),\n      tree_depth = tree_depth(range = c(25, 80)),\n      learn_rate = learn_rate(range = c(-2.4, -1.7)),\n      num_leaves = num_leaves(range = c(20, 60))\n   ) %>%\n   dials::finalize(raw_df)                 \n\nburnin <- tune_grid(\n  wf,\n  grid = 4,\n  resamples = folds,\n  control = ctrl,\n  metrics = metric_set(mn_log_loss),\n  param_info = param)\n\nlgbm_rs <- finetune::tune_sim_anneal(\n  wf,\n  resamples = folds,\n  iter = 12,\n  initial = burnin,\n  control = ctrl,\n  metrics =  metric_set(mn_log_loss),\n  param_info = param) \n\nshow_best(lgbm_rs)  \n\nautoplot(lgbm_rs)\n\nclassification_fit <- wf %>%\n  finalize_workflow(select_best(lgbm_rs)) %>%\n  fit(raw_df)\n\naugment(classification_fit, train_df) %>% \n  accuracy(target, .pred_class)\n                   \naugment(classification_fit, train_df) %>% \n  conf_mat(target, .pred_class) %>%\n  autoplot( type = \"heatmap\")\n                   \n\nclassification_fit %>% \n  extract_fit_engine() %>% \n  lightgbm::lgb.importance() %>% \n  lightgbm::lgb.plot.importance()  \n\n```         \n                   \n                   \n\n\n## Stacks\n\n```{r}\n#| label: stacks\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12                   \n                   \nfuture::plan(\"multisession\", workers = future::availableCores())\n\nens <- stacks() %>%\n  stacks::add_candidates(lgbm_rs) %>%\n  stacks::add_candidates(cat_rs) %>%\n  stacks::add_candidates(rf_rs) %>%\n  stacks::add_candidates(glmnet_rs) %>%                 \n  stacks::blend_predictions(\n    metric = metric_set(accuracy),\n    penalty = c(seq(0.001, 0.15, 0.005)),\n    mixture = c(seq(0, 0.10, 0.05)),\n    non_negative = FALSE,\n    control = tune::control_grid(allow_par = TRUE)\n  ) \n\nfuture::plan(\"sequential\")\n\nautoplot(ens)\n\nautoplot(ens, \"weights\")\n\nensemble <- fit_members(ens)\n                   \naugment(ensemble, train_df) %>% \n  accuracy(target, .pred_class)                   \n\n```                   \n                   \n## Submission\n                 \n             \n                   \n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n\nsubmit_df <- augment(ensemble, competition_df, type = \"class\") %>%\n             select(id, Target = .pred_class)\n\nhead(submit_df)  %>% \n     knitr::kable()      \n\nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n```       ","metadata":{"_uuid":"152faf12-7a21-45d7-9457-19ace63b23d9","_cell_guid":"48d92e85-1c98-408d-981e-3c0bdf98b7c9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}