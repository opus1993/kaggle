{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":83504,"databundleVersionId":9257194,"sourceType":"competition"}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jimgruman/don-t-let-the-fear-of-striking-out-hold-you-back?scriptVersionId=191341256\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"---\ntitle: \"Predict hits with NEW MLB swing data\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict hits with NEW MLB swing data.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nIf you fork this on kaggle, be sure to choose the kernel Environment setting for \"Always use latest environment\"\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/nwds-batted-balls/\"\n    \noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2021-03-22\"))\n# install.packages(\"vip\", quiet = TRUE)\n    \ncores <- future::availableCores()\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n\ncores <- future::availableCores(omit = 1)\n}\n\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\nlibrary(bonsai)\n    \nlibrary(DALEXtra) # Tidymodels Explainability    \n})\n\ntidymodels_prefer()\n\noptions(tidymodels.dark = TRUE)\n\ntheme_set(cowplot::theme_minimal_grid())\n\n```\n\n\n## Load Data\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\n\ntrain_spec <- cols(\n  uid = col_double(),\n  pitch_type = col_character(),\n  release_speed = col_double(),\n  release_pos_x = col_double(),\n  release_pos_z = col_double(),\n  game_type = col_character(),\n  is_lhp = readr::col_factor(),\n  is_lhb = readr::col_factor(),\n  balls = readr::col_factor(),\n  strikes = readr::col_factor(),\n  game_year = readr::col_factor(),\n  pfx_x = col_double(),\n  pfx_z = col_double(),\n  plate_x = col_double(),\n  plate_z = col_double(),\n  on_3b = readr::col_character(),\n  on_2b = readr::col_character(),\n  on_1b = readr::col_character(),\n  outs_when_up = readr::col_factor(),\n  inning = readr::col_factor(),\n  is_top = readr::col_factor(),\n  vx0 = col_double(),\n  vy0 = col_double(),\n  vz0 = col_double(),\n  ax = col_double(),\n  ay = col_double(),\n  az = col_double(),\n  sz_top = col_double(),\n  sz_bot = col_double(),\n  effective_speed = col_double(),\n  release_spin_rate = col_double(),\n  release_extension = col_double(),\n  release_pos_y = col_double(),\n  pitch_number = readr::col_factor(),\n  pitch_name = readr::col_factor(),\n  spin_axis = col_double(),\n  spray_angle = col_double(),\n  bat_speed = col_double(),\n  swing_length = col_double(),\n  outcome = col_character(),\n  outcome_code = col_character()\n)\n\n\ncompetition_spec <- cols(\n  uid = col_double(),\n  pitch_type = col_character(),\n  release_speed = col_double(),\n  release_pos_x = col_double(),\n  release_pos_z = col_double(),\n  game_type = col_character(),\n  is_lhp = readr::col_factor(),\n  is_lhb = readr::col_factor(),\n  balls = readr::col_factor(),\n  strikes = readr::col_factor(),\n  game_year = readr::col_factor(),\n  pfx_x = col_double(),\n  pfx_z = col_double(),\n  plate_x = col_double(),\n  plate_z = col_double(),\n  on_3b = readr::col_character(),\n  on_2b = readr::col_character(),\n  on_1b = readr::col_character(),\n  outs_when_up = readr::col_factor(),\n  inning = readr::col_factor(),\n  is_top = readr::col_factor(),\n  vx0 = col_double(),\n  vy0 = col_double(),\n  vz0 = col_double(),\n  ax = col_double(),\n  ay = col_double(),\n  az = col_double(),\n  sz_top = col_double(),\n  sz_bot = col_double(),\n  effective_speed = col_double(),\n  release_spin_rate = col_double(),\n  release_extension = col_double(),\n  release_pos_y = col_double(),\n  pitch_number = readr::col_factor(),\n  pitch_name = readr::col_factor(),\n  spin_axis = col_double(),\n  spray_angle = col_double(),\n  bat_speed = col_double(),\n  swing_length = col_double()\n)\n\npreprocessor <- function(dataframe) {\n\ndataframe <- dataframe %>%\n    janitor::clean_names() %>%\n    \n    mutate(across(starts_with(\"on_\"), ~ if_else(.x == \"-1\", \"-1\", \"0\"))) %>%\n  \n    mutate(pitch_speed_diff = release_speed - effective_speed, # from Michael Semenoff notebook\n           pitch_location = sqrt(plate_x^2 + plate_z^2),\n           count_pressure = as.numeric(balls) - as.numeric(strikes) + 1,\n           bat_pitch_speed_ratio = bat_speed / release_speed,\n           swing_efficiency = bat_speed / swing_length) %>%\n    \n    mutate(cos_spray_angle = cos(spray_angle * pi / 180),\n           sin_spray_angle = sin(spray_angle * pi / 180)) %>%\n  \n    mutate(across(c(where(is.character)), ~ as.factor(.x))) \n\nreturn(dataframe)\n}\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   col_types = train_spec,\n                   show_col_types = FALSE) %>%\n          preprocessor() \n\ntst_df <- read_csv(str_c(path, \"test.csv\"),\n                   col_types = competition_spec,\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nall_df <-\n    bind_rows(raw_df %>% mutate(source = \"train\"),\n              tst_df %>% mutate(source = \"test\"))\n\ntrain_df <- all_df %>% \n  filter(source == \"train\") %>% \n  select(-source, -outcome_code, -game_year, -game_type, -spray_angle) %>% \n  mutate(outcome = factor(outcome, levels = c(\"out\",\"single\",\"double\",\"triple\",\"home_run\")))\n\ncompetition_df <- all_df %>% \n  filter(source == \"test\") %>% \n  select(-source, -outcome, -outcome_code, -game_year, -game_type, -spray_angle)\n\nfeatures <- train_df %>%\n  select(-uid, -outcome) %>%\n  names()\n\ntrain_df <- train_df %>% \n  distinct(pick(all_of(features)), .keep_all = TRUE)\n\nnom_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n\nbundle_split <- initial_split(train_df, prop = 0.95, strata = outcome)\nbundle_train <- training(bundle_split)\nbundle_test <- testing(bundle_split)\n\n\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\nLogical features: \n\n`r logical_features`\n\n\nSize of the combined train and competition datasets:\n\n`r nrow(all_df)`\n\nSize of the split made available to machine learning\n\n`r nrow(bundle_train)`\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Numeric features\n\nConsider where features require univariate transformation, or clipping outliers.\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 24\n#| fig.width: 6\n\ntrain_df %>% \n  select(all_of(num_features), outcome ) %>% \n  pivot_longer(-outcome,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = outcome)) +\n  geom_histogram(bins = 200) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\", \n        plot.title.position = \"plot\") +\n  labs(color = NULL, fill = NULL,\n       title = \"Numeric Feature Univariate Distributions\",\n       caption = \"Data: MLB Statcast via Baseball Savant | Visual: Jim Gruman\")\n```\n\n\n```{r}\n#| label: plate position\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6\n\ntrain_df |> \n  ggplot(aes(plate_x, plate_z, z = outcome)) +\n  stat_bin_hex(bins = 30,aes(z = after_stat(density)), show.legend = FALSE) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  facet_wrap(vars(outcome), ncol = 5) +\n  coord_fixed() +\n  labs(fill = NULL, caption = \"Data: MLB Statcast via Baseball Savant | Visual: Jim Gruman\") +\n  theme(legend.position = \"top\") \n\n```\n\n```{r}\n#| label: GGally pairwise\n#| warning: false\n#| message: false\n#| fig.height: 18\n#| fig.width: 18\n\ntrain_df |> \n    slice_sample(prop = 0.1) %>%\n    select(all_of(num_features), outcome) %>% \n    GGally::ggpairs(aes(color = outcome), columns = 1:7, progress = FALSE, bins = 30)\n\ntrain_df |> \n    slice_sample(prop = 0.1) %>%\n    select(all_of(num_features), outcome) %>% \n    GGally::ggpairs(aes(color = outcome), columns = 8:16, progress = FALSE, bins = 30)\n\ntrain_df |> \n    slice_sample(prop = 0.1) %>%\n    select(all_of(num_features), outcome) %>% \n    GGally::ggpairs(aes(color = outcome), columns = 17:29, progress = FALSE, bins = 30)\n\ntrain_df |> \n    slice_sample(prop = 0.1) %>%\n    select(all_of(num_features), outcome) %>% \n    GGally::ggpairs(aes(color = outcome), columns = c(\"spray_angle\",\"plate_x\",\"swing_length\",\"bat_speed\",\"plate_z\"), progress = FALSE, bins = 30)\n\n```\n\n## Nominal features\n\nExplore the distribution of outcome class by factor level, and the factor levels that exist in test that do not exist in training.\n\n\n```{r}\n#| label: nominal\n#| warning: false\n#| message: false\n#| fig.height: 18\n#| fig.width: 6\n\n\nif(length(nom_features) >0){\n\nall_df %>% \n  select(all_of(nom_features), outcome) %>% \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-outcome,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  \n  summarise(n = n(),\n            .by = c(outcome, metric, value)) %>%\n      \n  mutate(value = tidytext::reorder_within(value, n, metric)) %>%\n    \n  ggplot(aes(x = n, y = value, fill = outcome)) +\n  geom_col() +\n  tidytext::scale_y_reordered() +\n  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Nominal Feature Counts\",\n       fill = NULL,\n       caption = \"Data: MLB Statcast via Baseball Savant | Visual: Jim Gruman\")\n\n}\n\n```\n\n## Logical features\n\n```{r}\n#| label: logical\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6\n\n\nif(length(logical_features) >0){\n\ntrain_df %>% \n  select(all_of(logical_features), class) %>% \n  pivot_longer(-class,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(y = value, fill = class)) +\n  geom_bar() +\n  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2)) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Logical Feature Counts\",\n       fill = \"Class\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n}\n\n```\n\n## Counts of Missingness\n                  \n```{r}\n#| label: counts of missingness\n\n#naniar::gg_miss_upset(train_df,  nintersects = 6) \n\n#naniar::vis_miss(train_df |> slice_sample(prop = 0.01))\n\ntrain_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n                   \n\n                  \n```\n\n## Counts of Distinct\n                   \n               \n```{r}\n#| label: counts of distinct\n               \nraw_df %>%\n  summarize(across(all_of(features), n_distinct)) %>%\n  pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct train\") |>\n  left_join(\n    tst_df %>%\n      summarize(across(all_of(features), n_distinct)) %>%\n      pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct test\")\n  ) %>% \n                   knitr::kable()\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct label?\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\nall_df %>%\n    select(all_of(features), source) %>% \n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>%\n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>%\n    count(is_duplicated) %>% \n                   knitr::kable()\n               \n\n```\n                   \n\n\n\n## Target\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 6\n\n\nbundle_train %>% \n  summarize(outcome_sum = n(),\n            .by = outcome) %>%\n  arrange(outcome_sum) %>%\n  mutate(prop = outcome_sum / nrow(bundle_train)) %>%\n  mutate(ypos = cumsum(prop) - 0.5 * prop) %>%\n\n  ggplot(aes(x = \"\", y = prop, fill = outcome)) +\n  geom_bar(stat = \"identity\", width = 1, show.legend = FALSE) +\n  geom_text(aes(y = ypos, label = paste0(outcome\n                                  ,\"\\n\",round(prop,2)*100,\"%\")),\n            color = \"white\",\n            nudge_x = 0.3,\n            size = 3) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() + \n  labs(title = \"Outcome\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n```\n                              \n           \n                \n# Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n\n## Recipe\n\n```{r}\n#| label: recipe\n                   \nrec <- recipe(\n    \n    formula(paste0(\"outcome ~ \", \n               str_c(features,  collapse = \" + \"))),\n    data = bundle_train\n  ) %>% \n  step_novel(all_nominal_predictors()) %>%\n  step_nzv(all_predictors())\n                   \nfolds <- vfold_cv(bundle_train, \n                  v = 3,\n                  repeats = 1,\n                  strata = outcome)    \n                   \nctrl <- finetune::control_sim_anneal(     \n     verbose = FALSE,\n     verbose_iter = TRUE,\n     save_pred = TRUE,\n     parallel_over = \"everything\",\n     save_workflow = FALSE)                   \n                                     \n```\n\n\n\n## Lightgbm\n\n```{r}\n#| label: lgbm\n\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 2000L,\n   tree_depth = tune(),\n   learn_rate =  tune(),\n   min_n = tune(),\n   mtry = tune(),\n   loss_reduction = 0\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n             num_threads = cores\n             ) %>%\n  set_mode(mode = \"classification\") \n                   \nwf <- workflow(rec,\n               boost_tree_lgbm_spec) \n\nparam <- wf %>%\n   extract_parameter_set_dials() %>%\n   recipes::update(\n      min_n = min_n(range = c(5,30)),\n      tree_depth = tree_depth(range = c(4,30)),\n      learn_rate = learn_rate(range = c(-1.1,-1.7)),\n      num_leaves = num_leaves(range = c(100,400))\n   ) %>%\n   dials::finalize(raw_df)                 \n\nburnin <- tune_grid(\n  wf,\n  grid = 4,\n  resamples = folds,\n  control = ctrl,\n  metrics = metric_set(roc_auc, accuracy),\n  param_info = param)\n\nlgbm_rs <- finetune::tune_sim_anneal(\n  wf,\n  resamples = folds,\n  iter = 3,\n  initial = burnin,\n  control = ctrl,\n  metrics =  metric_set(roc_auc, accuracy),\n  param_info = param) \n\nhighest_roc <-   select_best(lgbm_rs, metric = \"roc_auc\")                       \n                   \nlgbm_rs %>% \n  collect_predictions(parameters = highest_roc) %>% \n  group_by(id) %>%\n  roc_curve(outcome, .pred_out, .pred_single, .pred_double, .pred_triple, .pred_home_run) %>%\n  autoplot()                \n                   \nshow_best(lgbm_rs, metric = \"roc_auc\")  \n\nautoplot(lgbm_rs)\n\ncollect_metrics(lgbm_rs) %>% \n  filter(.metric == \"roc_auc\") %>% \n  mutate(.config = fct_reorder(.config, mean)) %>% \n  ggplot(aes(mean, .config)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = mean - std_err, xmax = mean + std_err)) +\n  labs(title = \"ROC AUC across resample folds with std_err\")\n                   \n\n```\n                   \n                   \n# Model Explainability\n\n\n```{r}\n#| label: explainability\n#| warning: false\n#| message: false\n#| fig.height: 12\n#| fig.width: 6                   \nfinal_lgbm <- wf %>%\n    finalize_workflow(highest_roc) \n\nlast_fit(\n  final_lgbm,\n  bundle_split,\n  metrics = metric_set(roc_auc, accuracy)\n) %>%\n  collect_metrics() %>%\n  knitr::kable()\n\nclassification_fit <- final_lgbm %>%\n                        fit(bundle_train)                    \n\nexplainer <- \n  explain_tidymodels(\n    classification_fit, \n    data = train_df |> select(all_of(features)), \n    y = as.numeric(train_df$outcome),\n    label = \"Ensemble\",\n    verbose = FALSE\n  )  %>% \n  model_parts()\n                   \nggplot_imp <- function(...) {\n  obj <- list(...)\n  metric_name <- attr(obj[[1]], \"loss_name\")\n  metric_lab <- paste(metric_name, \n                      \"after permutations\\n(higher indicates more important)\")\n  \n  full_vip <- bind_rows(obj) %>%\n    filter(variable != \"_baseline_\")\n  \n  perm_vals <- full_vip %>% \n    filter(variable == \"_full_model_\") %>% \n    group_by(label) %>% \n    summarise(dropout_loss = mean(dropout_loss))\n  \n  p <- full_vip %>%\n    filter(variable != \"_full_model_\") %>% \n    mutate(variable = fct_reorder(variable, dropout_loss)) %>%\n    ggplot(aes(dropout_loss, variable)) \n  if(length(obj) > 1) {\n    p <- p + \n      facet_wrap(vars(label)) +\n      geom_vline(data = perm_vals, aes(xintercept = dropout_loss, color = label),\n                 linewidth = 1.4, lty = 2, alpha = 0.7) +\n      geom_boxplot(aes(color = label, fill = label), alpha = 0.2)\n  } else {\n    p <- p + \n      geom_vline(data = perm_vals, aes(xintercept = dropout_loss),\n                 linewidth = 1.4, lty = 2, alpha = 0.7) +\n      geom_boxplot(fill = \"#91CBD765\", alpha = 0.4)\n    \n  }\n  p +\n    theme(legend.position = \"none\") +\n    labs(x = metric_lab, \n         y = NULL,  fill = NULL,  color = NULL,  caption = \"LightGBM Model | Visual: Jim Gruman\")\n}\n                   \nggplot_imp(explainer)                   \n\n```                       \n                   \n# Submission\n                   \n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n             \n                    \n\nsubmit_df <-  augment(classification_fit, competition_df) %>%\n       select(uid, out = .pred_out, single = .pred_single, double = .pred_double, triple = .pred_triple, home_run = .pred_home_run)\n\nhead(submit_df)  %>% \n     knitr::kable()      \n\nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n```    ","metadata":{"_uuid":"6711b35a-33eb-4cb0-93b0-016e9df90181","_cell_guid":"c94c821f-017d-42ed-aa7b-04437069ac7d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}