{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"---\ntitle: \"Car Prices\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    number_sections: true\n    fig_caption: true\n    toc: true\n    fig_width: 7\n    fig_height: 4.5\n    theme: cosmo\n    highlight: tango\n    code_folding: hide\n---\n  \n# Introduction  {.tabset .tabset-fade .tabset-pills}\n\nThe goal of this competition is to predict car prices.\n\nMy notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.\n\nIf you fork this on kaggle, be sure to choose the kernel Environment setting for \"Always use latest environment\"\n\nLets dive right in.\n\nThe Kaggle kernels have many of the common r packages built in.  \n\n## Load libraries\n\nIn addition to `tidymodels` we will load the `bonsai` interface to lightgbm.\n\n```{r }\n#| label: setup\n#| warning: false\n#| message: false\n\nif (dir.exists(\"/kaggle\")){\n  path <- \"/kaggle/input/playground-series-s4e9/\"\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2021-03-22\"))\n# install.packages(\"vip\", quiet = TRUE)\n    \ncores <- future::availableCores()\n\n} else {\n  path <- stringr::str_c(here::here(\"data\"),\"/\")\n\ncores <- future::availableCores(omit = 1)\n}\n\n \nsuppressPackageStartupMessages({\nlibrary(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages\nlibrary(tidymodels) # metapackage see https://www.tidymodels.org/\nlibrary(bonsai)\n})\n\ntidymodels_prefer()\n\noptions(tidymodels.dark = TRUE)\n\ntheme_kaggle <- function(){\n  theme_minimal(base_family = \"open Sans\", base_size = 12) +\n    theme(plot.title.position = \"plot\")\n}\n\nupdate_geom_defaults(\n  geom = \"text\",\n  aes(family = \"Open Sans\")\n)\n\n\nscale_colour_brewer_d <- function(..., palette = \"Spectral\") {\n  scale_colour_brewer(..., palette = palette )\n}\n\nscale_fill_brewer_d <- function(..., palette = \"Spectral\") {\n  scale_fill_brewer(..., palette = palette)\n}\n\noptions(\n  ggplot2.discrete.colour = scale_colour_brewer_d,\n  ggplot2.discrete.fill = scale_fill_brewer_d\n)\n\nggplot2::theme_set(theme_kaggle())\n\n\n```\n\n\n## Load Data\n\n```{r }\n#| label: load data\n#| warning: false\n#| message: false\n\n\ntrain_spec <- cols(\n  id = col_integer(),\n  brand = col_character(),\n  model = col_character(),\n  model_year = col_character(),\n  milage = col_integer(),\n  fuel_type = col_character(),\n  engine = col_character(),\n  transmission = col_character(),\n  ext_col = col_character(),\n  int_col = col_character(),\n  accident = col_character(),\n  clean_title = col_character(),\n  price = col_integer()\n)\n\n\ncompetition_spec <- cols(\n  id = col_integer(),\n  brand = col_character(),\n  model = col_character(),\n  model_year = col_character(),\n  milage = col_integer(),\n  fuel_type = col_character(),\n  engine = col_character(),\n  transmission = col_character(),\n  ext_col = col_character(),\n  int_col = col_character(),\n  accident = col_character(),\n  clean_title = col_character()\n)\n\npreprocessor <- function(dataframe) {\n\ndataframe <- dataframe %>%\n    janitor::clean_names() %>%\n    \n    mutate(engine = str_replace_all(engine, \" Liter\", \"L\")) %>% \n\n    mutate(transmission = str_replace_all(transmission, \"[[:punct:]]\", \"\")) %>% \n  \n    mutate(transmission = str_replace_all(transmission, \"MT\", \"Manual\")) %>% \n    mutate(transmission = str_replace_all(transmission, \"AT\", \"Automatic\")) %>% \n\n    mutate(transmission = str_replace_all(transmission, \"SingleSpeed\", \"Single\")) %>% \n    mutate(transmission = str_replace_all(transmission, \"Single Fixed Gear\", \"Single\")) %>% \n    \n    mutate(model_length = stringr::str_length(model)) %>%\n  \n    mutate(displacement = parse_number(str_extract(engine, \".{3}(?=L)\"))) %>%   \n    mutate(power = parse_number(str_extract(engine, \".{5}(?=HP )\"))) %>%   \n    mutate(speeds = parse_number(str_extract(transmission, \".*(?=Speed )\"))) %>%   \n  \n \n    mutate(fuel_type = case_when(\n      \n      str_detect(engine, \"Electric\") ~ \"Electric\",\n      str_detect(brand, \"Tesla\") ~ \"Electric\",\n      str_detect(engine, \"Gasoline\") ~ \"Gasoline\",\n      str_detect(engine, \"Diesel\") ~ \"Diesel\",\n      fuel_type == \"Diesel\" ~ \"Diesel\",\n      displacement > 0  ~ \"Gasoline\", \n\n      TRUE ~ NA_character_      \n      \n    )) %>% \n\n    mutate(across(c(where(is.character)), ~ as.factor(.x))) \n\nreturn(dataframe)\n}\n\nraw_df <- read_csv(str_c(path, \"train.csv\"),\n                   col_types = train_spec,\n                   show_col_types = FALSE) %>%\n          preprocessor() \n\ntst_df <- read_csv(str_c(path, \"test.csv\"),\n                   col_types = competition_spec,\n                   show_col_types = FALSE)  %>% \n  preprocessor() \n\nall_df <-\n    bind_rows(raw_df %>% mutate(source = \"train\"),\n              tst_df %>% mutate(source = \"test\"))\n\ntrain_df <- all_df %>% \n  filter(source == \"train\") %>% \n  select(-source, -engine) \n\ncompetition_df <- all_df %>% \n  filter(source == \"test\") %>% \n  select(-source, -price, -engine)\n\nfeatures <- train_df %>%\n  select(-id, -price) %>%\n  names()\n\ntrain_df <- train_df %>% \n  distinct(pick(all_of(features)), .keep_all = TRUE)\n\nnom_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.character), where(is.factor)) %>%\n  names() \n\nlogical_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.logical)) %>%\n  names() \n\nnum_features <- train_df %>%\n  select(all_of(features)) %>%\n  select(where(is.numeric)) %>%\n  names()\n\nauto_split <- initial_split(train_df, prop = 0.95, strata = price)\nauto_train <- training(auto_split)\nauto_test <- testing(auto_split)\n\n\n```\n\nNominal features:\n\n`r nom_features`\n\nNumeric features: \n\n`r num_features`\n\nLogical features: \n\n`r logical_features`\n\n\nSize of the combined train and competition datasets:\n\n`r nrow(all_df)`\n\nSize of the split made available to machine learning\n\n`r nrow(auto_train)`\n\n\n# EDA {.tabset .tabset-fade .tabset-pills}\n\n## Numeric features\n\nConsider where features require univariate transformation, or clipping outliers.\n```{r}\n#| label: numeric\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 12\n\ntrain_df %>% \n  select(all_of(num_features), price ) %>% \n  pivot_longer(-price,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n  ggplot(aes(value, fill = ggplot2::cut_number(price, 5))) +\n  geom_histogram(bins = 200) +\n   facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\") +\n  labs(color = NULL, fill = NULL,\n       title = \"Numeric Feature Univariate Distributions\",\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n```\n\n\n## Nominal features\n\nExplore the distribution of outcome class by factor level, and the factor levels that exist in test that do not exist in training.\n\n\n```{r}\n#| label: nominal\n#| warning: false\n#| message: false\n#| fig.height: 18\n#| fig.width: 18\n\n\nif(length(nom_features) >0){\n\ntrain_df %>% \n  select(all_of(nom_features), price) %>% \n  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other')) %>%\n  pivot_longer(-price,\n    names_to = \"metric\",\n    values_to = \"value\"\n  ) %>%\n    \n  filter(!is.na(price)) %>% \n    \n  summarise(n = n(),\n            .by = c(price, metric, value)) %>%\n      \n  mutate(value = tidytext::reorder_within(value, n, metric)) %>%\n    \n  ggplot(aes(x = n, y = value, fill = ggplot2::cut_number(price, 5))) +\n  geom_col() +\n  tidytext::scale_y_reordered() +\n  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +\n  facet_wrap(vars(metric), scales = \"free\", ncol = 2) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n       legend.position = \"bottom\") +\n  labs(title = \"Nominal Feature Counts\",\n       fill = NULL,\n       caption = \"Data: Kaggle | Visual: Jim Gruman\")\n\n}\n\n```\n\n## Counts of Missingness\n                  \n```{r}\n#| label: counts of missingness\n\n#naniar::gg_miss_upset(train_df,  nintersects = 6) \n\n#naniar::vis_miss(train_df |> slice_sample(prop = 0.01))\n\ntrain_df %>% \n  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% \n  pivot_longer(everything(),\n              names_to = \"feature\",\n              values_to = \"Count of Missing\") %>% \n                   knitr::kable()\n                   \n\n                  \n```\n\n## Counts of Distinct\n                   \n               \n```{r}\n#| label: counts of distinct\n               \ntrain_df %>%\n  summarize(across(all_of(features), n_distinct)) %>%\n  pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct train\") |>\n  left_join(\n    tst_df %>%\n      summarize(across(all_of(features), n_distinct)) %>%\n      pivot_longer(everything(), names_to = \"feature\", values_to = \"Count of distinct test\")\n  ) %>% \n                   knitr::kable()\n               \n```\n\n## Duplicated\n\nIs this competition transaction already in the training data with a correct label?\n\nIt looks like we have a few car models with more than one selling price.\n\n```{r}\n#| label: duplicates\n#| warning: false\n#| message: false\nall_df %>%\n    select(all_of(features), source) %>% \n    group_by_at(features) %>%\n    mutate(num_dups = n(),\n           dup_id = row_number()) %>%\n    ungroup() %>%\n    group_by(source) %>%\n    mutate(is_duplicated = dup_id > 1) %>%\n    count(is_duplicated) %>% \n                   knitr::kable()\n               \n\n```\n                   \n\n\n\n## Target\n\nThere are some strange exotic cars on the top end of this distribution. There are odd bins at exactly 2,954,083, 1,950,995, 1,599,000 and so on. \n\nThese are all positive integers.\n\n```{r}\n#| label: outcome \n#| warning: false\n#| message: false\n#| fig.width: 6\n\ntrain_df %>% \n  ggplot(aes(price)) +\n  geom_histogram(bins = 100) +\n  scale_x_log10(labels = scales::comma) +\n  labs(title = \"Car prices\",\n       caption = \"Data: Kaggle.com | Visual: Jim Gruman\")\n\n\n\n```\n \n           \n                \n# Regression Machine Learning {.tabset .tabset-fade .tabset-pills}\n\n\n## Recipe\n\n```{r}\n#| label: recipe\n                   \nrec <- recipe(\n    \n    formula(paste0(\"price ~ \", \n               str_c(features,  collapse = \" + \"))),\n    data = auto_train\n  ) %>% \n  step_novel(all_nominal_predictors()) %>%\n                   \n#  textrecipes::step_textfeature(model, transmission, ext_col, int_col, keep_original_cols = TRUE) %>%                   \n  \n  textrecipes::step_tokenize(model, transmission, ext_col, int_col) %>% \n  textrecipes::step_tokenfilter(model, max_tokens = 148L) %>% \n  \n  textrecipes::step_tfidf(model, transmission, ext_col, int_col) %>% \n  \n  step_nzv(all_predictors()) %>%\n  step_corr(all_predictors())\n                   \nfolds <- vfold_cv(auto_train, \n                  v = 5,\n                  repeats = 1,\n                  strata = price)    \n                   \nctrl <- finetune::control_sim_anneal(     \n     verbose = FALSE,\n     verbose_iter = TRUE,\n     parallel_over = \"everything\",\n     save_pred = TRUE,\n     save_workflow = FALSE)                   \n                                     \n```\n\n\n\n## Lightgbm\n\n```{r}\n#| label: lgbm\n\n\nboost_tree_lgbm_spec <- \n  boost_tree(\n    trees = 2000L,\n   tree_depth = 6L,\n   learn_rate =  tune(),\n   min_n = 6L,\n   loss_reduction = 0\n  ) %>% \n  set_engine(engine = \"lightgbm\",\n             is_unbalance = TRUE,\n             num_leaves = tune(),\n             num_threads = cores\n             ) %>%\n  set_mode(mode = \"regression\") \n                   \nwf <- workflow(rec,\n               boost_tree_lgbm_spec) \n\nparam <- wf %>%\n   extract_parameter_set_dials() %>%\n   recipes::update(\n#      min_n = min_n(range = c(5,20)),\n#      max_tokens = max_tokens(range = c(75,200)),\n#      tree_depth = tree_depth(range = c(3,30)),\n      learn_rate = learn_rate(range = c(-1.7,-4)),\n      num_leaves = num_leaves(range = c(150,250))\n   ) %>%\n   dials::finalize(auto_train)                 \n\nburnin <- tune_grid(\n  wf,\n  grid = 4,\n  resamples = folds,\n  control = ctrl,\n  metrics = metric_set(rmse, mae),\n  param_info = param)\n\nlgbm_rs <- finetune::tune_sim_anneal(\n  wf,\n  resamples = folds,\n  iter = 5,\n  initial = burnin,\n  control = ctrl,\n  metrics =  metric_set(rmse, mae),\n  param_info = param) \n\nshow_best(lgbm_rs, metric = \"rmse\")  \n\nautoplot(lgbm_rs)\n\ncollect_metrics(lgbm_rs) %>% \n  filter(.metric == \"rmse\") %>% \n  mutate(.config = fct_reorder(.config, -mean)) %>% \n  ggplot(aes(mean, .config)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = mean - std_err, xmax = mean + std_err)) +\n  labs(title = \"RMSE across resample folds with std_err\")\n                   \n\n```\n\n\n\n\n                   \n# Submission\n                   \n```{r}\n#| label: last fit\n#| warning: false\n#| message: false\n#| fig.height: 24\n#| fig.width: 6                   \n                   \n\nbest_rmse <-   select_best(lgbm_rs, metric = \"rmse\")                 \n                   \nfinal_lgbm <- wf %>%\n    finalize_workflow(best_rmse) \n\nlast_fit(\n  final_lgbm,\n  auto_split,\n  metrics = metric_set(rmse)\n) %>%\n  collect_metrics() %>%\n  knitr::kable()\n\nregression_fit <- final_lgbm %>%\n                        fit(train_df) \n\nregression_fit |> \n  extract_fit_engine() |> \n  lgb.importance() |> \n  as_tibble() %>% \n  ggplot(aes(x = Gain, y = fct_reorder(Feature, Gain))) +\n  geom_col() +\n  labs(title = \"Feature Importance\", x = \"Gain\", y = NULL, caption = \"LightGBM Model | Visual: Jim Gruman\")    \n```\n                   \n                   \n```{r}\n#| label: submission\n#| warning: false\n#| message: false\n#| fig.height: 6\n#| fig.width: 6                   \n                   \naugment(regression_fit, train_df) %>% \n  ggplot(aes(.pred, price)) +\n  geom_point(alpha = 0.1, shape = 20) +\n  geom_abline() \n\n\nsubmit_df <-  augment(regression_fit, competition_df) %>%\n       select(id, price = .pred) %>%\n       mutate(price = if_else(price < 0, 0, price))\n\nhead(submit_df)  %>% \n     knitr::kable()      \n\nsubmit_df  %>% \n  write_csv(\"submission.csv\")\n```   ","metadata":{"_uuid":"34abd325-bd8c-4b5d-a5dd-f38eb4b8d41d","_cell_guid":"0217ba01-c3bc-48a4-b240-47f949dd9fd7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}