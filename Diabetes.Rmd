---
title: "Diabetes"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

![](https://doralhw.org/wp-content/uploads/2024/10/Diabetes-First-steps-to-take-after-diabetes-diagnosis-071923.jpg)

# Introduction  {.tabset .tabset-fade .tabset-pills}

The goal of this competition is to predict a diabetes diagnosis.

My notebook serves as a demonstration of some of the possible techniques available to arrive at a solution.  I intend to add to this as I have time available. Your questions and comments are welcome.

Lets dive right in.

The Kaggle kernels have many of the common r packages built in.  

## Load libraries

```{r }
#| label: setup
#| warning: false
#| message: false

if (dir.exists("/kaggle")){
  path <- "/kaggle/input/playground-series-s5e12/"
    
# options(repos = c(CRAN = "https://packagemanager.posit.co/cran/2021-03-22"))

install.packages("pak")
pak::pak("bonsai")
    
remotes::install_github("luisdva/hexsession", quiet = TRUE)    

cores <- future::availableCores()

} else {
  path <- stringr::str_c(here::here("data"),"/")
  orig_path <- stringr::str_c(here::here("data"),"/")

  cores <- future::availableCores(omit = 2)
  
}
 
suppressPackageStartupMessages({
library(tidyverse, quietly = TRUE) # metapackage of all tidyverse packages
library(tidymodels, quietly = TRUE) # metapackage see https://www.tidymodels.org/

library(correlationfunnel)
    
library(brulee) # tidymodels access to torch    

library(bonsai)  # Tidymodels access to lightgbm
library(stacks)  # Tidymodels ensembling
    
library(DALEXtra) # Tidymodels Explainability         

# library(mirai)
# daemons(cores)
    
})

tidymodels_prefer()

options(tidymodels.dark = TRUE)

theme_set(theme_minimal())

hexsession::make_tile(packages = c("DALEXtra", "DALEX", "lightgbm","bonsai", "brulee", "correlationfunnel","yardstick","workflows",
                                   "tune","rsample","recipes","parsnip","infer","dials","broom","tidymodels","lubridate",
                                   "forcats","stringr","dplyr","purrr","readr","tidyr","tibble","ggplot2","tidyverse","reprex","testthat","usethis"))


```



## Load Data

```{r }
#| label: load data
#| warning: false
#| message: false

alphabet_lookup <- setNames(1:26, LETTERS)

preprocessor <- function(dataframe) {

dataframe <- dataframe |>
    janitor::clean_names() |>
  mutate(
      pulse_pressure = systolic_bp - diastolic_bp,
      mean_arterial_pressure = diastolic_bp + (pulse_pressure / 3),
      hypertension_stage = case_when(
        systolic_bp >= 140 | diastolic_bp >= 90 ~ 3,
        systolic_bp >= 130 | diastolic_bp >= 80 ~ 2,
        systolic_bp >= 120 ~ 1,
        TRUE ~ 0
      ),
      high_blood_pressure = as.integer(systolic_bp >= 130 | diastolic_bp >= 80),      

      ldl_hdl_ratio = ldl_cholesterol / (hdl_cholesterol + 0.01),
      triglyceride_hdl_ratio = triglycerides / (hdl_cholesterol + 0.01),
      non_hdl_cholesterol = cholesterol_total - hdl_cholesterol,
      atherogenic_index = log10(triglycerides / (hdl_cholesterol + 0.01)),
      
      cholesterol_risk = case_when(
        cholesterol_total >= 240 ~ 3,
        cholesterol_total >= 200 ~ 2,
        cholesterol_total >= 180 ~ 1,
        TRUE ~ 0
      ),
      
      ldl_risk = case_when(
        ldl_cholesterol >= 160 ~ 3,
        ldl_cholesterol >= 130 ~ 2,
        ldl_cholesterol >= 100 ~ 1,
        TRUE ~ 0
      ),
      
      low_hdl = as.integer(hdl_cholesterol < 40),
      high_triglycerides = as.integer(triglycerides >= 150),
      

      bmi_category = case_when(
        bmi >= 35 ~ 4,
        bmi >= 30 ~ 3,
        bmi >= 25 ~ 2,
        bmi >= 18.5 ~ 1,
        TRUE ~ 0
      ),
      
      obesity = as.integer(bmi >= 30),
      central_obesity = as.integer(waist_to_hip_ratio > 0.90),
      

      metabolic_syndrome_score = 
        as.integer(waist_to_hip_ratio > 0.90) +
        as.integer(triglycerides >= 150) +
        as.integer(hdl_cholesterol < 40) +
        as.integer(systolic_bp >= 130 | diastolic_bp >= 85) +
        as.integer(bmi >= 30),
      
      has_metabolic_syndrome = as.integer(metabolic_syndrome_score >= 3),
      

      lifestyle_score = (diet_score / 10) * 0.25 + 
                        (pmin(physical_activity_minutes_per_week, 300) / 300) * 0.35 +
                        (pmin(sleep_hours_per_day, 9) / 8) * 0.2 -
                        (pmin(screen_time_hours_per_day, 12) / 12) * 0.2,
      
      poor_diet = as.integer(diet_score < 5),
      sedentary = as.integer(physical_activity_minutes_per_week < 150),
      poor_sleep = as.integer(sleep_hours_per_day < 6 | sleep_hours_per_day > 9),
      excessive_screen = as.integer(screen_time_hours_per_day > 6),
      
      activity_sedentary_ratio = physical_activity_minutes_per_week / 
                                 (screen_time_hours_per_day * 60 + 1),
      
      unhealthy_lifestyle_count = poor_diet + sedentary + poor_sleep + excessive_screen,
      

      smoking_risk = case_when(
        smoking_status == "current" ~ 2,
        smoking_status == "former" ~ 1,
        TRUE ~ 0
      ),
      
      is_smoker = as.integer(smoking_status == "current"),
      
      alcohol_risk = case_when(
        alcohol_consumption_per_week > 14 ~ 2,
        alcohol_consumption_per_week > 7 ~ 1,
        TRUE ~ 0
      ),
      
      high_alcohol = as.integer(alcohol_consumption_per_week > 14),
      combined_substance_risk = smoking_risk + alcohol_risk,
      

      age_group = cut(age, 
                      breaks = c(0, 35, 45, 55, 65, 100),
                      labels = c("under_35", "35_44", "45_54", "55_64", "65_plus"),
                      include.lowest = TRUE),
      
      elderly = as.integer(age >= 65),
      

      age_bmi_risk = (age / 50) * (bmi / 25),
      age_cholesterol = age * cholesterol_total / 100,
      bmi_cholesterol = bmi * cholesterol_total / 100,
      bmi_triglycerides = bmi * triglycerides / 100,
      

      comorbidity_count = family_history_diabetes + 
                         hypertension_history + 
                         cardiovascular_history,
      
      has_family_history = as.integer(family_history_diabetes == 1),
      has_cv_risk = as.integer(hypertension_history == 1 | cardiovascular_history == 1),
      multiple_comorbidities = as.integer(comorbidity_count >= 2),
      
      high_risk_profile = as.integer(
        comorbidity_count >= 1 & 
        (bmi >= 30 | cholesterol_total >= 240 | systolic_bp >= 140)
      ),
      

      global_risk_score = 
        (age / 100) * 20 +
        (bmi / 50) * 15 +
        (cholesterol_total / 300) * 10 +
        (systolic_bp / 180) * 10 +
        (triglycerides / 300) * 10 +
        comorbidity_count * 10 +
        smoking_risk * 5 +
        alcohol_risk * 5 +
        as.integer(hdl_cholesterol < 40) * 10 +
        sedentary * 5
    ) |> 
  
      mutate(across(c(where(is.character),contains("history")), ~ as.factor(.x))) |> 
   
      mutate(across(
    c(
      age,
      alcohol_consumption_per_week,
      physical_activity_minutes_per_week,
      diet_score,
      sleep_hours_per_day,
      screen_time_hours_per_day,
      bmi,
      waist_to_hip_ratio,
      systolic_bp,
      diastolic_bp,
      heart_rate,
      cholesterol_total,
      hdl_cholesterol,
      ldl_cholesterol,
      triglycerides
    ),
    \(x) as.numeric(scale(x)),
    .names = "scale_family_history_diabetes_{.col}"     #1
  ), .by = family_history_diabetes) |> 
  
    mutate(across(
    c(
      age,
      alcohol_consumption_per_week,
      physical_activity_minutes_per_week,
      diet_score,
      sleep_hours_per_day,
      screen_time_hours_per_day,
      bmi,
      waist_to_hip_ratio,
      systolic_bp,
      diastolic_bp,
      heart_rate,
      cholesterol_total,
      hdl_cholesterol,
      ldl_cholesterol,
      triglycerides
    ),
    \(x) as.numeric(scale(x)),
    .names = "scale_hypertension_history_{.col}"   #3
  ), .by = hypertension_history            ) |> 
  
      mutate(across(
    c(
      age,
      alcohol_consumption_per_week,
      physical_activity_minutes_per_week,
      diet_score,
      sleep_hours_per_day,
      screen_time_hours_per_day,
      bmi,
      waist_to_hip_ratio,
      systolic_bp,
      diastolic_bp,
      heart_rate,
      cholesterol_total,
      hdl_cholesterol,
      ldl_cholesterol,
      triglycerides
    ),
    \(x) as.numeric(scale(x)),
    .names = "scale_cardiovascular_history_{.col}"        #2
  ), .by = cardiovascular_history             ) 

return(dataframe)
}

raw_df <- read_csv(str_c(path, "train.csv"),
                   show_col_types = FALSE) 

tst_df <- read_csv(str_c(path, "test.csv"),
                   show_col_types = FALSE)  

all_df <-
    bind_rows(raw_df %>% mutate(source = "train"),
              tst_df %>% mutate(source = "test")) |>
          preprocessor() |> 
   mutate(diagnosed_diabetes = factor(diagnosed_diabetes))

train_df <- all_df %>% 
  filter(source == "train") %>% 
  select(-source) 

competition_df <- all_df %>% 
  filter(source == "test") %>% 
  select(-source, -diagnosed_diabetes)


```

# EDA {.tabset .tabset-fade .tabset-pills}

## Features

We will de-duplicate the training set and use the first diagnosed_diabetes data point.

```{r}
#| label: Features
#| warning: false
#| message: false
#| fig.width: 6

conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("where", "dplyr")

features <- train_df %>%
  dplyr::select(-id, -diagnosed_diabetes) |> 
  names()

train_df <- train_df %>% 
  dplyr::group_by(pick({{features}})) |> 
  dplyr::summarise(id = min(id),
                   diagnosed_diabetes = first(diagnosed_diabetes),
                   .groups = "drop")

nom_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.factor)) |> 
  names() 

logical_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.logical)) |> 
  names() 

num_features <- train_df |> 
  dplyr::select(dplyr::all_of(features)) |> 
  dplyr::select(dplyr::where(is.numeric)) |> 
  names()
```

Nominal features:

`r nom_features`

Numeric features: 

`r num_features`

Logical features: 

`r logical_features`


Size of the combined train and competition datasets:

`r nrow(all_df)`

Size of training set after de-duplication

`r nrow(train_df)`


## Numeric features



```{r}
#| label: numeric
#| warning: false
#| message: false
#| fig.height: 18

train_df %>% 
  dplyr::select(all_of(num_features), diagnosed_diabetes ) %>% 
  pivot_longer(-diagnosed_diabetes,
    names_to = "metric",
    values_to = "value"
  ) %>%
  ggplot(aes(value, fill = diagnosed_diabetes)) +
  geom_density(position = "stack") +
  scale_fill_brewer(type = "seq", palette = "Dark2") +
  facet_wrap(vars(metric), scales = "free", ncol = 6) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "top") +
  labs(color = NULL, fill = "diagnosed_diabetes",
       title = "Numeric Feature Univariate Distributions",
       caption = "Data: Kaggle | Visual: Jim Gruman")

```


## Nominal and Logical features

Explore the distribution of outcome class by factor level.


```{r}
#| label: nominal
#| warning: false
#| message: false
#| fig.height: 12


if(length(nom_features) >0){

train_df %>% 
  select(all_of(nom_features), all_of(logical_features), diagnosed_diabetes) %>% 
  mutate(across(nom_features, fct_lump_n,n = 10, other_level = 'other'),
         across(logical_features, \(x) factor(x))) %>%
  pivot_longer(-diagnosed_diabetes,
    names_to = "metric",
    values_to = "value"
  ) %>%
    
  filter(!is.na(diagnosed_diabetes)) %>% 
    
  summarise(n = n(),
            .by = c(diagnosed_diabetes, metric, value)) %>%
      
  mutate(value = tidytext::reorder_within(value, n, metric)) %>%
    
  ggplot(aes(x = n, y = value, fill = diagnosed_diabetes)) +
  geom_col() +
  tidytext::scale_y_reordered() +
  scale_x_continuous(n.breaks = 3, guide = guide_axis(n.dodge = 2))  +
  scale_fill_brewer(type = "seq", palette = "Dark2") +
  facet_wrap(vars(metric), scales = "free", ncol = 2) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
       legend.position = "bottom") +
  labs(title = "Nominal Feature Counts",
       fill = NULL, y = NULL,
       caption = "Data: Kaggle | Visual: Jim Gruman")

}

```

## Counts of Missingness

                  
```{r}
#| label: counts of missingness

train_df %>% 
  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% 
  pivot_longer(everything(),
              names_to = "feature",
              values_to = "Count of Missing") %>% 
                   knitr::kable()

competition_df %>% 
  summarize(across(all_of(features), function(x) sum(is.na(x)))) %>% 
  pivot_longer(everything(),
              names_to = "feature",
              values_to = "Count of Missing") %>% 
                   knitr::kable()

```

## Counts of Distinct
                   
               
```{r}
#| label: counts of distinct
               
train_df %>%
  summarize(across(all_of(features), n_distinct)) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "Count of distinct train") |>
  left_join(
    competition_df %>%
      summarize(across(all_of(features), n_distinct)) %>%
      pivot_longer(everything(), names_to = "feature", values_to = "Count of distinct test"),
    by = join_by(feature)
  ) %>% 
                   knitr::kable()
               
```

## Duplicated

Is this competition transaction already in the training data with a correct label?


```{r}
#| label: duplicates
#| warning: false
#| message: false

bind_rows(train_df %>% mutate(source = "train"),
              competition_df %>% mutate(source = "test")) |> 
    group_by_at(features) %>%
    mutate(num_dups = n(),
           dup_id = row_number()) %>% 
    ungroup() %>% 
    group_by(source) %>%
    mutate(is_duplicated = dup_id > 1) %>% 
    count(is_duplicated) %>% 
                   knitr::kable()



```


## Pairwise Correlations
                   
`ggcorrplot` provides a quick look at numeric features where the correlation may be significant. 

```{r}
#| label: pairwise correlations
#| fig.height: 36                 
                 
#trainp.mat <- train_df %>% 
#  mutate(diagnosed_diabetes = as.numeric(diagnosed_diabetes)) |> 
#  select(all_of(num_features), diagnosed_diabetes) %>% 
#  ggcorrplot::cor_pmat() 
                   
#train_df %>% 
#  mutate(diagnosed_diabetes = as.numeric(diagnosed_diabetes)) |> 
#  select(all_of(num_features), diagnosed_diabetes) %>%                    
#  cor() %>% 
#  ggcorrplot::ggcorrplot(hc.order = TRUE, 
#                         lab = TRUE,
#                         p.mat = trainp.mat,
#    type = "lower", insig = "blank") +
#  labs(title = "Pairwise Correlations Training Set")
                   
#competitionp.mat <- competition_df %>% 
#  select(all_of(num_features)) %>% 
#  ggcorrplot::cor_pmat() 
                   
#competition_df %>% 
#  select(all_of(num_features)) %>%                   
#  cor()   %>% 
#  ggcorrplot::ggcorrplot(hc.order = TRUE, lab = TRUE,
#                         p.mat = competitionp.mat,
#    type = "lower", insig = "blank") +
#  labs(title = "Pairwise Correlations Competition Set")

```                    

## Correlation Funnel


```{r}
#| label: correlation funnel 
#| warning: false
#| message: false
#| fig.height: 24


#y_df <- train_df |>
#  transmute(diagnosed_diabetes = as.numeric(diagnosed_diabetes)-1) 

#train_df %>%
#  select(all_of(num_features)) %>%
#  correlationfunnel::binarize(one_hot = TRUE, n_bins = 5) %>%
#  bind_cols(y_df) |>
#  correlate(target = diagnosed_diabetes) %>%
#  plot_correlation_funnel()

``` 
                   
                   
## Target

```{r}
#| label: outcome 
#| warning: false
#| message: false
#| fig.width: 6


train_df %>%
  summarize(outcome_sum = n(), .by = diagnosed_diabetes) %>%
  arrange(desc(outcome_sum)) %>%
  mutate(prop = outcome_sum / nrow(train_df)) %>%
  mutate(ypos = cumsum(prop) - 0.5 * prop) %>%
  ggplot(aes(x = "", y = prop, fill = diagnosed_diabetes)) +
  geom_bar(stat = "identity",
           width = 1,
           show.legend = FALSE) +
  geom_text(
    aes(
      y = ypos,
      label= paste0(diagnosed_diabetes
                     , "\n", round(prop, 2) * 100, "%")
    ),
    color = "white",
    nudge_x = 0,
    size = 3
  ) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +                   
  coord_polar("y", start = 0) +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank()  ) +
  labs(title = "diagnosed_diabetes", caption = "Data: Kaggle.com | Visual: Jim Gruman")

``` 
                
# Machine Learning {.tabset .tabset-fade .tabset-pills}


## Recipe

```{r}
#| label: recipe
#| warning: false
#| message: false
#| fig.width: 6
                   
lgbm_rec <- recipe(formula(paste0("diagnosed_diabetes ~ ", str_c(features, collapse = " + "))), data = train_df) 
                   
folds <- vfold_cv(train_df,
                  v = 14,
                  repeats = 1,
                  strata = diagnosed_diabetes)
                   
```


## Machine Learning 


```{r}
#| label: ml 
#| warning: false
#| message: false
        
                                   
                   
boost_tree_lgbm_spec <- 
  boost_tree(
    trees = 450,
    tree_depth = tune(),
    learn_rate =  tune(),
    min_n = tune()
  ) %>% 
  set_engine(engine = "lightgbm" ,
             num_threads = cores,
             num_leaves = tune()) %>%  
  set_mode(mode = "classification")             


dep_models <-
   workflow_set(
      preproc = list(base = lgbm_rec),
      models = list(lgbm = boost_tree_lgbm_spec),
      cross = FALSE
   ) %>%
  option_add_parameters() |>
  option_add(
    control = finetune::control_sim_anneal( save_pred = TRUE, save_workflow = TRUE, verbose = TRUE, verbose_iter = TRUE),
    metrics = metric_set(roc_auc)
  )

lgbm_params <- dep_models |>
 extract_workflow("base_lgbm") |>
 extract_parameter_set_dials() |>
 update(
         tree_depth = tree_depth(range = c(7, 14)),
         min_n = min_n(range = c(2, 25)),
         num_leaves = num_leaves(range = c(60,120)),
         learn_rate = learn_rate(range = c(-2.2,-1.0))
 ) 

all_models <- dep_models |>
 option_add(
   param_info = lgbm_params,
   id = "base_lgbm"
 ) |>
  workflow_map("tune_sim_anneal", 
               resamples = folds, 
               iter = 9,
               metrics = metric_set(roc_auc), verbose = TRUE)
                   

autoplot(all_models) +
  geom_text(aes(y = mean -0.002, label = wflow_id), angle = 90, hjust = 1)+
  scale_y_continuous(limits = c(0,NA), expand = c(0, 0)) |>
  theme(legend.position = "none")

rank_results(all_models, rank_metric = "roc_auc", select_best = TRUE) %>%
   select(rank, mean, model, wflow_id, .config) |> 
   arrange(desc(mean))
 
all_models %>%
  dplyr::filter(grepl("lgbm", wflow_id)) %>%
  mutate(metrics = map(result, collect_metrics)) %>%
  dplyr::select(wflow_id, metrics) %>%
  tidyr::unnest(cols = metrics) |>
  arrange(desc(mean))

all_models |>
  workflowsets::extract_workflow_set_result("base_lgbm") |>
  autoplot() +
  labs(title = "Lightgbm Hyperparameter Search")


dep_stack <- stacks() %>%
  add_candidates(all_models) %>%
  blend_predictions(  metric =  metric_set(roc_auc),
      penalty = c(10^seq(-3.7, -2.3, 0.1)),
      non_negative = TRUE,
      control = tune::control_grid(allow_par = TRUE))

autoplot(dep_stack)

autoplot(dep_stack, type = "members")        
                   
autoplot(dep_stack, "weights")
                   
classification_fit <- dep_stack %>% 
    fit_members()                   

```



# Performance {.tabset .tabset-fade .tabset-pills}

```{r}
#| label: explainer
#| warning: false
#| message: false
#| fig.height: 12
                   
                   
explainer <- 
  explain_tidymodels(
    classification_fit, 
    data = train_df %>% dplyr::select(all_of(features)), 
    y = as.numeric(train_df$diagnosed_diabetes),
    label = "Ensemble",
    verbose = FALSE
  )  %>% 
  model_parts()

ggplot_imp <- function(...) {
  obj <- list(...)
  metric_name <- attr(obj[[1]], "loss_name")
  metric_lab <- paste(metric_name, 
                      "after permutations\n(lower indicates more important)")
  
  full_vip <- bind_rows(obj) %>%
    filter(variable != "_baseline_")
  
  perm_vals <- full_vip %>% 
    filter(variable == "_full_model_") %>% 
    group_by(label) %>% 
    summarise(dropout_loss = mean(dropout_loss))
  
  p <- full_vip %>%
    filter(variable != "_full_model_") %>% 
    mutate(variable = fct_reorder(variable, -dropout_loss)) %>%
    ggplot(aes(dropout_loss, variable)) 
  
  if(length(obj) > 1) {
    p <- p + 
      facet_wrap(vars(label)) +
      geom_vline(data = perm_vals, aes(xintercept = dropout_loss, color = label),
                 linewidth = 1.4, lty = 2, alpha = 0.7) +
      geom_boxplot(aes(color = label, fill = label), alpha = 0.2)
  } else {
    p <- p + 
      geom_vline(data = perm_vals, aes(xintercept = dropout_loss),
                 linewidth = 1.4, lty = 2, alpha = 0.7) +
      geom_boxplot(fill = "#91CBD765", alpha = 0.4)
    
  }
  p +
    theme(legend.position = "none") +
    labs(x = metric_lab, 
         y = NULL,  fill = NULL,  color = NULL)
}
                   
ggplot_imp(explainer)      
                       

```                      


                   
# Submission
                   

```{r}
#| label: submission
#| warning: false
#| message: false
#| fig.height: 6
#| fig.width: 6   
                 
predict(classification_fit, train_df, type = "prob") %>% 
  bind_cols(train_df) |> 
  mutate(
    .pred_class = factor(if_else(.pred_1 > 0.5, "1","0"))
  ) |>                    
  conf_mat(diagnosed_diabetes, .pred_class) %>%
  yardstick:::autoplot.conf_mat(type = "heatmap") +
  scale_fill_distiller(palette = "RdPu")

submit_df <- predict(classification_fit, competition_df, type = "prob") |>
  bind_cols(competition_df) |>
  transmute(
    id,
    diagnosed_diabetes = .pred_1)


head(submit_df)  %>% 
     knitr::kable()      

submit_df  %>% 
  write_csv("submission.csv")
```    